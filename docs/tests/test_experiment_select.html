<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.1">
<title>topicnet.tests.test_experiment_select API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>topicnet.tests.test_experiment_select</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="topicnet.tests.test_experiment_select.combine_constraints"><code class="name flex">
<span>def <span class="ident">combine_constraints</span></span>(<span>*constraints, connector='and', symbol_before=' ', symbol_after=' ')</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.format_init_parameter"><code class="name flex">
<span>def <span class="ident">format_init_parameter</span></span>(<span>init_parameter)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.format_score"><code class="name flex">
<span>def <span class="ident">format_score</span></span>(<span>score)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.get_models"><code class="name flex">
<span>def <span class="ident">get_models</span></span>(<span>scores: list = None, init_parameters: list = None, score_range=None, init_parameters_range=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.get_models_with_one_init_parameter"><code class="name flex">
<span>def <span class="ident">get_models_with_one_init_parameter</span></span>(<span>init_parameter=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.get_models_with_one_score"><code class="name flex">
<span>def <span class="ident">get_models_with_one_score</span></span>(<span>score=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.get_models_with_two_scores_two_init_parameters"><code class="name flex">
<span>def <span class="ident">get_models_with_two_scores_two_init_parameters</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="topicnet.tests.test_experiment_select.MockTopicModel"><code class="flex name class">
<span>class <span class="ident">MockTopicModel</span></span>
<span>(</span><span>name, depth=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Topic Model contains artm model and all necessary information: scores, training pipeline, etc.</p>
<p>Initialize stage, also used for loading previously saved experiments.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>artm_model</code></strong> :&ensp;<code>artm model</code> or <code>None</code></dt>
<dd>model to use, None if you want to create model (Default value = None)</dd>
<dt><strong><code>model_id</code></strong> :&ensp;<code>str</code></dt>
<dd>model id (Default value = None)</dd>
<dt><strong><code>parent_model_id</code></strong> :&ensp;<code>str</code></dt>
<dd>model id from which current model was created (Default value = None)</dd>
<dt><strong><code>data_path</code></strong> :&ensp;<code>str</code></dt>
<dd>path to the data (Default value = None)</dd>
<dt><strong><code>description</code></strong> :&ensp;<code>list</code> of <code>dict</code></dt>
<dd>description of the model (Default value = None)</dd>
<dt><strong><code>experiment</code></strong> :&ensp;<code>Experiment</code></dt>
<dd>the experiment to which the model is bound (Default value = None)</dd>
<dt><strong><code>callbacks</code></strong> :&ensp;<code>list</code> of <code>objects with invoke() method</code></dt>
<dd>function called inside _fit which alters model parameters
mainly used for fancy regularizer coefficients manipulation</dd>
<dt><strong><code>custom_scores</code></strong> :&ensp;<code>dict</code></dt>
<dd>dictionary with score names as keys and score classes as functions
(score class with functionality like those of BaseScore)</dd>
<dt><strong><code>custom_regularizers</code></strong> :&ensp;<code>dict</code></dt>
<dd>dictionary with regularizer names as keys and regularizer classes as values</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MockTopicModel(TopicModel):
    def __init__(self, name, depth=1):
        super().__init__(model_id=name, artm_model=ARTM_MODEL)

        self._name = name
        self._depth = depth
        self._scores = dict()
        self._init_parameters = dict()

    @property
    def depth(self):
        return self._depth

    @property
    def scores(self):
        return self._scores

    @property
    def init_parameters(self):
        return self._init_parameters

    @property
    def name(self):
        return self._name

    def __str__(self):
        # Trying to gather all the info so as not to log during tests
        result = f&#39;{self._name}&#39;

        for s, v in self.scores.items():
            formatted_score_value = f&#39;{v[-1]:.2f}&#39; if len(v) &gt; 0 else f&#39;{None}&#39;
            result += f&#39;__s:{s}:{formatted_score_value}&#39;

        for p, v in self.init_parameters.items():
            formatted_parameter_value = f&#39;{v:.2f}&#39; if v is not None else f&#39;{None}&#39;
            result += f&#39;__p:{p}:{formatted_parameter_value}&#39;

        return result

    def __repr__(self):
        return self.__str__()

    def __hash__(self):
        return hash(self._name)

    def __eq__(self, other):
        if other is None:
            return False

        if not isinstance(other, MockTopicModel):
            return False

        if self.__hash__() != other.__hash__():
            return False

        return True

    def get_init_parameters(self):
        return self._init_parameters

    def set_init_parameter(self, name: str, value):
        self._init_parameters[name] = value

        return self

    def set_score(self, name: str, values: list):
        assert isinstance(name, str) and len(name) &gt; 0
        assert isinstance(values, list)

        self._scores[name] = values

        return self

    @staticmethod
    def get_start_model():
        # TODO: can we rename it because of Win compatibility?
        model = MockTopicModel(name=&#39;&lt;&lt;&lt; Start Model &gt;&gt;&gt;&#39;, depth=0)

        for score in SCORES:
            model.set_score(score, [])

        for init_parameter in INIT_PARAMETERS:
            model.set_init_parameter(init_parameter, None)

        model.set_init_parameter(DEFINED_START_INIT_PARAMETER, 0)

        # Call get_jsonable_from_parameters() in experiment breaks stuff:
        # &#34;ARTM model not initialized&#34;
        # but if initialize ARTM_MODEL, tests become very slow
        model.get_jsonable_from_parameters = lambda: None

        return model

    @staticmethod
    def generate_specified_models(scores_ranges: dict = None, init_parameters_ranges: dict = None):
        def get_all_names_all_ranges_and_model_id_prefix():
            nonlocal scores_ranges
            nonlocal init_parameters_ranges

            if scores_ranges is not None and init_parameters_ranges is not None:
                return (
                    list(scores_ranges.keys()) + list(init_parameters_ranges.keys()),
                    list(scores_ranges.values()) + list(init_parameters_ranges.values()),
                    &#39;m_sp_&#39;
                )
            elif scores_ranges is None:
                return (
                    list(init_parameters_ranges.keys()),
                    list(init_parameters_ranges.values()),
                    &#39;m_p_&#39;
                )
            else:  # init_parameters_ranges is None
                return (
                    list(scores_ranges.keys()),
                    list(scores_ranges.values()),
                    &#39;m_s_&#39;
                )

        if scores_ranges is None:
            scores_ranges = dict()
        if init_parameters_ranges is None:
            init_parameters_ranges = dict()

        scores_names = set(scores_ranges.keys())
        names, ranges, model_id_prefix = get_all_names_all_ranges_and_model_id_prefix()
        models = []

        for ranges_section in product(*ranges):
            model = MockTopicModel(name=f&#39;{model_id_prefix}{len(models):04}&#39;)

            for name, value in zip(names, ranges_section):
                if name in scores_names:
                    model.set_score(name, [value])
                else:
                    model.set_init_parameter(name, value)

            models.append(model)

        return models</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="topicnet.cooking_machine.models.topic_model.TopicModel" href="../cooking_machine/models/topic_model.html#topicnet.cooking_machine.models.topic_model.TopicModel">TopicModel</a></li>
<li><a title="topicnet.cooking_machine.models.base_model.BaseModel" href="../cooking_machine/models/base_model.html#topicnet.cooking_machine.models.base_model.BaseModel">BaseModel</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="topicnet.tests.test_experiment_select.MockTopicModel.generate_specified_models"><code class="name flex">
<span>def <span class="ident">generate_specified_models</span></span>(<span>scores_ranges: dict = None, init_parameters_ranges: dict = None)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.MockTopicModel.get_start_model"><code class="name flex">
<span>def <span class="ident">get_start_model</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="topicnet.tests.test_experiment_select.MockTopicModel.init_parameters"><code class="name">prop <span class="ident">init_parameters</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def init_parameters(self):
    return self._init_parameters</code></pre>
</details>
</dd>
<dt id="topicnet.tests.test_experiment_select.MockTopicModel.name"><code class="name">prop <span class="ident">name</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def name(self):
    return self._name</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="topicnet.tests.test_experiment_select.MockTopicModel.get_init_parameters"><code class="name flex">
<span>def <span class="ident">get_init_parameters</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.MockTopicModel.set_init_parameter"><code class="name flex">
<span>def <span class="ident">set_init_parameter</span></span>(<span>self, name: str, value)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.MockTopicModel.set_score"><code class="name flex">
<span>def <span class="ident">set_score</span></span>(<span>self, name: str, values: list)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="topicnet.cooking_machine.models.topic_model.TopicModel" href="../cooking_machine/models/topic_model.html#topicnet.cooking_machine.models.topic_model.TopicModel">TopicModel</a></b></code>:
<ul class="hlist">
<li><code><a title="topicnet.cooking_machine.models.topic_model.TopicModel.add_cube" href="../cooking_machine/models/base_model.html#topicnet.cooking_machine.models.base_model.BaseModel.add_cube">add_cube</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.topic_model.TopicModel.all_regularizers" href="../cooking_machine/models/topic_model.html#topicnet.cooking_machine.models.topic_model.TopicModel.all_regularizers">all_regularizers</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.topic_model.TopicModel.clone" href="../cooking_machine/models/topic_model.html#topicnet.cooking_machine.models.topic_model.TopicModel.clone">clone</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.topic_model.TopicModel.depth" href="../cooking_machine/models/base_model.html#topicnet.cooking_machine.models.base_model.BaseModel.depth">depth</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.topic_model.TopicModel.get_jsonable_from_parameters" href="../cooking_machine/models/topic_model.html#topicnet.cooking_machine.models.topic_model.TopicModel.get_jsonable_from_parameters">get_jsonable_from_parameters</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.topic_model.TopicModel.get_parameters" href="../cooking_machine/models/base_model.html#topicnet.cooking_machine.models.base_model.BaseModel.get_parameters">get_parameters</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.topic_model.TopicModel.get_phi" href="../cooking_machine/models/topic_model.html#topicnet.cooking_machine.models.topic_model.TopicModel.get_phi">get_phi</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.topic_model.TopicModel.get_phi_dense" href="../cooking_machine/models/topic_model.html#topicnet.cooking_machine.models.topic_model.TopicModel.get_phi_dense">get_phi_dense</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.topic_model.TopicModel.get_phi_sparse" href="../cooking_machine/models/topic_model.html#topicnet.cooking_machine.models.topic_model.TopicModel.get_phi_sparse">get_phi_sparse</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.topic_model.TopicModel.get_regularizer" href="../cooking_machine/models/topic_model.html#topicnet.cooking_machine.models.topic_model.TopicModel.get_regularizer">get_regularizer</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.topic_model.TopicModel.get_theta" href="../cooking_machine/models/topic_model.html#topicnet.cooking_machine.models.topic_model.TopicModel.get_theta">get_theta</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.topic_model.TopicModel.load" href="../cooking_machine/models/topic_model.html#topicnet.cooking_machine.models.topic_model.TopicModel.load">load</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.topic_model.TopicModel.make_dummy" href="../cooking_machine/models/topic_model.html#topicnet.cooking_machine.models.topic_model.TopicModel.make_dummy">make_dummy</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.topic_model.TopicModel.regularizers" href="../cooking_machine/models/topic_model.html#topicnet.cooking_machine.models.topic_model.TopicModel.regularizers">regularizers</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.topic_model.TopicModel.save" href="../cooking_machine/models/topic_model.html#topicnet.cooking_machine.models.topic_model.TopicModel.save">save</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.topic_model.TopicModel.save_parameters" href="../cooking_machine/models/base_model.html#topicnet.cooking_machine.models.base_model.BaseModel.save_parameters">save_parameters</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.topic_model.TopicModel.scores" href="../cooking_machine/models/topic_model.html#topicnet.cooking_machine.models.topic_model.TopicModel.scores">scores</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.topic_model.TopicModel.select_topics" href="../cooking_machine/models/topic_model.html#topicnet.cooking_machine.models.topic_model.TopicModel.select_topics">select_topics</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.topic_model.TopicModel.to_dummy" href="../cooking_machine/models/topic_model.html#topicnet.cooking_machine.models.topic_model.TopicModel.to_dummy">to_dummy</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect"><code class="flex name class">
<span>class <span class="ident">TestExperimentSelect</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TestExperimentSelect:
    experiments_folder = None
    current_experiment_id = -1
    query_sample = None

    @classmethod
    def setup_class(cls):
        cls.experiments_folder = tempfile.mkdtemp()
        cls.query_sample = CONSTRAINT_MAXIMIZE.format(format_score(SCORES[0]))

    @classmethod
    def teardown_class(cls):
        shutil.rmtree(cls.experiments_folder)
        ARTM_MODEL.dispose()

    @staticmethod
    def get_experiment(with_models=True):
        TestExperimentSelect.current_experiment_id += 1

        start_model = MockTopicModel.get_start_model()
        start_model.experiment = None

        experiment = Experiment(
            start_model,
            experiment_id=f&#39;{TestExperimentSelect.current_experiment_id:03}&#39;,
            save_path=TestExperimentSelect.experiments_folder,
            save_experiment=False
        )

        if with_models:
            TestExperimentSelect.set_models(
                experiment,
                get_models_with_two_scores_two_init_parameters()
            )

        return experiment

    @staticmethod
    def set_models(experiment, models):
        # without erasing experiment.models: &#34;start model&#34; stays
        experiment.models.update(
            {m.name: m for m in models}
        )

    @staticmethod
    def get_filter_for_score(query, score, threshold, models):
        if f&#39;{ARROW_TO} {MAX}&#39; in query:
            return lambda m: m.scores[score][-1] == max(
                model.scores[score][-1]
                for model in models if score in model.scores and len(model.scores[score]) &gt; 0
            ) if score in m.scores and len(m.scores[score]) &gt; 0 else False
        if f&#39;{ARROW_TO} {MIN}&#39; in query:
            return lambda m: m.scores[score][-1] == min(
                model.scores[score][-1]
                for model in models if score in model.scores and len(model.scores[score]) &gt; 0
            ) if score in m.scores and len(m.scores[score]) &gt; 0 else False
        if LESS in query:
            return lambda m: m.scores[score][-1] &lt; threshold\
                if score in m.scores and len(m.scores[score]) &gt; 0 else False
        if EQUALS in query:
            return lambda m: m.scores[score][-1] == threshold\
                if score in m.scores and len(m.scores[score]) &gt; 0 else False
        if GREATER in query:
            return lambda m: m.scores[score][-1] &gt; threshold\
                if score in m.scores and len(m.scores[score]) &gt; 0 else False

        raise ValueError(
            f&#39;Don\&#39;t know what to do with query &#34;{query}&#34; for score &#34;{score}&#34;...&#39;)

    @staticmethod
    def get_filter_for_init_parameter(query, parameter, threshold):
        # First &#34;start&#34; model is BaseModel and don&#39;t have get_init_parameters()
        # so need isinstance() check
        if LESS in query:
            return lambda m: m.get_init_parameters().get(parameter) &lt; threshold\
                if isinstance(m, TopicModel) and parameter in m.get_init_parameters() else False
        if EQUALS in query:
            return lambda m: m.get_init_parameters().get(parameter) == threshold \
                if isinstance(m, TopicModel) and parameter in m.get_init_parameters() else False
        if GREATER in query:
            return lambda m: m.get_init_parameters().get(parameter) &gt; threshold \
                if isinstance(m, TopicModel) and parameter in m.get_init_parameters() else False

        raise ValueError(
            f&#39;Don\&#39;t know what to do with query &#34;{query}&#34; for init parameter &#34;{parameter}&#34;...&#39;)

    @pytest.mark.parametrize(&#39;level&#39;, LEVELS_INVALID_TYPE_NUMERIC)
    def test_invalid_level_without_models(self, level):
        experiment = TestExperimentSelect.get_experiment(with_models=False)

        with warnings.catch_warnings():
            # TODO: check that tests on warnings are here somewhere
            warnings.filterwarnings(action=&#34;ignore&#34;, message=W_TOO_STRICT)
            warnings.filterwarnings(action=&#34;ignore&#34;, message=W_NOT_ENOUGH_MODELS_FOR_CHOICE)
            warnings.filterwarnings(action=&#34;ignore&#34;, message=W_RETURN_FEWER_MODELS)

            query = TestExperimentSelect.query_sample
            selection = experiment.select(query, models_num=1, level=level)

        assert len(selection) == 0, &#39;Some models selected with invalid &#34;level&#34;&#39;

    @pytest.mark.parametrize(&#39;level&#39;, LEVELS_INVALID_TYPE)
    def test_invalid_level_with_models(self, level):
        experiment = TestExperimentSelect.get_experiment()

        with pytest.raises(ValueError):
            _ = experiment.select(TestExperimentSelect.query_sample, models_num=1, level=level)

    def test_invalid_num_models_with_models(self):
        experiment = TestExperimentSelect.get_experiment()

        with pytest.raises(ValueError):
            _ = experiment.select(
                TestExperimentSelect.query_sample, models_num=NUM_MODELS_INVALID_TYPE
            )

    @pytest.mark.parametrize(&#39;with_models&#39;, [False, True])
    def test_zero_num_models(self, with_models):
        experiment = TestExperimentSelect.get_experiment(with_models=with_models)

        selection = experiment.select(TestExperimentSelect.query_sample, models_num=0)

        assert len(selection) == 0, &#39;Some models selected&#39;

    @pytest.mark.parametrize(&#39;with_models&#39;, [False, True])
    def test_wrong_num_models(self, with_models):
        experiment = TestExperimentSelect.get_experiment(with_models=with_models)

        with pytest.raises(ValueError):
            _ = experiment.select(
                TestExperimentSelect.query_sample, models_num=NUM_MODELS_INVALID_VALUE
            )

    def test_default_level(self):
        experiment = TestExperimentSelect.get_experiment()

        selection = experiment.select(TestExperimentSelect.query_sample, models_num=1)
        max_depth = max(m.depth for m in experiment.models.values())

        assert len(selection) &gt; 0,\
            &#39;None models selected&#39;
        assert all(s.depth == max_depth for s in selection),\
            &#39;Some models among selected have wrong depth&#39;

    @pytest.mark.parametrize(
        &#39;score, init_parameter, score_threshold, init_parameter_threshold&#39;,
        [(SCORES[0], INIT_PARAMETERS[0], MIDDLE_SCORE, MAX_INIT_PARAMETER)]
    )
    def test_default_num_models(
            self, score, init_parameter, score_threshold, init_parameter_threshold):

        experiment = TestExperimentSelect.get_experiment()
        with warnings.catch_warnings():
            warnings.filterwarnings(action=&#34;ignore&#34;, message=W_TOO_STRICT)

            selection_a = experiment.select(
                CONSTRAINT_MAXIMIZE.format(format_score(score)),
                models_num=1
            )
            selection_b = experiment.select(
                CONSTRAINT_GREATER_THAN.format(format_score(score), score_threshold),
                models_num=1
            )
            selection_c = experiment.select(
                CONSTRAINT_LESS_THAN.format(
                    format_init_parameter(init_parameter), init_parameter_threshold
                ),
                models_num=1
            )

        # TODO:
        # assert len(selection_a) == len(selection_b) == len(selection_c),\
        # были большие проблемы из-за того, что сейчас MAXIMIZE возвращает
        # несколько моделей с одинаковыми скорами,
        # а модели для теста генерятся как раз с кучей одинаковых скоров.
        # Я пытался это исправить быстро, но не получилось
        del selection_a

        assert len(selection_b) == len(selection_c),\
            &#39;Returns different number of models for different queries&#39;

    @pytest.mark.parametrize(
        &#39;score, threshold&#39;,
        [(SCORES[0], MIDDLE_SCORE)]
    )
    @pytest.mark.parametrize(
        &#39;query_template&#39;,
        [CONSTRAINT_LESS_THAN, CONSTRAINT_EQUALS_TO, CONSTRAINT_GREATER_THAN,
         CONSTRAINT_MAXIMIZE, CONSTRAINT_MINIMIZE]
    )
    @pytest.mark.parametrize(
        &#39;get_models_func&#39;,
        [get_models_with_two_scores_two_init_parameters,
         lambda: get_models_with_one_score(SCORES[0])]  # need to pass score, but call later
    )
    def test_select_by_score(self, score, threshold, query_template, get_models_func):
        # Need to find satisfying in test, because &#34;models&#34; is a class variable
        experiment = TestExperimentSelect.get_experiment(with_models=False)
        TestExperimentSelect.set_models(experiment, get_models_func())

        query = query_template.format(format_score(score), threshold)
        selection = experiment.select(query, models_num=1)

        filter_func = TestExperimentSelect.get_filter_for_score(
            query, score, threshold, experiment.models.values()
        )
        satisfying = list(filter(filter_func, experiment.models.values()))

        assert set(selection).issubset(set(satisfying)),\
            f&#39;Some models among selected don\&#39;t satisfy &#39; \
            f&#39;the query &#34;{query}&#34;&#39;

    @pytest.mark.parametrize(
        &#39;init_parameter, threshold&#39;,
        [(INIT_PARAMETERS[0], MIDDLE_INIT_PARAMETER)]
    )
    @pytest.mark.parametrize(
        &#39;query_template&#39;,
        [CONSTRAINT_LESS_THAN, CONSTRAINT_EQUALS_TO, CONSTRAINT_GREATER_THAN]
    )
    @pytest.mark.parametrize(
        &#39;get_models_func&#39;,
        [get_models_with_two_scores_two_init_parameters,
         lambda: get_models_with_one_init_parameter(INIT_PARAMETERS[0])]
    )
    def test_select_by_parameter(self, init_parameter, threshold, query_template, get_models_func):
        experiment = TestExperimentSelect.get_experiment(with_models=False)
        TestExperimentSelect.set_models(experiment, get_models_func())

        query = query_template.format(format_init_parameter(init_parameter), threshold)
        selection = experiment.select(query, models_num=1)

        filter_func = TestExperimentSelect.get_filter_for_init_parameter(
            query, init_parameter, threshold
        )
        satisfying = list(filter(filter_func, experiment.models.values()))

        assert set(selection).issubset(set(satisfying)), \
            f&#39;Some models among selected don\&#39;t satisfy &#39; \
            f&#39;the query &#34;{query}&#34;&#39;

    @pytest.mark.parametrize(
        &#39;constraint_a_template&#39;,
        [CONSTRAINT_LESS_THAN, CONSTRAINT_EQUALS_TO, CONSTRAINT_GREATER_THAN,
         CONSTRAINT_MAXIMIZE, CONSTRAINT_MINIMIZE]
    )
    @pytest.mark.parametrize(
        &#39;constraint_b_template&#39;,
        [CONSTRAINT_LESS_THAN, CONSTRAINT_EQUALS_TO, CONSTRAINT_GREATER_THAN,
         CONSTRAINT_MAXIMIZE, CONSTRAINT_MINIMIZE]
    )
    @pytest.mark.parametrize(
        &#39;score_a, threshold_a, score_b, threshold_b&#39;,
        [(SCORES[0], MIDDLE_SCORE, SCORES[1], MIDDLE_SCORE)]
    )
    def test_select_by_scores(
            self, constraint_a_template, constraint_b_template,
            score_a, threshold_a, score_b, threshold_b):

        experiment = TestExperimentSelect.get_experiment()

        constraint_a = constraint_a_template.format(format_score(score_a), threshold_a)
        constraint_b = constraint_b_template.format(format_score(score_b), threshold_b)
        query = combine_constraints(constraint_a, constraint_b)

        if ARROW_TO in constraint_a and ARROW_TO in constraint_b:
            # The case is not considered in this test
            return

        selection = experiment.select(query, models_num=1)

        filter_func_a = TestExperimentSelect.get_filter_for_score(
            constraint_a, score_a, threshold_a, experiment.models.values()
        )
        satisfying = list(filter(filter_func_a, experiment.models.values()))
        filter_func_b = TestExperimentSelect.get_filter_for_score(
            constraint_b, score_b, threshold_b, experiment.models.values()
        )
        satisfying = list(filter(filter_func_b, satisfying))

        assert set(selection).issubset(set(satisfying)), \
            f&#39;Some models among selected don\&#39;t satisfy &#39; \
            f&#39;the query &#34;{query}&#34;&#39;

    @pytest.mark.parametrize(
        &#39;constraint_a_template&#39;, [CONSTRAINT_MAXIMIZE, CONSTRAINT_MINIMIZE]
    )
    @pytest.mark.parametrize(
        &#39;constraint_b_template&#39;, [CONSTRAINT_MAXIMIZE, CONSTRAINT_MINIMIZE]
    )
    @pytest.mark.parametrize(
        &#39;score_a, score_b&#39;, [(SCORES[0], SCORES[1])]
    )
    def test_two_optimizations(
            self, constraint_a_template, constraint_b_template, score_a, score_b):

        experiment = TestExperimentSelect.get_experiment()

        optimization_a = constraint_a_template.format(format_score(score_a))
        optimization_b = constraint_b_template.format(format_score(score_b))
        query = combine_constraints(optimization_a, optimization_b)

        with pytest.raises(ValueError, match=&#34;Cannot process more than one target&#34;):
            _ = experiment.select(query, models_num=1)

    @pytest.mark.parametrize(
        &#39;constraint_a_template&#39;,
        [CONSTRAINT_LESS_THAN, CONSTRAINT_EQUALS_TO, CONSTRAINT_GREATER_THAN]
    )
    @pytest.mark.parametrize(
        &#39;constraint_b_template&#39;,
        [CONSTRAINT_LESS_THAN, CONSTRAINT_EQUALS_TO, CONSTRAINT_GREATER_THAN]
    )
    @pytest.mark.parametrize(
        &#39;init_parameter_a, threshold_a, init_parameter_b, threshold_b&#39;,
        [(INIT_PARAMETERS[0], MIDDLE_INIT_PARAMETER, INIT_PARAMETERS[1], MIDDLE_INIT_PARAMETER)]
    )
    def test_select_by_init_parameters(
            self, constraint_a_template, constraint_b_template,
            init_parameter_a, threshold_a, init_parameter_b, threshold_b):

        experiment = TestExperimentSelect.get_experiment()

        constraint_a = constraint_a_template.format(
            format_init_parameter(init_parameter_a), threshold_a)
        constraint_b = constraint_a_template.format(
            format_init_parameter(init_parameter_b), threshold_b)
        query = combine_constraints(constraint_a, constraint_b)
        selection = experiment.select(query, models_num=1)

        filter_func_a = TestExperimentSelect.get_filter_for_init_parameter(
            constraint_a, init_parameter_a, threshold_a
        )
        satisfying = list(filter(filter_func_a, experiment.models.values()))
        filter_func_b = TestExperimentSelect.get_filter_for_init_parameter(
            constraint_b, init_parameter_b, threshold_b
        )
        satisfying = list(filter(filter_func_b, satisfying))

        assert set(selection).issubset(set(satisfying)), \
            f&#39;Some models among selected don\&#39;t satisfy &#39; \
            f&#39;the query &#34;{query}&#34;&#39;

    @pytest.mark.parametrize(
        &#39;constraint_score_template&#39;,
        [CONSTRAINT_LESS_THAN, CONSTRAINT_EQUALS_TO, CONSTRAINT_GREATER_THAN,
         CONSTRAINT_MAXIMIZE, CONSTRAINT_MINIMIZE]
    )
    @pytest.mark.parametrize(
        &#39;constraint_init_parameter_template&#39;,
        [CONSTRAINT_LESS_THAN, CONSTRAINT_EQUALS_TO, CONSTRAINT_GREATER_THAN]
    )
    @pytest.mark.parametrize(
        &#39;score, threshold_score, init_parameter, threshold_init_parameter&#39;,
        [(SCORES[0], MIDDLE_SCORE, INIT_PARAMETERS[1], MIDDLE_INIT_PARAMETER)]
    )
    def test_select_by_score_and_init_parameter(
            self, constraint_score_template, constraint_init_parameter_template,
            score, threshold_score,
            init_parameter, threshold_init_parameter):

        experiment = TestExperimentSelect.get_experiment()

        constraint_score = constraint_score_template.format(
            format_score(score), threshold_score)
        constraint_init_parameter = constraint_init_parameter_template.format(
            format_init_parameter(init_parameter), threshold_init_parameter)
        query = combine_constraints(constraint_score, constraint_init_parameter)
        selection = experiment.select(query, models_num=1)

        filter_func_score = TestExperimentSelect.get_filter_for_score(
            constraint_score, score, threshold_score, experiment.models.values()
        )
        satisfying = list(filter(filter_func_score, experiment.models.values()))
        filter_func_init_parameter = TestExperimentSelect.get_filter_for_init_parameter(
            constraint_init_parameter, init_parameter, threshold_init_parameter
        )
        satisfying = list(filter(filter_func_init_parameter, satisfying))

        assert set(selection).issubset(set(satisfying)), \
            f&#39;Some models among selected don\&#39;t satisfy &#39; \
            f&#39;the query &#34;{query}&#34;&#39;

    def test_empty_level(self):
        level_with_models = 1
        level_without_models = 2

        experiment = TestExperimentSelect.get_experiment(with_models=False)
        TestExperimentSelect.set_models(
            experiment, [MockTopicModel(&#39;model&#39;, depth=level_with_models)]
        )

        with warnings.catch_warnings():
            # TODO: check that tests on warnings are here somewhere
            warnings.filterwarnings(action=&#34;ignore&#34;, message=W_TOO_STRICT)
            warnings.filterwarnings(action=&#34;ignore&#34;, message=W_NOT_ENOUGH_MODELS_FOR_CHOICE)
            warnings.filterwarnings(action=&#34;ignore&#34;, message=W_RETURN_FEWER_MODELS)

            selection = experiment.select(
                TestExperimentSelect.query_sample,
                models_num=1,
                level=level_without_models
            )

        assert all(m.depth == level_without_models for m in selection),\
            &#39;Some models have depth other than required&#39;
        assert len(selection) == 0,\
            &#39;Some models selected on level with no models&#39;

    @pytest.mark.parametrize(&#39;num_models&#39;, [1, 2])
    @pytest.mark.parametrize(&#39;difference_with_num_satisfying&#39;, [1, 0, -1])
    @pytest.mark.parametrize(&#39;total_num_models&#39;, [20])
    @pytest.mark.parametrize(
        &#39;score, target_value, other_value, query_template&#39;,
        [
            (SCORES[0], 100, 1, CONSTRAINT_EQUALS_TO.format(&#39;{0}&#39;, 100)),
            (SCORES[0], 100, 1, CONSTRAINT_MAXIMIZE),
            (SCORES[0], 100, 1000, CONSTRAINT_MINIMIZE),
            (SCORES[0], 100, 1, CONSTRAINT_GREATER_THAN.format(&#39;{0}&#39;, 1)),
            (SCORES[0], 100, 1000, CONSTRAINT_LESS_THAN.format(&#39;{0}&#39;, 1000))
        ]
    )
    def test_num_models(
            self, num_models, total_num_models, difference_with_num_satisfying,
            score, target_value, other_value, query_template):

        experiment = TestExperimentSelect.get_experiment(with_models=False)

        # Don&#39;t consider &#34;num_satisfying = 0&#34; here
        num_satisfying = max(num_models + difference_with_num_satisfying, 1)
        num_other = max(total_num_models - num_satisfying, 0)

        # TODO: investigate more thoroughly the case with START model
        #  (can in be chosen here, if not specify depth?)
        models_satisfying = [
            MockTopicModel(f&#39;model_satisfying_{i}&#39;, depth=1).set_score(score, [target_value])
            for i in range(num_satisfying)
        ]
        models_other = [
            MockTopicModel(f&#39;model_other_{i}&#39;, depth=1).set_score(score, [other_value])
            for i in range(num_other)
        ]

        TestExperimentSelect.set_models(experiment, models_satisfying + models_other)

        query = query_template.format(format_score(score))

        with warnings.catch_warnings():
            warnings.filterwarnings(action=&#34;ignore&#34;, message=W_RETURN_FEWER_MODELS)

            selection_first_time = experiment.select(query, models_num=num_models)
            selection_second_time = experiment.select(query, models_num=num_models)

        if ARROW_TO in query:
            expected_num_models = min(num_models, num_satisfying + num_other)
        else:
            expected_num_models = min(num_models, num_satisfying)

        assert len(selection_first_time) == expected_num_models,\
            f&#39;Wrong number of selected models on first select by&#34;{query}&#34;&#39;
        assert len(selection_second_time) == expected_num_models,\
            f&#39;Wrong number of selected models on second select by &#34;{query}&#34;&#39;
        assert selection_first_time == selection_second_time,\
            f&#39;First and second select() results not the same on &#34;{query}&#34;&#39;

    @pytest.mark.parametrize(&#39;with_models&#39;, [True, False])
    def test_blank_query(self, with_models):
        experiment = TestExperimentSelect.get_experiment(with_models=False)

        if not with_models:
            expected_num_models = 0
        else:
            TestExperimentSelect.set_models(
                experiment, [MockTopicModel(&#39;model_name&#39;).set_score(&#39;some_score&#39;, [1])]
            )

            expected_num_models = len([m for m in experiment.models.values() if m.depth == 1])

        selection = experiment.select(&#39;&#39;, level=1)

        assert len(selection) == expected_num_models, &#39;Some models selected&#39;

    @pytest.mark.parametrize(&#39;score, threshold&#39;, [(SCORES[0], MIDDLE_SCORE)])
    def test_whitespace(self, score, threshold):
        experiment = TestExperimentSelect.get_experiment()

        one_space = &#39; &#39;
        two_spaces = &#39;  &#39;
        tab = &#39;\t&#39;
        newline = &#39;\n&#39;

        query_one_space_template =\
            f&#39;{{0}}{one_space}{GREATER}{one_space}{{1}}&#39;
        query_two_spaces_template =\
            f&#39;{{0}}{two_spaces}{GREATER}{two_spaces}{{1}}&#39;
        query_tab_template =\
            f&#39;{{0}}{tab}{GREATER}{tab}{{1}}&#39;
        query_newline_template =\
            f&#39;{{0}}{newline}{GREATER}{newline}{{1}}&#39;
        query_space_at_the_beginning_template =\
            f&#39;{one_space}{{0}}{one_space}{GREATER}{one_space}{{1}}&#39;
        query_space_at_the_end_template =\
            f&#39;{{0}}{one_space}{GREATER}{one_space}{{1}}{one_space}&#39;

        selections = [
            experiment.select(q, models_num=1)
            for q in [
                query_one_space_template.format(format_score(score), threshold),
                query_two_spaces_template.format(format_score(score), threshold),
                query_tab_template.format(format_score(score), threshold),
                query_newline_template.format(format_score(score), threshold),
                query_space_at_the_beginning_template.format(format_score(score), threshold),
                query_space_at_the_end_template.format(format_score(score), threshold)
            ]
        ]

        assert all(s == selections[0] for s in selections[1:]), &#39;Some queries differ&#39;

    def test_wrong_case_in_constraints_connector(self):
        experiment = TestExperimentSelect.get_experiment()

        connector_in_wrong_case = AND.lower() if AND != AND.lower() else AND.upper()
        constraint_a = CONSTRAINT_EQUALS_TO.format(
            format_init_parameter(INIT_PARAMETERS[0]), MIDDLE_INIT_PARAMETER
        )
        constraint_b = CONSTRAINT_LESS_THAN.format(
            format_init_parameter(INIT_PARAMETERS[1]), MIDDLE_INIT_PARAMETER
        )
        query = combine_constraints(constraint_a, constraint_b, connector_in_wrong_case)

        with pytest.raises(ValueError):
            _ = experiment.select(query, models_num=1)

    @pytest.mark.parametrize(&#39;max_min&#39;, [MAX, MIN])
    def test_wrong_case_in_max_min(self, max_min):
        experiment = TestExperimentSelect.get_experiment()

        max_min_in_wrong_case = max_min.lower()\
            if max_min != max_min.lower()\
            else max_min.upper()
        query = f&#39;{format_score(SCORES[1])} {ARROW_TO} {max_min_in_wrong_case}&#39;

        with pytest.raises(ValueError):
            _ = experiment.select(query, models_num=1)

    @pytest.mark.parametrize(&#39;score&#39;, [SCORES[1]])
    def test_wrong_case_in_score(self, score):
        experiment = TestExperimentSelect.get_experiment()

        score_in_wrong_case = score.lower() if score != score.lower() else score.upper()
        query = CONSTRAINT_MAXIMIZE.format(format_score(score_in_wrong_case))

        with pytest.raises(ValueError):
            _ = experiment.select(query, models_num=1)

    @pytest.mark.parametrize(&#39;init_parameter&#39;, [INIT_PARAMETERS[1]])
    def test_wrong_case_in_parameter(self, init_parameter):
        experiment = TestExperimentSelect.get_experiment()

        init_parameter_in_wrong_case = init_parameter.lower()\
            if init_parameter != init_parameter.lower()\
            else init_parameter.upper()
        query = CONSTRAINT_EQUALS_TO.format(
            format_init_parameter(init_parameter_in_wrong_case), MIDDLE_INIT_PARAMETER
        )

        with pytest.raises(ValueError):
            _ = experiment.select(query, models_num=1)

    @pytest.mark.parametrize(
        &#39;init_parameter, threshold&#39;, [(INIT_PARAMETERS[1], MIDDLE_INIT_PARAMETER)]
    )
    @pytest.mark.parametrize(
        &#39;wrong_prefix&#39;, [&#39;model&#39;, &#39;model,&#39;, &#39;mdel.&#39;, &#39;Model.&#39;, &#39;&#39;]
    )
    def test_wrong_parameter_prefix(self, init_parameter, threshold, wrong_prefix):
        experiment = TestExperimentSelect.get_experiment()

        query = CONSTRAINT_EQUALS_TO.format(wrong_prefix + init_parameter, threshold)

        with pytest.raises(ValueError):
            _ = experiment.select(query, models_num=1)

    @pytest.mark.parametrize(
        &#39;constraint_a&#39;,
        [CONSTRAINT_EQUALS_TO.format(format_score(SCORES[0]), MIDDLE_SCORE)]
    )
    @pytest.mark.parametrize(
        &#39;constraint_b&#39;,
        [CONSTRAINT_LESS_THAN.format(
            format_init_parameter(INIT_PARAMETERS[1]), MIDDLE_INIT_PARAMETER)]
    )
    @pytest.mark.parametrize(
        &#39;wrong_connector&#39;,
        [AND + AND, AND + &#39; &#39; + AND, &#39;or&#39;, &#39;model&#39;, INIT_PARAMETER_PREFIX,
         ARROW_TO, GREATER, LESS, EQUALS, &#39;&#39;]
    )
    def test_wrong_constraints_connector(self, constraint_a, constraint_b, wrong_connector):
        experiment = TestExperimentSelect.get_experiment()

        query = combine_constraints(constraint_a, constraint_b, wrong_connector)

        with pytest.raises(ValueError):
            _ = experiment.select(query, models_num=1)

    @pytest.mark.parametrize(
        &#39;query_template&#39;, [f&#39;{format_score(SCORES[0])} {{0}} {MIDDLE_SCORE}&#39;]
    )
    @pytest.mark.parametrize(
        &#39;wrong_sign&#39;,
        [
            &#39;&gt;=&#39;, &#39;&lt;=&#39;, &#39;&lt;&gt;&#39;, &#39;&lt;&lt;&#39;, &#39;&gt;&gt;&#39;,  # inequality
            &#39;==&#39;, &#39;===&#39;, &#39;equals&#39;, &#39;equal&#39;, &#39;is&#39;,  # equality
            &#39;=&gt;&#39;, &#39;=&lt;&#39;  # others
        ]
    )
    def test_wrong_comparison_sign(self, query_template, wrong_sign):
        experiment = TestExperimentSelect.get_experiment()

        query = query_template.format(wrong_sign)

        with pytest.raises(ValueError):
            _ = experiment.select(query, models_num=1)

    @pytest.mark.parametrize(
        &#39;query_template&#39;, [f&#39;{format_score(SCORES[0])} {LESS} {{0}}&#39;]
    )
    @pytest.mark.parametrize(
        &#39;not_a_number&#39;, [&#39;NUMBER&#39;, &#39;number&#39;, LESS, GREATER, EQUALS, ARROW_TO, MAX, MIN, AND]
    )
    def test_not_a_number(self, query_template, not_a_number):
        experiment = TestExperimentSelect.get_experiment()

        query = query_template.format(not_a_number)

        with pytest.raises(ValueError):
            _ = experiment.select(query, models_num=1)

    @pytest.mark.parametrize(
        &#39;query_template&#39;, [f&#39;{format_score(SCORES[0])} {{0}} {MIN}&#39;]
    )
    @pytest.mark.parametrize(
        &#39;wrong_arrow&#39;,
        [&#39;&lt;-&#39;, &#39;-&lt;&#39;, &#39;&gt;-&#39;, &#39;=&gt;&#39;, &#39;&lt;=&#39;, &#39;--&gt;&#39;, &#39;to&#39;, &#39;→&#39;, &#39;←&#39;, &#39;-&gt;&gt;&#39;, &#39;-&gt;&lt;&#39;,
         EQUALS, GREATER, LESS, &#39;&#39;]
    )
    def test_wrong_arrow(self, query_template, wrong_arrow):
        experiment = TestExperimentSelect.get_experiment()

        query = query_template.format(wrong_arrow)

        with pytest.raises(ValueError):
            _ = experiment.select(query, models_num=1)

    @pytest.mark.parametrize(
        &#39;query_template&#39;, [f&#39;{format_score(SCORES[1])} {ARROW_TO} {{0}}&#39;]
    )
    @pytest.mark.parametrize(
        &#39;wrong_max_min&#39;,
        [1, 1.2, &#39;inf&#39;, &#39;+inf&#39;, &#39;maximize&#39;,
         MAX + MAX, MAX + MIN, ARROW_TO, GREATER, LESS, EQUALS, &#39;&#39;]
    )
    def test_wrong_max_min(self, query_template, wrong_max_min):
        experiment = TestExperimentSelect.get_experiment()

        query = query_template.format(wrong_max_min)

        with pytest.raises(ValueError):
            _ = experiment.select(query, models_num=1)

    @pytest.mark.parametrize(
        &#39;constraint&#39;,
        [
            CONSTRAINT_GREATER_THAN.format(format_score(SCORES[1]), MIDDLE_SCORE),
            CONSTRAINT_EQUALS_TO.format(
                format_init_parameter(INIT_PARAMETERS[1]), MIDDLE_INIT_PARAMETER),
            CONSTRAINT_MINIMIZE.format(format_score(SCORES[0]))
        ]
    )
    def test_duplicate_constraint(self, constraint):
        experiment = TestExperimentSelect.get_experiment()

        query_one_constraint = constraint
        query_duplicate_constraints = combine_constraints(constraint, constraint)

        selection_with_one = experiment.select(query_one_constraint, models_num=1)
        selection_with_duplicate = experiment.select(query_duplicate_constraints, models_num=1)

        assert selection_with_one == selection_with_duplicate,\
            &#39;Duplicate constraints changed query result&#39;

    @pytest.mark.parametrize(
        &#39;constraint_to_duplicate&#39;,
        [
            CONSTRAINT_LESS_THAN.format(format_score(SCORES[0]), MIDDLE_SCORE),
            CONSTRAINT_EQUALS_TO.format(
                format_init_parameter(INIT_PARAMETERS[0]), MIDDLE_INIT_PARAMETER),
            CONSTRAINT_MAXIMIZE.format(format_score(SCORES[0]))
        ]
    )
    @pytest.mark.parametrize(
        &#39;constraint_other&#39;,
        [
            CONSTRAINT_GREATER_THAN.format(format_score(SCORES[1]), MIDDLE_SCORE),
            CONSTRAINT_EQUALS_TO.format(
                format_init_parameter(INIT_PARAMETERS[1]), MIDDLE_INIT_PARAMETER),
            CONSTRAINT_MINIMIZE.format(format_score(SCORES[1]))
        ]
    )
    def test_duplicate_constraint_after_another(self, constraint_to_duplicate, constraint_other):
        if ARROW_TO in constraint_to_duplicate and ARROW_TO in constraint_other:
            # Several &#34;-&gt; max/min&#34; not allowed
            return

        experiment = TestExperimentSelect.get_experiment()

        query_constraint_and_other = combine_constraints(constraint_to_duplicate, constraint_other)
        query_duplicate_constraints_and_other = combine_constraints(
            constraint_to_duplicate, constraint_other, constraint_to_duplicate)

        selection_with_one = experiment.select(
            query_constraint_and_other, models_num=1
        )
        selection_with_duplicate = experiment.select(
            query_duplicate_constraints_and_other, models_num=1
        )

        assert selection_with_one == selection_with_duplicate,\
            &#39;Other constraint changed query result&#39;

    @pytest.mark.parametrize(
        &#39;parameter, soft_constraint_template, hard_constraint_template&#39;,
        [
            (
                format_score(SCORES[0]),
                CONSTRAINT_GREATER_THAN.format(&#39;{0}&#39;, MIN_SCORE),
                CONSTRAINT_GREATER_THAN.format(&#39;{0}&#39;, MIDDLE_SCORE)
            ),
            (
                format_init_parameter(INIT_PARAMETERS[0]),
                CONSTRAINT_GREATER_THAN.format(&#39;{0}&#39;, MIN_INIT_PARAMETER),
                CONSTRAINT_GREATER_THAN.format(&#39;{0}&#39;, MIDDLE_INIT_PARAMETER)
            ),
            (
                format_score(SCORES[1]),
                CONSTRAINT_GREATER_THAN.format(&#39;{0}&#39;, MIDDLE_SCORE),
                CONSTRAINT_MAXIMIZE + &#34; COLLECT 1&#34;
            )
        ]
    )
    def test_constraints_on_same_attribute(
            self, parameter, soft_constraint_template, hard_constraint_template):

        experiment = TestExperimentSelect.get_experiment()

        soft_constraint = soft_constraint_template.format(parameter)
        hard_constraint = hard_constraint_template.format(parameter)

        soft_query = soft_constraint
        hard_query = combine_constraints(soft_constraint, hard_constraint)

        with warnings.catch_warnings():
            warnings.filterwarnings(action=&#34;ignore&#34;, message=W_RETURN_FEWER_MODELS)

            soft_selection = experiment.select(soft_query)
            hard_selection = experiment.select(hard_query)

        assert len(hard_selection) &lt; len(soft_selection),\
            f&#39;Hard constraint &#34;{hard_query}&#34; not proper subset of soft one &#34;{soft_query}&#34;&#39;
        assert set(hard_selection).issubset(set(soft_selection)),\
            f&#39;Hard constraint &#34;{hard_query}&#34; not subset of soft one &#34;{soft_query}&#34;&#39;

    @pytest.mark.parametrize(
        &#39;score, threshold, constraint_template, optimization_template&#39;,
        [
            # constraint affects
            (SCORES[0], MIDDLE_SCORE, CONSTRAINT_LESS_THAN, CONSTRAINT_MAXIMIZE),
            # constraint affects
            (SCORES[1], MIDDLE_SCORE, CONSTRAINT_GREATER_THAN, CONSTRAINT_MINIMIZE),
            # constraint affects
            (SCORES[1], MIDDLE_SCORE, CONSTRAINT_EQUALS_TO, CONSTRAINT_MINIMIZE),
            # constraint doesn&#39;t affect
            (SCORES[1], MIDDLE_SCORE, CONSTRAINT_LESS_THAN, CONSTRAINT_MINIMIZE),
            # constraint doesn&#39;t affect
            (SCORES[1], MIDDLE_SCORE, CONSTRAINT_GREATER_THAN, CONSTRAINT_MAXIMIZE)
        ]
    )
    def test_constrained_optimization(
            self, score, threshold, constraint_template, optimization_template):

        experiment = TestExperimentSelect.get_experiment()

        constraint = constraint_template.format(format_score(score), threshold)
        optimization = optimization_template.format(format_score(score))
        query = combine_constraints(constraint, optimization)
        selection = experiment.select(query, models_num=1)

        filter_func_constraint = TestExperimentSelect.get_filter_for_score(
            constraint, score, threshold, experiment.models.values()
        )
        satisfying = list(filter(filter_func_constraint, experiment.models.values()))
        filter_func_optimization = TestExperimentSelect.get_filter_for_score(
            optimization, score, None, satisfying
        )
        satisfying = list(filter(filter_func_optimization, satisfying))

        assert set(selection).issubset(set(satisfying)),\
            f&#39;Some selected models don\&#39;t satisfy the query &#34;{query}&#34;&#39;

    @pytest.mark.parametrize(
        &#39;parameter, threshold&#39;,
        [
            (format_score(SCORES[1]), MIDDLE_SCORE),
            (format_init_parameter(INIT_PARAMETERS[1]), MIDDLE_INIT_PARAMETER)
        ]
    )
    @pytest.mark.parametrize(&#39;signs&#39;, combinations([LESS, GREATER, EQUALS], 2))
    def test_constraints_on_same_attribute_contradict(self, parameter, threshold, signs):
        experiment = TestExperimentSelect.get_experiment()

        constraint_template = f&#39;{{0}} {{1}} {{2}}&#39;  # noqa: F541
        query = combine_constraints(
            *[constraint_template.format(parameter, sign, threshold)
              for sign in signs]
        )

        with warnings.catch_warnings():
            warnings.filterwarnings(action=&#34;ignore&#34;, message=W_TOO_STRICT)
            warnings.filterwarnings(action=&#34;ignore&#34;, message=W_RETURN_FEWER_MODELS)

            selection = experiment.select(query, models_num=1)

        assert len(selection) == 0, &#39;Some models selected&#39;

    @pytest.mark.parametrize(&#39;score&#39;, [SCORES[0]])
    def test_error_optimizations_contradict(self, score):
        experiment = TestExperimentSelect.get_experiment()

        optimization_max = CONSTRAINT_MAXIMIZE.format(score)
        optimization_min = CONSTRAINT_MINIMIZE.format(score)
        query = combine_constraints(optimization_max, optimization_min)

        with pytest.raises(ValueError):
            _ = experiment.select(query, models_num=1)

    @pytest.mark.parametrize(
        &#39;parameter, opposite_constraints&#39;,
        [
            (
                format_score(SCORES[0]),
                [CONSTRAINT_LESS_THAN.format(&#39;{0}&#39;, MIDDLE_SCORE),
                 CONSTRAINT_GREATER_THAN.format(&#39;{0}&#39;, MIDDLE_SCORE)]
            ),
            (
                format_init_parameter(INIT_PARAMETERS[1]),
                [CONSTRAINT_EQUALS_TO.format(&#39;{0}&#39;, MIDDLE_INIT_PARAMETER),
                 CONSTRAINT_LESS_THAN.format(&#39;{0}&#39;, MIDDLE_INIT_PARAMETER)]
            )
        ]
    )
    def test_warning_constraints_contradict(self, parameter, opposite_constraints):
        experiment = TestExperimentSelect.get_experiment()

        query = combine_constraints(*[c.format(parameter) for c in opposite_constraints])

        with pytest.warns(UserWarning):
            _ = experiment.select(query, models_num=1)

    @pytest.mark.parametrize(
        &#39;query_template&#39;,
        [
            CONSTRAINT_GREATER_THAN.format(&#39;{0}&#39;, 0),
            CONSTRAINT_LESS_THAN.format(&#39;{0}&#39;, 0),
            CONSTRAINT_EQUALS_TO.format(&#39;{0}&#39;, 0),
            CONSTRAINT_MAXIMIZE.format(&#39;{0}&#39;),
            CONSTRAINT_MINIMIZE.format(&#39;{0}&#39;)
        ]
    )
    @pytest.mark.parametrize(
        &#39;get_models_func&#39;,
        [
            # Pass func-s here, not func()-s, because it seems pytest does not like func()-s
            # (test running slows greatly and may even lead to system freeze)
            get_models_with_two_scores_two_init_parameters,
            get_models_with_one_init_parameter
        ]
    )
    def test_unknown_score(self, query_template, get_models_func):
        experiment = TestExperimentSelect.get_experiment(with_models=False)
        TestExperimentSelect.set_models(experiment, get_models_func())

        query = query_template.format(format_score(&#39;UNKNOWN_SCORE&#39;))

        with pytest.raises(ValueError):
            _ = experiment.select(query, models_num=1)

    @pytest.mark.parametrize(
        &#39;query_template&#39;,
        [
            CONSTRAINT_GREATER_THAN.format(&#39;{0}&#39;, 0),
            CONSTRAINT_LESS_THAN.format(&#39;{0}&#39;, 0),
            CONSTRAINT_EQUALS_TO.format(&#39;{0}&#39;, 0),
        ]
    )
    @pytest.mark.parametrize(
        &#39;get_models_func&#39;,
        [
            get_models_with_two_scores_two_init_parameters,
            get_models_with_one_score
        ]
    )
    def test_unknown_init_parameter(self, query_template, get_models_func):
        experiment = TestExperimentSelect.get_experiment(with_models=False)
        TestExperimentSelect.set_models(experiment, get_models_func())

        query = query_template.format(format_init_parameter(&#39;UNKNOWN_INIT_PARAMETER&#39;))

        with pytest.raises(ValueError):
            _ = experiment.select(query, models_num=1)

    @pytest.mark.parametrize(
        &#39;constraints&#39;,
        [
            (
                # one score, one init parameter
                CONSTRAINT_MINIMIZE.format(
                    format_score(SCORES[0])),
                CONSTRAINT_EQUALS_TO.format(
                    format_init_parameter(INIT_PARAMETERS[1]), MIDDLE_INIT_PARAMETER)
            ),
            (
                # two init parameters
                CONSTRAINT_GREATER_THAN.format(
                    format_init_parameter(INIT_PARAMETERS[0]), MIDDLE_INIT_PARAMETER),
                CONSTRAINT_EQUALS_TO.format(
                    format_init_parameter(INIT_PARAMETERS[1]), MIDDLE_INIT_PARAMETER)
            ),
            (
                # two scores
                CONSTRAINT_GREATER_THAN.format(
                    format_score(SCORES[0]), MIDDLE_SCORE),
                CONSTRAINT_LESS_THAN.format(
                    format_score(SCORES[1]), MIDDLE_SCORE)
            ),
            (
                # two scores, one init parameter
                CONSTRAINT_MINIMIZE.format(
                    format_score(SCORES[0])),
                CONSTRAINT_EQUALS_TO.format(
                    format_init_parameter(INIT_PARAMETERS[1]), MIDDLE_INIT_PARAMETER),
                CONSTRAINT_GREATER_THAN.format(
                    format_score(SCORES[1]), MIDDLE_SCORE)
            ),
            (
                # one score, two init parameters
                CONSTRAINT_GREATER_THAN.format(
                    format_score(SCORES[1]), MIDDLE_SCORE),
                CONSTRAINT_GREATER_THAN.format(
                    format_init_parameter(INIT_PARAMETERS[0]), MIDDLE_INIT_PARAMETER),
                CONSTRAINT_LESS_THAN.format(
                    format_init_parameter(INIT_PARAMETERS[1]), MIDDLE_INIT_PARAMETER)
            )
        ]
    )
    def test_change_order(self, constraints):
        experiment = TestExperimentSelect.get_experiment()

        query_ab = combine_constraints(*constraints)
        query_ba = combine_constraints(*constraints[::-1])

        selection_ab = experiment.select(query_ab, models_num=1)
        selection_ba = experiment.select(query_ba, models_num=1)

        assert selection_ab == selection_ba,\
            &#39;Different select() results if change order of constraints&#39;

    @pytest.mark.parametrize(
        &#39;query&#39;,
        [
            CONSTRAINT_LESS_THAN.format(format_score(SCORES[0]), MIDDLE_SCORE),
            CONSTRAINT_EQUALS_TO.format(
                format_init_parameter(INIT_PARAMETERS[0]), MIDDLE_INIT_PARAMETER),
            CONSTRAINT_MAXIMIZE.format(format_score(SCORES[1]))
        ]
    )
    def test_select_several_times(self, query):
        experiment = TestExperimentSelect.get_experiment()

        selection_a = experiment.select(query, models_num=1)
        selection_b = experiment.select(query, models_num=1)
        selection_c = experiment.select(query, models_num=1)

        assert selection_a == selection_b, &#39;Different select results after on second call&#39;
        assert selection_b == selection_c, &#39;Different select results after on third call&#39;

    @pytest.mark.parametrize(
        &#39;query_template&#39;, [CONSTRAINT_LESS_THAN]
    )
    @pytest.mark.parametrize(
        &#39;init_parameter, threshold_satisfying_all&#39;, [(INIT_PARAMETERS[0], MAX_INIT_PARAMETER + 1)]
    )
    @pytest.mark.parametrize(
        &#39;num_models&#39;, [0, 1, 2, 5, &#39;minus-one&#39;, &#39;equals&#39;, &#39;plus-one&#39;, &#39;very-big&#39;]
    )
    def test_num_and_models(
            self, query_template, init_parameter, threshold_satisfying_all, num_models):

        experiment = TestExperimentSelect.get_experiment()

        total_num_models = len(experiment.models)

        if isinstance(num_models, int):
            pass
        elif num_models == &#39;minus-one&#39;:
            num_models = total_num_models - 1
        elif num_models == &#39;equals&#39;:
            num_models = total_num_models
        elif num_models == &#39;plus-one&#39;:
            num_models = total_num_models + 1
        elif num_models == &#39;very-big&#39;:
            num_models = 100 * total_num_models
        else:
            raise ValueError()

        query_satisfying_all = query_template.format(
            format_init_parameter(init_parameter), threshold_satisfying_all)

        with warnings.catch_warnings():
            warnings.filterwarnings(action=&#34;ignore&#34;, message=W_RETURN_FEWER_MODELS)

            selection = experiment.select(query_satisfying_all, num_models, level=1)

        filter_func = TestExperimentSelect.get_filter_for_init_parameter(
            query_satisfying_all, init_parameter, threshold_satisfying_all
        )
        satisfying = list(filter(
            filter_func,
            [m for m in experiment.models.values() if m.depth == 1]
        ))

        if len(satisfying) &gt; total_num_models:
            raise RuntimeError(&#39;Satisfying models more than all models&#39;)

        assert len(selection) &lt;= total_num_models, &#39;Select more models than available&#39;

        if len(selection) &gt; num_models:
            assert False,\
                f&#39;Return more models than required: &#34;{len(selection)}&#34; &gt; &#34;{num_models}&#34;. &#39; \
                f&#39;Total number of models satisfying the query: &#34;{len(satisfying)}&#34;&#39;
        elif len(selection) &lt; num_models:
            assert set(selection) == set(satisfying),\
                &#39;Return fewer models, but they are not the ones that satisfy the condition&#39;
        else:
            assert set(selection).issubset(set(satisfying)),\
                &#39;Return models number as requested, &#39; \
                &#39;but not all returned models satisfy the constraint&#39;

    @pytest.mark.parametrize(
        &#39;query_satisfying_all&#39;,
        [CONSTRAINT_LESS_THAN.format(
            format_init_parameter(INIT_PARAMETERS[0]), MAX_INIT_PARAMETER + 1)]
    )
    def test_warning_fewer_than_requested(self, query_satisfying_all):
        experiment = TestExperimentSelect.get_experiment()

        total_num_models = len(experiment.models)
        num_models = 100 * total_num_models

        with pytest.warns(UserWarning):
            _ = experiment.select(query_satisfying_all, num_models)

    def test_select_blank_start_model_by_score(self):
        experiment = TestExperimentSelect.get_experiment(with_models=False)

        with pytest.warns(UserWarning):
            selection = experiment.select(
                CONSTRAINT_MAXIMIZE.format(format_score(SCORES[0])),
                level=0
            )

        assert len(selection) == 0, &#39;Some models selected&#39;

    def test_select_blank_start_model_by_defined_init_parameter(self):
        experiment = TestExperimentSelect.get_experiment(with_models=False)
        big_number = 10 ** 9

        selection = experiment.select(
            CONSTRAINT_LESS_THAN.format(
                format_init_parameter(DEFINED_START_INIT_PARAMETER), big_number
            ),
            level=0
        )

        assert len(selection) == 1, &#39;Wrong selection size&#39;
        assert selection[0].depth == 0, f&#39;Wrong model selected: with depth \&#34;{selection[0].depth}\&#34;&#39;

    def test_select_blank_start_model_by_undefined_init_parameter(self):
        experiment = TestExperimentSelect.get_experiment(with_models=False)
        init_parameter = list(set(INIT_PARAMETERS).difference([DEFINED_START_INIT_PARAMETER]))[0]
        some_value = 0

        with pytest.warns(UserWarning):
            selection = experiment.select(
                CONSTRAINT_LESS_THAN.format(
                    format_init_parameter(init_parameter), some_value
                ),
                level=0
            )

        assert len(selection) == 0, &#39;Some models selected&#39;</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.current_experiment_id"><code class="name">var <span class="ident">current_experiment_id</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.experiments_folder"><code class="name">var <span class="ident">experiments_folder</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.query_sample"><code class="name">var <span class="ident">query_sample</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.get_experiment"><code class="name flex">
<span>def <span class="ident">get_experiment</span></span>(<span>with_models=True)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.get_filter_for_init_parameter"><code class="name flex">
<span>def <span class="ident">get_filter_for_init_parameter</span></span>(<span>query, parameter, threshold)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.get_filter_for_score"><code class="name flex">
<span>def <span class="ident">get_filter_for_score</span></span>(<span>query, score, threshold, models)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.set_models"><code class="name flex">
<span>def <span class="ident">set_models</span></span>(<span>experiment, models)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.setup_class"><code class="name flex">
<span>def <span class="ident">setup_class</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.teardown_class"><code class="name flex">
<span>def <span class="ident">teardown_class</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_blank_query"><code class="name flex">
<span>def <span class="ident">test_blank_query</span></span>(<span>self, with_models)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_change_order"><code class="name flex">
<span>def <span class="ident">test_change_order</span></span>(<span>self, constraints)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_constrained_optimization"><code class="name flex">
<span>def <span class="ident">test_constrained_optimization</span></span>(<span>self, score, threshold, constraint_template, optimization_template)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_constraints_on_same_attribute"><code class="name flex">
<span>def <span class="ident">test_constraints_on_same_attribute</span></span>(<span>self, parameter, soft_constraint_template, hard_constraint_template)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_constraints_on_same_attribute_contradict"><code class="name flex">
<span>def <span class="ident">test_constraints_on_same_attribute_contradict</span></span>(<span>self, parameter, threshold, signs)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_default_level"><code class="name flex">
<span>def <span class="ident">test_default_level</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_default_num_models"><code class="name flex">
<span>def <span class="ident">test_default_num_models</span></span>(<span>self, score, init_parameter, score_threshold, init_parameter_threshold)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_duplicate_constraint"><code class="name flex">
<span>def <span class="ident">test_duplicate_constraint</span></span>(<span>self, constraint)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_duplicate_constraint_after_another"><code class="name flex">
<span>def <span class="ident">test_duplicate_constraint_after_another</span></span>(<span>self, constraint_to_duplicate, constraint_other)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_empty_level"><code class="name flex">
<span>def <span class="ident">test_empty_level</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_error_optimizations_contradict"><code class="name flex">
<span>def <span class="ident">test_error_optimizations_contradict</span></span>(<span>self, score)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_invalid_level_with_models"><code class="name flex">
<span>def <span class="ident">test_invalid_level_with_models</span></span>(<span>self, level)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_invalid_level_without_models"><code class="name flex">
<span>def <span class="ident">test_invalid_level_without_models</span></span>(<span>self, level)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_invalid_num_models_with_models"><code class="name flex">
<span>def <span class="ident">test_invalid_num_models_with_models</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_not_a_number"><code class="name flex">
<span>def <span class="ident">test_not_a_number</span></span>(<span>self, query_template, not_a_number)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_num_and_models"><code class="name flex">
<span>def <span class="ident">test_num_and_models</span></span>(<span>self, query_template, init_parameter, threshold_satisfying_all, num_models)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_num_models"><code class="name flex">
<span>def <span class="ident">test_num_models</span></span>(<span>self, num_models, total_num_models, difference_with_num_satisfying, score, target_value, other_value, query_template)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_select_blank_start_model_by_defined_init_parameter"><code class="name flex">
<span>def <span class="ident">test_select_blank_start_model_by_defined_init_parameter</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_select_blank_start_model_by_score"><code class="name flex">
<span>def <span class="ident">test_select_blank_start_model_by_score</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_select_blank_start_model_by_undefined_init_parameter"><code class="name flex">
<span>def <span class="ident">test_select_blank_start_model_by_undefined_init_parameter</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_select_by_init_parameters"><code class="name flex">
<span>def <span class="ident">test_select_by_init_parameters</span></span>(<span>self, constraint_a_template, constraint_b_template, init_parameter_a, threshold_a, init_parameter_b, threshold_b)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_select_by_parameter"><code class="name flex">
<span>def <span class="ident">test_select_by_parameter</span></span>(<span>self, init_parameter, threshold, query_template, get_models_func)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_select_by_score"><code class="name flex">
<span>def <span class="ident">test_select_by_score</span></span>(<span>self, score, threshold, query_template, get_models_func)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_select_by_score_and_init_parameter"><code class="name flex">
<span>def <span class="ident">test_select_by_score_and_init_parameter</span></span>(<span>self, constraint_score_template, constraint_init_parameter_template, score, threshold_score, init_parameter, threshold_init_parameter)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_select_by_scores"><code class="name flex">
<span>def <span class="ident">test_select_by_scores</span></span>(<span>self, constraint_a_template, constraint_b_template, score_a, threshold_a, score_b, threshold_b)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_select_several_times"><code class="name flex">
<span>def <span class="ident">test_select_several_times</span></span>(<span>self, query)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_two_optimizations"><code class="name flex">
<span>def <span class="ident">test_two_optimizations</span></span>(<span>self, constraint_a_template, constraint_b_template, score_a, score_b)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_unknown_init_parameter"><code class="name flex">
<span>def <span class="ident">test_unknown_init_parameter</span></span>(<span>self, query_template, get_models_func)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_unknown_score"><code class="name flex">
<span>def <span class="ident">test_unknown_score</span></span>(<span>self, query_template, get_models_func)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_warning_constraints_contradict"><code class="name flex">
<span>def <span class="ident">test_warning_constraints_contradict</span></span>(<span>self, parameter, opposite_constraints)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_warning_fewer_than_requested"><code class="name flex">
<span>def <span class="ident">test_warning_fewer_than_requested</span></span>(<span>self, query_satisfying_all)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_whitespace"><code class="name flex">
<span>def <span class="ident">test_whitespace</span></span>(<span>self, score, threshold)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_wrong_arrow"><code class="name flex">
<span>def <span class="ident">test_wrong_arrow</span></span>(<span>self, query_template, wrong_arrow)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_wrong_case_in_constraints_connector"><code class="name flex">
<span>def <span class="ident">test_wrong_case_in_constraints_connector</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_wrong_case_in_max_min"><code class="name flex">
<span>def <span class="ident">test_wrong_case_in_max_min</span></span>(<span>self, max_min)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_wrong_case_in_parameter"><code class="name flex">
<span>def <span class="ident">test_wrong_case_in_parameter</span></span>(<span>self, init_parameter)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_wrong_case_in_score"><code class="name flex">
<span>def <span class="ident">test_wrong_case_in_score</span></span>(<span>self, score)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_wrong_comparison_sign"><code class="name flex">
<span>def <span class="ident">test_wrong_comparison_sign</span></span>(<span>self, query_template, wrong_sign)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_wrong_constraints_connector"><code class="name flex">
<span>def <span class="ident">test_wrong_constraints_connector</span></span>(<span>self, constraint_a, constraint_b, wrong_connector)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_wrong_max_min"><code class="name flex">
<span>def <span class="ident">test_wrong_max_min</span></span>(<span>self, query_template, wrong_max_min)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_wrong_num_models"><code class="name flex">
<span>def <span class="ident">test_wrong_num_models</span></span>(<span>self, with_models)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_wrong_parameter_prefix"><code class="name flex">
<span>def <span class="ident">test_wrong_parameter_prefix</span></span>(<span>self, init_parameter, threshold, wrong_prefix)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="topicnet.tests.test_experiment_select.TestExperimentSelect.test_zero_num_models"><code class="name flex">
<span>def <span class="ident">test_zero_num_models</span></span>(<span>self, with_models)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="topicnet.tests" href="index.html">topicnet.tests</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="topicnet.tests.test_experiment_select.combine_constraints" href="#topicnet.tests.test_experiment_select.combine_constraints">combine_constraints</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.format_init_parameter" href="#topicnet.tests.test_experiment_select.format_init_parameter">format_init_parameter</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.format_score" href="#topicnet.tests.test_experiment_select.format_score">format_score</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.get_models" href="#topicnet.tests.test_experiment_select.get_models">get_models</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.get_models_with_one_init_parameter" href="#topicnet.tests.test_experiment_select.get_models_with_one_init_parameter">get_models_with_one_init_parameter</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.get_models_with_one_score" href="#topicnet.tests.test_experiment_select.get_models_with_one_score">get_models_with_one_score</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.get_models_with_two_scores_two_init_parameters" href="#topicnet.tests.test_experiment_select.get_models_with_two_scores_two_init_parameters">get_models_with_two_scores_two_init_parameters</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="topicnet.tests.test_experiment_select.MockTopicModel" href="#topicnet.tests.test_experiment_select.MockTopicModel">MockTopicModel</a></code></h4>
<ul class="">
<li><code><a title="topicnet.tests.test_experiment_select.MockTopicModel.generate_specified_models" href="#topicnet.tests.test_experiment_select.MockTopicModel.generate_specified_models">generate_specified_models</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.MockTopicModel.get_init_parameters" href="#topicnet.tests.test_experiment_select.MockTopicModel.get_init_parameters">get_init_parameters</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.MockTopicModel.get_start_model" href="#topicnet.tests.test_experiment_select.MockTopicModel.get_start_model">get_start_model</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.MockTopicModel.init_parameters" href="#topicnet.tests.test_experiment_select.MockTopicModel.init_parameters">init_parameters</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.MockTopicModel.name" href="#topicnet.tests.test_experiment_select.MockTopicModel.name">name</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.MockTopicModel.set_init_parameter" href="#topicnet.tests.test_experiment_select.MockTopicModel.set_init_parameter">set_init_parameter</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.MockTopicModel.set_score" href="#topicnet.tests.test_experiment_select.MockTopicModel.set_score">set_score</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect" href="#topicnet.tests.test_experiment_select.TestExperimentSelect">TestExperimentSelect</a></code></h4>
<ul class="">
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.current_experiment_id" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.current_experiment_id">current_experiment_id</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.experiments_folder" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.experiments_folder">experiments_folder</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.get_experiment" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.get_experiment">get_experiment</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.get_filter_for_init_parameter" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.get_filter_for_init_parameter">get_filter_for_init_parameter</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.get_filter_for_score" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.get_filter_for_score">get_filter_for_score</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.query_sample" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.query_sample">query_sample</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.set_models" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.set_models">set_models</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.setup_class" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.setup_class">setup_class</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.teardown_class" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.teardown_class">teardown_class</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_blank_query" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_blank_query">test_blank_query</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_change_order" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_change_order">test_change_order</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_constrained_optimization" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_constrained_optimization">test_constrained_optimization</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_constraints_on_same_attribute" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_constraints_on_same_attribute">test_constraints_on_same_attribute</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_constraints_on_same_attribute_contradict" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_constraints_on_same_attribute_contradict">test_constraints_on_same_attribute_contradict</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_default_level" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_default_level">test_default_level</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_default_num_models" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_default_num_models">test_default_num_models</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_duplicate_constraint" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_duplicate_constraint">test_duplicate_constraint</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_duplicate_constraint_after_another" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_duplicate_constraint_after_another">test_duplicate_constraint_after_another</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_empty_level" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_empty_level">test_empty_level</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_error_optimizations_contradict" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_error_optimizations_contradict">test_error_optimizations_contradict</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_invalid_level_with_models" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_invalid_level_with_models">test_invalid_level_with_models</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_invalid_level_without_models" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_invalid_level_without_models">test_invalid_level_without_models</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_invalid_num_models_with_models" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_invalid_num_models_with_models">test_invalid_num_models_with_models</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_not_a_number" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_not_a_number">test_not_a_number</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_num_and_models" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_num_and_models">test_num_and_models</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_num_models" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_num_models">test_num_models</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_select_blank_start_model_by_defined_init_parameter" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_select_blank_start_model_by_defined_init_parameter">test_select_blank_start_model_by_defined_init_parameter</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_select_blank_start_model_by_score" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_select_blank_start_model_by_score">test_select_blank_start_model_by_score</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_select_blank_start_model_by_undefined_init_parameter" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_select_blank_start_model_by_undefined_init_parameter">test_select_blank_start_model_by_undefined_init_parameter</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_select_by_init_parameters" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_select_by_init_parameters">test_select_by_init_parameters</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_select_by_parameter" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_select_by_parameter">test_select_by_parameter</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_select_by_score" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_select_by_score">test_select_by_score</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_select_by_score_and_init_parameter" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_select_by_score_and_init_parameter">test_select_by_score_and_init_parameter</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_select_by_scores" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_select_by_scores">test_select_by_scores</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_select_several_times" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_select_several_times">test_select_several_times</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_two_optimizations" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_two_optimizations">test_two_optimizations</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_unknown_init_parameter" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_unknown_init_parameter">test_unknown_init_parameter</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_unknown_score" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_unknown_score">test_unknown_score</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_warning_constraints_contradict" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_warning_constraints_contradict">test_warning_constraints_contradict</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_warning_fewer_than_requested" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_warning_fewer_than_requested">test_warning_fewer_than_requested</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_whitespace" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_whitespace">test_whitespace</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_wrong_arrow" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_wrong_arrow">test_wrong_arrow</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_wrong_case_in_constraints_connector" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_wrong_case_in_constraints_connector">test_wrong_case_in_constraints_connector</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_wrong_case_in_max_min" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_wrong_case_in_max_min">test_wrong_case_in_max_min</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_wrong_case_in_parameter" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_wrong_case_in_parameter">test_wrong_case_in_parameter</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_wrong_case_in_score" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_wrong_case_in_score">test_wrong_case_in_score</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_wrong_comparison_sign" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_wrong_comparison_sign">test_wrong_comparison_sign</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_wrong_constraints_connector" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_wrong_constraints_connector">test_wrong_constraints_connector</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_wrong_max_min" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_wrong_max_min">test_wrong_max_min</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_wrong_num_models" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_wrong_num_models">test_wrong_num_models</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_wrong_parameter_prefix" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_wrong_parameter_prefix">test_wrong_parameter_prefix</a></code></li>
<li><code><a title="topicnet.tests.test_experiment_select.TestExperimentSelect.test_zero_num_models" href="#topicnet.tests.test_experiment_select.TestExperimentSelect.test_zero_num_models">test_zero_num_models</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.1</a>.</p>
</footer>
</body>
</html>

<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.6.3" />
<title>topicnet.cooking_machine.models.thetaless_regularizer API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>topicnet.cooking_machine.models.thetaless_regularizer</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>Source code</summary>
<pre><code class="python">import numpy as np
import os
import pandas as pd
import scipy.sparse
import warnings

from numba import jit

import artm

from .base_regularizer import BaseRegularizer
from ..dataset import Dataset


# TODO: move this to BigARTM
# ==================================

FIELDS = &#39;token class_id token_value token_tf token_df&#39;.split()


def artm_dict2df(artm_dict):
    &#34;&#34;&#34;
    :Description: converts the BigARTM dictionary of the collection
        to the pandas.DataFrame.
        This is approximately equivalent to the dictionary.save_text()
        but has no I/O overhead

    &#34;&#34;&#34;
    dictionary_data = artm_dict._master.get_dictionary(artm_dict._name)
    dict_pandas = {field: list(getattr(dictionary_data, field))
                   for field in FIELDS}

    return pd.DataFrame(dict_pandas)

# ==================================


EPS = 1e-20


# TODO: is there a better way to do this?
def obtain_token2id(dataset: Dataset):
    &#34;&#34;&#34;
    Allows one to obtain the mapping from token to the artm.dictionary id of that token
    (useful for low-level operations such as reading batches manually)

    Returns
    -------
    dict:
        maps (token, class_id) to integer (corresponding to the row of Phi / dictionary id)

    &#34;&#34;&#34;
    df = artm_dict2df(dataset.get_dictionary())
    df_inverted_index = df[[&#39;token&#39;, &#39;class_id&#39;]].reset_index().set_index([&#39;token&#39;, &#39;class_id&#39;])

    return df_inverted_index.to_dict()[&#39;index&#39;]


def dataset2sparse_matrix(dataset, modality, modalities_to_use=None, remove_nans=True):
    &#34;&#34;&#34;
    Builds a sparse matrix from batch_vectorizer linked to the Dataset.

    If you need an inverse mapping:

    &gt;&gt;&gt; d = sparse_n_dw_matrix.todok()  # convert to dictionary of keys format
    &gt;&gt;&gt; dict_of_csr = dict(d.items())

    Parameters
    ----------
    dataset: Dataset
    modality: str
        the remaining modalities will be ignored
        (their occurrences will be replaced with zeros, but they will continue to exist).
    modalities_to_use: iterable
        a set of modalities the underlying topic model is using (this is about topic model,
        not regularizer; this parameter ensures that the shapes of n_dw matrix and actual
        Phi matrix match).

        The tokens outside of this list will be discarded utterly
        (the resulting matrix will have no entries corresponding to them)

        For artm.ARTM() models, you need to pass whatever is inside class_ids;
        while TopicModel usually requires this to be set inside modalities_to_use.

        If you hadn&#39;t explicitly listed any modalities yet, you probably could
        leave this argument as None.

        If you use a single modality, wrap it into a list (e.g.[&#39;@word&#39;]).
    remove_nans: bool
        whether to re-encode values to transform NaNs in n_dw matrix to explicitly stored zeros.

    Returns
    -------
    n_dw_matrix: scipy.sparse.csr_matrix
        the matrix of document-word occurrences
        (`n_dw` is a number of the occurrences of the word `w` in the document `d`.)
        This matrix determines the dependence between the Theta and Phi matrices
        (Phi is the result of one iteration of the ARTM&#39;s EM algorihtm
        with uniform Theta initialization and `n_dw` matrix of the document-word occurrences).

    &#34;&#34;&#34;  # noqa: W291
    token2id = obtain_token2id(dataset)

    batch_vectorizer = dataset.get_batch_vectorizer()

    return _batch_vectorizer2sparse_matrix(
        batch_vectorizer, token2id, modality, modalities_to_use, remove_nans
    )


def _batch_vectorizer2sparse_matrix(batch_vectorizer, token2id, modality, modalities_to_use=None, remove_nans=True):
    &#34;&#34;&#34;
    &#34;&#34;&#34;
    theta_column_naming = &#39;id&#39;  # scipy sparse matrix doesn&#39;t support non-integer indices
    matrix_row, matrix_col, matrix_data = [], [], []

    for batch_id in range(len(batch_vectorizer._batches_list)):
        batch_name = batch_vectorizer._batches_list[batch_id]._filename
        batch = artm.messages.Batch()
        with open(batch_name, &#34;rb&#34;) as f:
            batch.ParseFromString(f.read())

        for item_id in range(len(batch.item)):
            item = batch.item[item_id]
            theta_item_id = getattr(item, theta_column_naming)

            for local_token_id, token_weight in zip(item.token_id, item.token_weight):
                token_class_id = batch.class_id[local_token_id]
                token = batch.token[local_token_id]
                if (token, token_class_id) not in token2id:
                    # probably dictionary was filtered
                    continue
                if modalities_to_use and token_class_id not in modalities_to_use:
                    # skip foreign modality
                    continue
                if token_class_id != modality:
                    # we still need these tokens,
                    # shapes of n_dw matrix and actual Phi matrix should be in sync.
                    # this will be changed to zero at the end
                    token_weight = np.nan
                token_id = token2id[(token, token_class_id)]
                matrix_row.append(theta_item_id)
                matrix_col.append(token_id)
                matrix_data.append(token_weight)

    sparse_n_dw_matrix = scipy.sparse.csr_matrix(
        (matrix_data, (matrix_row, matrix_col)),
    )
    # remove the columns whose all elements are zero
    # (i.e. tokens which are of different modalities)
    # and renumber index (fill any &#34;holes&#34;)
    # this is needed to be in sync with artm dictionary after filtering elements out
    # (they need to have the same shape)
    ind = sparse_n_dw_matrix.sum(axis=0)
    nonzeros = np.ravel((ind &gt; 0) | (ind != ind))  # also includes NaN-s
    sparse_n_dw_matrix = sparse_n_dw_matrix[:, nonzeros]

    if remove_nans:
        # re-encode values to transform NaNs to explicitly stored zeros
        sparse_n_dw_matrix.data = np.nan_to_num(sparse_n_dw_matrix.data)

    return sparse_n_dw_matrix


@jit(nopython=True)
def memory_efficient_inner1d(fst_arr, fst_indices, snd_arr, snd_indices):
    &#34;&#34;&#34;
    Parameters
    ----------

    fst_arr: array-like
        2d array, shape is N x T
    fst_indices: array-like
        indices of the rows in fst_arr
    snd_arr: array-like
        2d array, shape is M x T
    snd_indices: array-like
        indices of the rows in fst_arr

    Returns
    -------
    np.array
        This is an array of the following form:
            np.array([
                sum(fst_arr[i, k] * snd_arr[j, k] for k in 0..T)
                for i, j in fst_indices, snd_indices
            ])
    &#34;&#34;&#34;
    assert fst_arr.shape[1] == snd_arr.shape[1]
    assert len(fst_indices) == len(snd_indices)

    _, T = fst_arr.shape
    size = len(fst_indices)
    result = np.zeros(size)
    for i in range(size):
        fst_index = fst_indices[i]
        snd_index = snd_indices[i]
        for j in range(T):
            result[i] += fst_arr[fst_index, j] * snd_arr[snd_index, j]
    return result


@jit(nopython=True)
def _get_docptr(D, indptr):
    docptr = []
    for doc_num in range(D):
        docptr.extend(
            [doc_num] * (indptr[doc_num + 1] - indptr[doc_num])
        )
    return np.array(docptr, dtype=np.int32)


def get_docptr(n_dw_matrix):
    &#34;&#34;&#34;
    Parameters
    ----------
    n_dw_matrix: array-like

    Returns
    -------
    np.array
        row indices for the provided matrix
    &#34;&#34;&#34;
    return _get_docptr(n_dw_matrix.shape[0], n_dw_matrix.indptr)


def calc_docsizes(n_dw_matrix):
    D, _ = n_dw_matrix.shape
    docsizes = []
    indptr = n_dw_matrix.indptr
    for doc_num in range(D):
        size = indptr[doc_num + 1] - indptr[doc_num]
        value = np.sum(
            n_dw_matrix.data[indptr[doc_num]:indptr[doc_num + 1]]
        )
        docsizes.extend([value] * size)
    return np.array(docsizes)


def get_prob_matrix_by_counters(counters, inplace=False):
    if inplace:
        res = counters
    else:
        res = np.copy(counters)
    res[res &lt; 0] = 0.
    # set rows where sum of row is small to uniform
    res[np.sum(res, axis=1) &lt; EPS, :] = 1.
    res /= np.sum(res, axis=1)[:, np.newaxis]

    return res


def calc_A_matrix(
    n_dw_matrix, theta_matrix, docptr, phi_matrix_tr, wordptr
):
    s_data = memory_efficient_inner1d(
        theta_matrix, docptr,
        phi_matrix_tr, wordptr
    )
    return scipy.sparse.csr_matrix(
        (
            n_dw_matrix.data / (s_data + EPS),
            n_dw_matrix.indices,
            n_dw_matrix.indptr
        ),
        shape=n_dw_matrix.shape
    )


class ThetalessRegularizer(BaseRegularizer):
    def __init__(self, name, tau, modality, dataset: Dataset, modalities_to_use=None):
        &#34;&#34;&#34;
        A regularizer based on a &#34;thetaless&#34; topic model inference

        Note: this implementation stores sparse `n_dw` matrix in memory,
        so this is not particularly memory- and space-efficient for huge datasets

        Parameters
        ----------
        name: str
            name of the regularizer.
        tau: Number
            according to the math, `tau` should be set to 1 (to correctly emulate a different  
            inference process). But you do you, it&#39;s not like there&#39;s a regularizer  
            police or something.  
        modality: str
            name of modality on which the inference should be based.
        dataset: Dataset
            will be transformed to n_dw_matrix.
        modalities_to_use: iterable
            a set of modalities the underlying topic model is using (this is about topic model,
            not regularizer; this parameter ensures that the shapes of n_dw matrix and actual
            Phi matrix match).
    
            The tokens outside of this list will be discarded utterly
            (the resulting matrix will have no entries corresponding to them)
    
            For artm.ARTM() models, you need to pass whatever is inside class_ids;
            while TopicModel usually requires this to be set inside modalities_to_use.
    
            If you hadn&#39;t explicitly listed any modalities yet, you probably could
            leave this argument as None.
    
            If you use a single modality, wrap it into a list (e.g.[&#39;@word&#39;]).

        &#34;&#34;&#34;  # noqa: W291
        super().__init__(name, tau)

        self.modality = modality
        self.modalities_to_use = modalities_to_use
        self.n_dw_matrix = None

        self.token2id = obtain_token2id(dataset)
        self._batches_path = os.path.join(dataset._internals_folder_path, &#34;batches&#34;)

    def _initialize_matrices(self, batch_vectorizer, token2id):
        self.n_dw_matrix = _batch_vectorizer2sparse_matrix(
            batch_vectorizer, token2id,
            self.modality, self.modalities_to_use,
            remove_nans=False,
        )
        ind = self.n_dw_matrix.sum(axis=0)
        self.modalities_mask = np.ravel((ind == ind))  # detects not-NaN-s
        self.n_dw_matrix.data = np.nan_to_num(self.n_dw_matrix.data)

        self.B = scipy.sparse.csr_matrix(
            (
                1. * self.n_dw_matrix.data / calc_docsizes(self.n_dw_matrix),
                self.n_dw_matrix.indices,
                self.n_dw_matrix.indptr
            ),
            shape=self.n_dw_matrix.shape
        ).tocsc()
        self.docptr = get_docptr(self.n_dw_matrix)
        self.wordptr = self.n_dw_matrix.indices

    def grad(self, pwt, nwt):
        phi_matrix_tr = np.array(pwt)
        phi_matrix = phi_matrix_tr.T
        phi_rev_matrix = get_prob_matrix_by_counters(phi_matrix_tr)

        if self.n_dw_matrix.shape[1] != phi_rev_matrix.shape[0]:
            raise ValueError(
                f&#34;Thetaless regularizer has prepared {self.n_dw_matrix.shape} n_dw matrix,&#34;
                f&#34; but was passed {phi_rev_matrix.T.shape} Phi matrix containing different&#34;
                f&#34; number of tokens ({self.n_dw_matrix.shape[1]} != {phi_rev_matrix.shape[0]})&#34;
                f&#34;\n(Are modalities the same?)&#34;
            )

        theta_matrix = get_prob_matrix_by_counters(
            self.n_dw_matrix.dot(phi_rev_matrix)
        )

        A = calc_A_matrix(
            self.n_dw_matrix,
            theta_matrix,
            self.docptr,
            phi_matrix_tr,
            self.wordptr
        ).tocsc()

        n_tw = A.T.dot(theta_matrix).T * phi_matrix
        g_dt = A.dot(phi_matrix_tr)
        tmp = g_dt.T * self.B / (phi_matrix_tr.sum(axis=1) + EPS)
        n_tw += (tmp - np.einsum(&#39;ij,ji-&gt;i&#39;, phi_rev_matrix, tmp)) * phi_matrix

        result = n_tw.T - nwt
        result = (result.T * self.modalities_mask).T

        return self.tau * result

    def attach(self, model):
        &#34;&#34;&#34;

        Parameters
        ----------
        model : ARTM model
            necessary to apply master component
        &#34;&#34;&#34;
        if model.num_document_passes != 1:
            warnings.warn(
                f&#34;num_document_passes is equal to {model.num_document_passes}, but it&#34;
                f&#34; should be set to {1} to correctly emulate a thetaless inference process&#34;
            )

        if not self.modalities_to_use:
            self.modalities_to_use = model.class_ids.keys()

        bv = artm.BatchVectorizer(data_path=self._batches_path, data_format=&#39;batches&#39;)
        self._initialize_matrices(bv, self.token2id)

        self._model = model</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="topicnet.cooking_machine.models.thetaless_regularizer.artm_dict2df"><code class="name flex">
<span>def <span class="ident">artm_dict2df</span></span>(<span>artm_dict)</span>
</code></dt>
<dd>
<section class="desc"><p>:Description: converts the BigARTM dictionary of the collection
to the pandas.DataFrame.
This is approximately equivalent to the dictionary.save_text()
but has no I/O overhead</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def artm_dict2df(artm_dict):
    &#34;&#34;&#34;
    :Description: converts the BigARTM dictionary of the collection
        to the pandas.DataFrame.
        This is approximately equivalent to the dictionary.save_text()
        but has no I/O overhead

    &#34;&#34;&#34;
    dictionary_data = artm_dict._master.get_dictionary(artm_dict._name)
    dict_pandas = {field: list(getattr(dictionary_data, field))
                   for field in FIELDS}

    return pd.DataFrame(dict_pandas)</code></pre>
</details>
</dd>
<dt id="topicnet.cooking_machine.models.thetaless_regularizer.calc_A_matrix"><code class="name flex">
<span>def <span class="ident">calc_A_matrix</span></span>(<span>n_dw_matrix, theta_matrix, docptr, phi_matrix_tr, wordptr)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def calc_A_matrix(
    n_dw_matrix, theta_matrix, docptr, phi_matrix_tr, wordptr
):
    s_data = memory_efficient_inner1d(
        theta_matrix, docptr,
        phi_matrix_tr, wordptr
    )
    return scipy.sparse.csr_matrix(
        (
            n_dw_matrix.data / (s_data + EPS),
            n_dw_matrix.indices,
            n_dw_matrix.indptr
        ),
        shape=n_dw_matrix.shape
    )</code></pre>
</details>
</dd>
<dt id="topicnet.cooking_machine.models.thetaless_regularizer.calc_docsizes"><code class="name flex">
<span>def <span class="ident">calc_docsizes</span></span>(<span>n_dw_matrix)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def calc_docsizes(n_dw_matrix):
    D, _ = n_dw_matrix.shape
    docsizes = []
    indptr = n_dw_matrix.indptr
    for doc_num in range(D):
        size = indptr[doc_num + 1] - indptr[doc_num]
        value = np.sum(
            n_dw_matrix.data[indptr[doc_num]:indptr[doc_num + 1]]
        )
        docsizes.extend([value] * size)
    return np.array(docsizes)</code></pre>
</details>
</dd>
<dt id="topicnet.cooking_machine.models.thetaless_regularizer.dataset2sparse_matrix"><code class="name flex">
<span>def <span class="ident">dataset2sparse_matrix</span></span>(<span>dataset, modality, modalities_to_use=None, remove_nans=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Builds a sparse matrix from batch_vectorizer linked to the Dataset.</p>
<p>If you need an inverse mapping:</p>
<pre><code>&gt;&gt;&gt; d = sparse_n_dw_matrix.todok()  # convert to dictionary of keys format
&gt;&gt;&gt; dict_of_csr = dict(d.items())
</code></pre>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>Dataset</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>modality</code></strong> :&ensp;<code>str</code></dt>
<dd>the remaining modalities will be ignored
(their occurrences will be replaced with zeros, but they will continue to exist).</dd>
<dt><strong><code>modalities_to_use</code></strong> :&ensp;<code>iterable</code></dt>
<dd>
<p>a set of modalities the underlying topic model is using (this is about topic model,
not regularizer; this parameter ensures that the shapes of n_dw matrix and actual
Phi matrix match).</p>
<p>The tokens outside of this list will be discarded utterly
(the resulting matrix will have no entries corresponding to them)</p>
<p>For artm.ARTM() models, you need to pass whatever is inside class_ids;
while TopicModel usually requires this to be set inside modalities_to_use.</p>
<p>If you hadn't explicitly listed any modalities yet, you probably could
leave this argument as None.</p>
<p>If you use a single modality, wrap it into a list (e.g.['@word']).</p>
</dd>
<dt><strong><code>remove_nans</code></strong> :&ensp;<code>bool</code></dt>
<dd>whether to re-encode values to transform NaNs in n_dw matrix to explicitly stored zeros.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>n_dw_matrix</code></strong> :&ensp;<code>scipy.sparse.csr_matrix</code></dt>
<dd>the matrix of document-word occurrences
(<code>n_dw</code> is a number of the occurrences of the word <code>w</code> in the document <code>d</code>.)
This matrix determines the dependence between the Theta and Phi matrices
(Phi is the result of one iteration of the ARTM's EM algorihtm
with uniform Theta initialization and <code>n_dw</code> matrix of the document-word occurrences).</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def dataset2sparse_matrix(dataset, modality, modalities_to_use=None, remove_nans=True):
    &#34;&#34;&#34;
    Builds a sparse matrix from batch_vectorizer linked to the Dataset.

    If you need an inverse mapping:

    &gt;&gt;&gt; d = sparse_n_dw_matrix.todok()  # convert to dictionary of keys format
    &gt;&gt;&gt; dict_of_csr = dict(d.items())

    Parameters
    ----------
    dataset: Dataset
    modality: str
        the remaining modalities will be ignored
        (their occurrences will be replaced with zeros, but they will continue to exist).
    modalities_to_use: iterable
        a set of modalities the underlying topic model is using (this is about topic model,
        not regularizer; this parameter ensures that the shapes of n_dw matrix and actual
        Phi matrix match).

        The tokens outside of this list will be discarded utterly
        (the resulting matrix will have no entries corresponding to them)

        For artm.ARTM() models, you need to pass whatever is inside class_ids;
        while TopicModel usually requires this to be set inside modalities_to_use.

        If you hadn&#39;t explicitly listed any modalities yet, you probably could
        leave this argument as None.

        If you use a single modality, wrap it into a list (e.g.[&#39;@word&#39;]).
    remove_nans: bool
        whether to re-encode values to transform NaNs in n_dw matrix to explicitly stored zeros.

    Returns
    -------
    n_dw_matrix: scipy.sparse.csr_matrix
        the matrix of document-word occurrences
        (`n_dw` is a number of the occurrences of the word `w` in the document `d`.)
        This matrix determines the dependence between the Theta and Phi matrices
        (Phi is the result of one iteration of the ARTM&#39;s EM algorihtm
        with uniform Theta initialization and `n_dw` matrix of the document-word occurrences).

    &#34;&#34;&#34;  # noqa: W291
    token2id = obtain_token2id(dataset)

    batch_vectorizer = dataset.get_batch_vectorizer()

    return _batch_vectorizer2sparse_matrix(
        batch_vectorizer, token2id, modality, modalities_to_use, remove_nans
    )</code></pre>
</details>
</dd>
<dt id="topicnet.cooking_machine.models.thetaless_regularizer.get_docptr"><code class="name flex">
<span>def <span class="ident">get_docptr</span></span>(<span>n_dw_matrix)</span>
</code></dt>
<dd>
<section class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_dw_matrix</code></strong> :&ensp;<code>array</code>-<code>like</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.array</code></dt>
<dd>row indices for the provided matrix</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_docptr(n_dw_matrix):
    &#34;&#34;&#34;
    Parameters
    ----------
    n_dw_matrix: array-like

    Returns
    -------
    np.array
        row indices for the provided matrix
    &#34;&#34;&#34;
    return _get_docptr(n_dw_matrix.shape[0], n_dw_matrix.indptr)</code></pre>
</details>
</dd>
<dt id="topicnet.cooking_machine.models.thetaless_regularizer.get_prob_matrix_by_counters"><code class="name flex">
<span>def <span class="ident">get_prob_matrix_by_counters</span></span>(<span>counters, inplace=False)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_prob_matrix_by_counters(counters, inplace=False):
    if inplace:
        res = counters
    else:
        res = np.copy(counters)
    res[res &lt; 0] = 0.
    # set rows where sum of row is small to uniform
    res[np.sum(res, axis=1) &lt; EPS, :] = 1.
    res /= np.sum(res, axis=1)[:, np.newaxis]

    return res</code></pre>
</details>
</dd>
<dt id="topicnet.cooking_machine.models.thetaless_regularizer.memory_efficient_inner1d"><code class="name flex">
<span>def <span class="ident">memory_efficient_inner1d</span></span>(<span>fst_arr, fst_indices, snd_arr, snd_indices)</span>
</code></dt>
<dd>
<section class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>fst_arr</code></strong> :&ensp;<code>array</code>-<code>like</code></dt>
<dd>2d array, shape is N x T</dd>
<dt><strong><code>fst_indices</code></strong> :&ensp;<code>array</code>-<code>like</code></dt>
<dd>indices of the rows in fst_arr</dd>
<dt><strong><code>snd_arr</code></strong> :&ensp;<code>array</code>-<code>like</code></dt>
<dd>2d array, shape is M x T</dd>
<dt><strong><code>snd_indices</code></strong> :&ensp;<code>array</code>-<code>like</code></dt>
<dd>indices of the rows in fst_arr</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.array</code></dt>
<dd>This is an array of the following form:
np.array([
sum(fst_arr[i, k] * snd_arr[j, k] for k in 0..T)
for i, j in fst_indices, snd_indices
])</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">@jit(nopython=True)
def memory_efficient_inner1d(fst_arr, fst_indices, snd_arr, snd_indices):
    &#34;&#34;&#34;
    Parameters
    ----------

    fst_arr: array-like
        2d array, shape is N x T
    fst_indices: array-like
        indices of the rows in fst_arr
    snd_arr: array-like
        2d array, shape is M x T
    snd_indices: array-like
        indices of the rows in fst_arr

    Returns
    -------
    np.array
        This is an array of the following form:
            np.array([
                sum(fst_arr[i, k] * snd_arr[j, k] for k in 0..T)
                for i, j in fst_indices, snd_indices
            ])
    &#34;&#34;&#34;
    assert fst_arr.shape[1] == snd_arr.shape[1]
    assert len(fst_indices) == len(snd_indices)

    _, T = fst_arr.shape
    size = len(fst_indices)
    result = np.zeros(size)
    for i in range(size):
        fst_index = fst_indices[i]
        snd_index = snd_indices[i]
        for j in range(T):
            result[i] += fst_arr[fst_index, j] * snd_arr[snd_index, j]
    return result</code></pre>
</details>
</dd>
<dt id="topicnet.cooking_machine.models.thetaless_regularizer.obtain_token2id"><code class="name flex">
<span>def <span class="ident">obtain_token2id</span></span>(<span>dataset)</span>
</code></dt>
<dd>
<section class="desc"><p>Allows one to obtain the mapping from token to the artm.dictionary id of that token
(useful for low-level operations such as reading batches manually)</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code>:</dt>
<dd>maps (token, class_id) to integer (corresponding to the row of Phi / dictionary id)</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def obtain_token2id(dataset: Dataset):
    &#34;&#34;&#34;
    Allows one to obtain the mapping from token to the artm.dictionary id of that token
    (useful for low-level operations such as reading batches manually)

    Returns
    -------
    dict:
        maps (token, class_id) to integer (corresponding to the row of Phi / dictionary id)

    &#34;&#34;&#34;
    df = artm_dict2df(dataset.get_dictionary())
    df_inverted_index = df[[&#39;token&#39;, &#39;class_id&#39;]].reset_index().set_index([&#39;token&#39;, &#39;class_id&#39;])

    return df_inverted_index.to_dict()[&#39;index&#39;]</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="topicnet.cooking_machine.models.thetaless_regularizer.ThetalessRegularizer"><code class="flex name class">
<span>class <span class="ident">ThetalessRegularizer</span></span>
<span>(</span><span>name, tau, modality, dataset, modalities_to_use=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Base regularizer class to construct custom regularizers.</p>
<p>A regularizer based on a "thetaless" topic model inference</p>
<p>Note: this implementation stores sparse <code>n_dw</code> matrix in memory,
so this is not particularly memory- and space-efficient for huge datasets</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code></dt>
<dd>name of the regularizer.</dd>
<dt><strong><code>tau</code></strong> :&ensp;<code>Number</code></dt>
<dd>according to the math, <code>tau</code> should be set to 1 (to correctly emulate a different<br>
inference process). But you do you, it's not like there's a regularizer<br>
police or something.</dd>
<dt><strong><code>modality</code></strong> :&ensp;<code>str</code></dt>
<dd>name of modality on which the inference should be based.</dd>
<dt><strong><code>dataset</code></strong> :&ensp;<code>Dataset</code></dt>
<dd>will be transformed to n_dw_matrix.</dd>
<dt><strong><code>modalities_to_use</code></strong> :&ensp;<code>iterable</code></dt>
<dd>
<p>a set of modalities the underlying topic model is using (this is about topic model,
not regularizer; this parameter ensures that the shapes of n_dw matrix and actual
Phi matrix match).</p>
<p>The tokens outside of this list will be discarded utterly
(the resulting matrix will have no entries corresponding to them)</p>
<p>For artm.ARTM() models, you need to pass whatever is inside class_ids;
while TopicModel usually requires this to be set inside modalities_to_use.</p>
<p>If you hadn't explicitly listed any modalities yet, you probably could
leave this argument as None.</p>
<p>If you use a single modality, wrap it into a list (e.g.['@word']).</p>
</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class ThetalessRegularizer(BaseRegularizer):
    def __init__(self, name, tau, modality, dataset: Dataset, modalities_to_use=None):
        &#34;&#34;&#34;
        A regularizer based on a &#34;thetaless&#34; topic model inference

        Note: this implementation stores sparse `n_dw` matrix in memory,
        so this is not particularly memory- and space-efficient for huge datasets

        Parameters
        ----------
        name: str
            name of the regularizer.
        tau: Number
            according to the math, `tau` should be set to 1 (to correctly emulate a different  
            inference process). But you do you, it&#39;s not like there&#39;s a regularizer  
            police or something.  
        modality: str
            name of modality on which the inference should be based.
        dataset: Dataset
            will be transformed to n_dw_matrix.
        modalities_to_use: iterable
            a set of modalities the underlying topic model is using (this is about topic model,
            not regularizer; this parameter ensures that the shapes of n_dw matrix and actual
            Phi matrix match).
    
            The tokens outside of this list will be discarded utterly
            (the resulting matrix will have no entries corresponding to them)
    
            For artm.ARTM() models, you need to pass whatever is inside class_ids;
            while TopicModel usually requires this to be set inside modalities_to_use.
    
            If you hadn&#39;t explicitly listed any modalities yet, you probably could
            leave this argument as None.
    
            If you use a single modality, wrap it into a list (e.g.[&#39;@word&#39;]).

        &#34;&#34;&#34;  # noqa: W291
        super().__init__(name, tau)

        self.modality = modality
        self.modalities_to_use = modalities_to_use
        self.n_dw_matrix = None

        self.token2id = obtain_token2id(dataset)
        self._batches_path = os.path.join(dataset._internals_folder_path, &#34;batches&#34;)

    def _initialize_matrices(self, batch_vectorizer, token2id):
        self.n_dw_matrix = _batch_vectorizer2sparse_matrix(
            batch_vectorizer, token2id,
            self.modality, self.modalities_to_use,
            remove_nans=False,
        )
        ind = self.n_dw_matrix.sum(axis=0)
        self.modalities_mask = np.ravel((ind == ind))  # detects not-NaN-s
        self.n_dw_matrix.data = np.nan_to_num(self.n_dw_matrix.data)

        self.B = scipy.sparse.csr_matrix(
            (
                1. * self.n_dw_matrix.data / calc_docsizes(self.n_dw_matrix),
                self.n_dw_matrix.indices,
                self.n_dw_matrix.indptr
            ),
            shape=self.n_dw_matrix.shape
        ).tocsc()
        self.docptr = get_docptr(self.n_dw_matrix)
        self.wordptr = self.n_dw_matrix.indices

    def grad(self, pwt, nwt):
        phi_matrix_tr = np.array(pwt)
        phi_matrix = phi_matrix_tr.T
        phi_rev_matrix = get_prob_matrix_by_counters(phi_matrix_tr)

        if self.n_dw_matrix.shape[1] != phi_rev_matrix.shape[0]:
            raise ValueError(
                f&#34;Thetaless regularizer has prepared {self.n_dw_matrix.shape} n_dw matrix,&#34;
                f&#34; but was passed {phi_rev_matrix.T.shape} Phi matrix containing different&#34;
                f&#34; number of tokens ({self.n_dw_matrix.shape[1]} != {phi_rev_matrix.shape[0]})&#34;
                f&#34;\n(Are modalities the same?)&#34;
            )

        theta_matrix = get_prob_matrix_by_counters(
            self.n_dw_matrix.dot(phi_rev_matrix)
        )

        A = calc_A_matrix(
            self.n_dw_matrix,
            theta_matrix,
            self.docptr,
            phi_matrix_tr,
            self.wordptr
        ).tocsc()

        n_tw = A.T.dot(theta_matrix).T * phi_matrix
        g_dt = A.dot(phi_matrix_tr)
        tmp = g_dt.T * self.B / (phi_matrix_tr.sum(axis=1) + EPS)
        n_tw += (tmp - np.einsum(&#39;ij,ji-&gt;i&#39;, phi_rev_matrix, tmp)) * phi_matrix

        result = n_tw.T - nwt
        result = (result.T * self.modalities_mask).T

        return self.tau * result

    def attach(self, model):
        &#34;&#34;&#34;

        Parameters
        ----------
        model : ARTM model
            necessary to apply master component
        &#34;&#34;&#34;
        if model.num_document_passes != 1:
            warnings.warn(
                f&#34;num_document_passes is equal to {model.num_document_passes}, but it&#34;
                f&#34; should be set to {1} to correctly emulate a thetaless inference process&#34;
            )

        if not self.modalities_to_use:
            self.modalities_to_use = model.class_ids.keys()

        bv = artm.BatchVectorizer(data_path=self._batches_path, data_format=&#39;batches&#39;)
        self._initialize_matrices(bv, self.token2id)

        self._model = model</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="topicnet.cooking_machine.models.base_regularizer.BaseRegularizer" href="base_regularizer.html#topicnet.cooking_machine.models.base_regularizer.BaseRegularizer">BaseRegularizer</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="topicnet.cooking_machine.models.thetaless_regularizer.ThetalessRegularizer.grad"><code class="name flex">
<span>def <span class="ident">grad</span></span>(<span>self, pwt, nwt)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def grad(self, pwt, nwt):
    phi_matrix_tr = np.array(pwt)
    phi_matrix = phi_matrix_tr.T
    phi_rev_matrix = get_prob_matrix_by_counters(phi_matrix_tr)

    if self.n_dw_matrix.shape[1] != phi_rev_matrix.shape[0]:
        raise ValueError(
            f&#34;Thetaless regularizer has prepared {self.n_dw_matrix.shape} n_dw matrix,&#34;
            f&#34; but was passed {phi_rev_matrix.T.shape} Phi matrix containing different&#34;
            f&#34; number of tokens ({self.n_dw_matrix.shape[1]} != {phi_rev_matrix.shape[0]})&#34;
            f&#34;\n(Are modalities the same?)&#34;
        )

    theta_matrix = get_prob_matrix_by_counters(
        self.n_dw_matrix.dot(phi_rev_matrix)
    )

    A = calc_A_matrix(
        self.n_dw_matrix,
        theta_matrix,
        self.docptr,
        phi_matrix_tr,
        self.wordptr
    ).tocsc()

    n_tw = A.T.dot(theta_matrix).T * phi_matrix
    g_dt = A.dot(phi_matrix_tr)
    tmp = g_dt.T * self.B / (phi_matrix_tr.sum(axis=1) + EPS)
    n_tw += (tmp - np.einsum(&#39;ij,ji-&gt;i&#39;, phi_rev_matrix, tmp)) * phi_matrix

    result = n_tw.T - nwt
    result = (result.T * self.modalities_mask).T

    return self.tau * result</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="topicnet.cooking_machine.models.base_regularizer.BaseRegularizer" href="base_regularizer.html#topicnet.cooking_machine.models.base_regularizer.BaseRegularizer">BaseRegularizer</a></b></code>:
<ul class="hlist">
<li><code><a title="topicnet.cooking_machine.models.base_regularizer.BaseRegularizer.attach" href="base_regularizer.html#topicnet.cooking_machine.models.base_regularizer.BaseRegularizer.attach">attach</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="topicnet.cooking_machine.models" href="index.html">topicnet.cooking_machine.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="topicnet.cooking_machine.models.thetaless_regularizer.artm_dict2df" href="#topicnet.cooking_machine.models.thetaless_regularizer.artm_dict2df">artm_dict2df</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.thetaless_regularizer.calc_A_matrix" href="#topicnet.cooking_machine.models.thetaless_regularizer.calc_A_matrix">calc_A_matrix</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.thetaless_regularizer.calc_docsizes" href="#topicnet.cooking_machine.models.thetaless_regularizer.calc_docsizes">calc_docsizes</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.thetaless_regularizer.dataset2sparse_matrix" href="#topicnet.cooking_machine.models.thetaless_regularizer.dataset2sparse_matrix">dataset2sparse_matrix</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.thetaless_regularizer.get_docptr" href="#topicnet.cooking_machine.models.thetaless_regularizer.get_docptr">get_docptr</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.thetaless_regularizer.get_prob_matrix_by_counters" href="#topicnet.cooking_machine.models.thetaless_regularizer.get_prob_matrix_by_counters">get_prob_matrix_by_counters</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.thetaless_regularizer.memory_efficient_inner1d" href="#topicnet.cooking_machine.models.thetaless_regularizer.memory_efficient_inner1d">memory_efficient_inner1d</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.thetaless_regularizer.obtain_token2id" href="#topicnet.cooking_machine.models.thetaless_regularizer.obtain_token2id">obtain_token2id</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="topicnet.cooking_machine.models.thetaless_regularizer.ThetalessRegularizer" href="#topicnet.cooking_machine.models.thetaless_regularizer.ThetalessRegularizer">ThetalessRegularizer</a></code></h4>
<ul class="">
<li><code><a title="topicnet.cooking_machine.models.thetaless_regularizer.ThetalessRegularizer.grad" href="#topicnet.cooking_machine.models.thetaless_regularizer.ThetalessRegularizer.grad">grad</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.6.3</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>
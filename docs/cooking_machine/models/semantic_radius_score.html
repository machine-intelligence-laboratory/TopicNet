<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.0" />
<title>topicnet.cooking_machine.models.semantic_radius_score API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>topicnet.cooking_machine.models.semantic_radius_score</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import artm

import operator
import functools
import numpy as np
import pandas as pd
from collections import Counter, OrderedDict
from scipy.optimize import curve_fit

from .base_score import BaseScore


def calculate_n(model, batch_vectorizer):
    &#34;&#34;&#34;
    Calculate all necessary statistics from batch. This may take some time.
    &#34;&#34;&#34;
    doc2token = {}
    for batch_id in range(len(batch_vectorizer._batches_list)):
        batch_name = batch_vectorizer._batches_list[batch_id]._filename
        batch = artm.messages.Batch()
        with open(batch_name, &#34;rb&#34;) as f:
            batch.ParseFromString(f.read())

        for item_id in range(len(batch.item)):
            item = batch.item[item_id]
            theta_item_id = getattr(item, model.theta_columns_naming)

            doc2token[theta_item_id] = {&#39;tokens&#39;: [], &#39;weights&#39;: []}
            for token_id, token_weight in zip(item.token_id, item.token_weight):
                doc2token[theta_item_id][&#39;tokens&#39;].append(batch.token[token_id])
                doc2token[theta_item_id][&#39;weights&#39;].append(token_weight)

    previous_num_document_passes = model._num_document_passes
    model._num_document_passes = 10
    ptdw = model.transform(batch_vectorizer=batch_vectorizer, theta_matrix_type=&#39;dense_ptdw&#39;)
    model._num_document_passes = previous_num_document_passes

    docs = ptdw.columns
    docs_unique = OrderedDict.fromkeys(docs).keys()

    tokens = [doc2token[doc_id][&#39;tokens&#39;] for doc_id in docs_unique]
    tokens = functools.reduce(operator.iconcat, tokens, [])

    ndw = np.concatenate([np.array(doc2token[doc_id][&#39;weights&#39;]) for doc_id in docs_unique])
    ndw = np.tile(ndw, (ptdw.shape[0], 1))

    ptdw.columns = pd.MultiIndex.from_arrays([docs, tokens], names=(&#39;doc&#39;, &#39;token&#39;))
    ntdw = ptdw * ndw

    ntd = ntdw.groupby(level=0, axis=1).sum()

    nwt = ntdw.groupby(level=1, axis=1).sum().T

    nt = nwt.sum(axis=0)

    return ntdw, ntd, nwt, nt


def synthetic_doc_ntdw_and_ntd(doc_len, nwt):
    &#34;&#34;&#34;
    Create synthetic document from nwt with specific doc_len.
    &#34;&#34;&#34;
    pwt = np.float64(nwt) / np.sum(np.float64(nwt)).astype(float)
    doc_idx = np.random.choice(len(pwt), doc_len, p=pwt)
    doc_count = dict(Counter(doc_idx))

    ntdw = np.empty((len(pwt)))
    for word_idx in range(len(ntdw)):
        ntdw[word_idx] = doc_count.get(word_idx, 0)
    ntd = np.sum(ntdw)

    return ntdw, ntd


def cressie_reed_sampled(topic, ntdw_calc, ntd_calc, nwt, nt, gimel=-1/2):
    &#34;&#34;&#34;
    Calculate Cressie-Reed divergence for sampled pseudo-document.
    &#34;&#34;&#34;
    mul_part = ntd_calc * nwt.iloc[:, topic]

    if np.all(ntdw_calc == 0) or nt[topic] == 0 or np.all(mul_part == 0):
        gimel_part = np.array([0])
    else:
        gimel_part = 0
        for token_id, token in enumerate(nwt.index):
            token_ntdw = ntdw_calc[token_id]
            token_denom = mul_part.iloc[token_id]
            if token_ntdw and token_denom:
                gimel_part += token_ntdw * (
                    np.power(token_ntdw * nt[topic] / token_denom, gimel) - 1
                )

    cressie_reed_for_l = 2 / (gimel * (gimel + 1)) * np.sum(gimel_part)

    return cressie_reed_for_l


def third_degree(x, a, b, c, d):
    return a + b * x + c * x ** 2 + d * x ** 3


def radius_vs_ndt(topic, max_len, sample_step, sample_size, nwt, nt, alpha):
    &#34;&#34;&#34;
    Calculate third degree approximation for radius vs ndt dependency.
    &#34;&#34;&#34;
    crs_for_alpha = []
    ntds_sampled = []
    for doc_len in range(1, max_len, sample_step):
        local_crs_for_alpha = []
        for _ in range(sample_size):
            ntdw_sampled, ntd_sampled = synthetic_doc_ntdw_and_ntd(doc_len, nwt.iloc[:, topic])
            local_crs_for_alpha.append(cressie_reed_sampled(
                topic, ntdw_sampled, ntd_sampled, nwt, nt
            ))

        crs_for_alpha.append(np.quantile(local_crs_for_alpha, 1 - alpha))
        ntds_sampled.append(ntd_sampled)

    regression_coeff, cov = curve_fit(third_degree, ntds_sampled, crs_for_alpha)
    return regression_coeff


def radii_vs_ntd(max_len, sample_step, sample_size, nwt, nt, alpha):
    regression_coeffs = []
    for topic in range(len(nt)):
        regression_coeffs.append(radius_vs_ndt(
            topic, max_len, sample_step, sample_size, nwt, nt, alpha
        ))

    return regression_coeffs


def radius_for_ntd(ntd, regression_coeff):
    return third_degree(ntd, *regression_coeff)


def radii_for_ntd(ntd, regression_coeff):
    return ntd.apply(lambda x: third_degree(x, *regression_coeff))


class SemanticRadiusScore(BaseScore):
    &#34;&#34;&#34;
    This score implements cluster semantic radius, described in paper
    &#39;Проверка гипотезы условной независимости 
    для оценивания качества тематической кластеризации&#39; by Rogozina A.
    At the core this score helps to discover topics uniformity.
    The lower this score - better
    &#34;&#34;&#34;  # noqa: W291
    def __init__(self, batch_vectorizer, name: str = None):
        &#34;&#34;&#34;

        Parameters
        ----------
        name:
            Name of the score
        batch_vectorizer

        &#34;&#34;&#34;
        super().__init__(name=name)

        self.batch_vectorizer = batch_vectorizer

    def __repr__(self):
        return f&#39;{self.__class__.__name__}(batch_vectorizer={self.batch_vectorizer!r})&#39;

    def update(self, score):
        known_errors = (ValueError, TypeError)
        try:
            score = np.array(score, float)
        except known_errors:
            raise ValueError(f&#39;Score call should return list of float but not {score}&#39;)
        self.value.append(score)

    def call(self, model, max_sampled_document_len=None, sample_step=5, sample_size=3, alpha=0.1):
        &#34;&#34;&#34;

        Parameters
        ----------
        model : TopicModel
        max_sampled_document_len : int
            Maximum length of pseudo-document for quantile regression
            (Default value = None)
        sample_step : int
            Grain for quantile regression
            (Default value = 5)
        sample_size : int
            Size of every sample for quantile regression  
            (Default value = 3)
        alpha : float
            (1 - alpha) quantile level, must be &lt;= 1  
            (Default value = 0.1)

        &#34;&#34;&#34;  # noqa: W291
        ntdw, ntd, nwt, nt = calculate_n(model._model, self.batch_vectorizer)

        if max_sampled_document_len is None:
            max_sampled_document_len = int(np.max(ntd.values))

        regression_coeffs = radii_vs_ntd(
            max_sampled_document_len, sample_step, sample_size, nwt, nt, alpha
        )
        radii = [
            radius_for_ntd(topic_ntd, coeff)
            for topic_ntd, coeff
            in zip(ntd.values.mean(axis=1), regression_coeffs)
        ]

        return radii</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="topicnet.cooking_machine.models.semantic_radius_score.calculate_n"><code class="name flex">
<span>def <span class="ident">calculate_n</span></span>(<span>model, batch_vectorizer)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate all necessary statistics from batch. This may take some time.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_n(model, batch_vectorizer):
    &#34;&#34;&#34;
    Calculate all necessary statistics from batch. This may take some time.
    &#34;&#34;&#34;
    doc2token = {}
    for batch_id in range(len(batch_vectorizer._batches_list)):
        batch_name = batch_vectorizer._batches_list[batch_id]._filename
        batch = artm.messages.Batch()
        with open(batch_name, &#34;rb&#34;) as f:
            batch.ParseFromString(f.read())

        for item_id in range(len(batch.item)):
            item = batch.item[item_id]
            theta_item_id = getattr(item, model.theta_columns_naming)

            doc2token[theta_item_id] = {&#39;tokens&#39;: [], &#39;weights&#39;: []}
            for token_id, token_weight in zip(item.token_id, item.token_weight):
                doc2token[theta_item_id][&#39;tokens&#39;].append(batch.token[token_id])
                doc2token[theta_item_id][&#39;weights&#39;].append(token_weight)

    previous_num_document_passes = model._num_document_passes
    model._num_document_passes = 10
    ptdw = model.transform(batch_vectorizer=batch_vectorizer, theta_matrix_type=&#39;dense_ptdw&#39;)
    model._num_document_passes = previous_num_document_passes

    docs = ptdw.columns
    docs_unique = OrderedDict.fromkeys(docs).keys()

    tokens = [doc2token[doc_id][&#39;tokens&#39;] for doc_id in docs_unique]
    tokens = functools.reduce(operator.iconcat, tokens, [])

    ndw = np.concatenate([np.array(doc2token[doc_id][&#39;weights&#39;]) for doc_id in docs_unique])
    ndw = np.tile(ndw, (ptdw.shape[0], 1))

    ptdw.columns = pd.MultiIndex.from_arrays([docs, tokens], names=(&#39;doc&#39;, &#39;token&#39;))
    ntdw = ptdw * ndw

    ntd = ntdw.groupby(level=0, axis=1).sum()

    nwt = ntdw.groupby(level=1, axis=1).sum().T

    nt = nwt.sum(axis=0)

    return ntdw, ntd, nwt, nt</code></pre>
</details>
</dd>
<dt id="topicnet.cooking_machine.models.semantic_radius_score.cressie_reed_sampled"><code class="name flex">
<span>def <span class="ident">cressie_reed_sampled</span></span>(<span>topic, ntdw_calc, ntd_calc, nwt, nt, gimel=-0.5)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate Cressie-Reed divergence for sampled pseudo-document.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cressie_reed_sampled(topic, ntdw_calc, ntd_calc, nwt, nt, gimel=-1/2):
    &#34;&#34;&#34;
    Calculate Cressie-Reed divergence for sampled pseudo-document.
    &#34;&#34;&#34;
    mul_part = ntd_calc * nwt.iloc[:, topic]

    if np.all(ntdw_calc == 0) or nt[topic] == 0 or np.all(mul_part == 0):
        gimel_part = np.array([0])
    else:
        gimel_part = 0
        for token_id, token in enumerate(nwt.index):
            token_ntdw = ntdw_calc[token_id]
            token_denom = mul_part.iloc[token_id]
            if token_ntdw and token_denom:
                gimel_part += token_ntdw * (
                    np.power(token_ntdw * nt[topic] / token_denom, gimel) - 1
                )

    cressie_reed_for_l = 2 / (gimel * (gimel + 1)) * np.sum(gimel_part)

    return cressie_reed_for_l</code></pre>
</details>
</dd>
<dt id="topicnet.cooking_machine.models.semantic_radius_score.radii_for_ntd"><code class="name flex">
<span>def <span class="ident">radii_for_ntd</span></span>(<span>ntd, regression_coeff)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def radii_for_ntd(ntd, regression_coeff):
    return ntd.apply(lambda x: third_degree(x, *regression_coeff))</code></pre>
</details>
</dd>
<dt id="topicnet.cooking_machine.models.semantic_radius_score.radii_vs_ntd"><code class="name flex">
<span>def <span class="ident">radii_vs_ntd</span></span>(<span>max_len, sample_step, sample_size, nwt, nt, alpha)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def radii_vs_ntd(max_len, sample_step, sample_size, nwt, nt, alpha):
    regression_coeffs = []
    for topic in range(len(nt)):
        regression_coeffs.append(radius_vs_ndt(
            topic, max_len, sample_step, sample_size, nwt, nt, alpha
        ))

    return regression_coeffs</code></pre>
</details>
</dd>
<dt id="topicnet.cooking_machine.models.semantic_radius_score.radius_for_ntd"><code class="name flex">
<span>def <span class="ident">radius_for_ntd</span></span>(<span>ntd, regression_coeff)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def radius_for_ntd(ntd, regression_coeff):
    return third_degree(ntd, *regression_coeff)</code></pre>
</details>
</dd>
<dt id="topicnet.cooking_machine.models.semantic_radius_score.radius_vs_ndt"><code class="name flex">
<span>def <span class="ident">radius_vs_ndt</span></span>(<span>topic, max_len, sample_step, sample_size, nwt, nt, alpha)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate third degree approximation for radius vs ndt dependency.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def radius_vs_ndt(topic, max_len, sample_step, sample_size, nwt, nt, alpha):
    &#34;&#34;&#34;
    Calculate third degree approximation for radius vs ndt dependency.
    &#34;&#34;&#34;
    crs_for_alpha = []
    ntds_sampled = []
    for doc_len in range(1, max_len, sample_step):
        local_crs_for_alpha = []
        for _ in range(sample_size):
            ntdw_sampled, ntd_sampled = synthetic_doc_ntdw_and_ntd(doc_len, nwt.iloc[:, topic])
            local_crs_for_alpha.append(cressie_reed_sampled(
                topic, ntdw_sampled, ntd_sampled, nwt, nt
            ))

        crs_for_alpha.append(np.quantile(local_crs_for_alpha, 1 - alpha))
        ntds_sampled.append(ntd_sampled)

    regression_coeff, cov = curve_fit(third_degree, ntds_sampled, crs_for_alpha)
    return regression_coeff</code></pre>
</details>
</dd>
<dt id="topicnet.cooking_machine.models.semantic_radius_score.synthetic_doc_ntdw_and_ntd"><code class="name flex">
<span>def <span class="ident">synthetic_doc_ntdw_and_ntd</span></span>(<span>doc_len, nwt)</span>
</code></dt>
<dd>
<div class="desc"><p>Create synthetic document from nwt with specific doc_len.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def synthetic_doc_ntdw_and_ntd(doc_len, nwt):
    &#34;&#34;&#34;
    Create synthetic document from nwt with specific doc_len.
    &#34;&#34;&#34;
    pwt = np.float64(nwt) / np.sum(np.float64(nwt)).astype(float)
    doc_idx = np.random.choice(len(pwt), doc_len, p=pwt)
    doc_count = dict(Counter(doc_idx))

    ntdw = np.empty((len(pwt)))
    for word_idx in range(len(ntdw)):
        ntdw[word_idx] = doc_count.get(word_idx, 0)
    ntd = np.sum(ntdw)

    return ntdw, ntd</code></pre>
</details>
</dd>
<dt id="topicnet.cooking_machine.models.semantic_radius_score.third_degree"><code class="name flex">
<span>def <span class="ident">third_degree</span></span>(<span>x, a, b, c, d)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def third_degree(x, a, b, c, d):
    return a + b * x + c * x ** 2 + d * x ** 3</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="topicnet.cooking_machine.models.semantic_radius_score.SemanticRadiusScore"><code class="flex name class">
<span>class <span class="ident">SemanticRadiusScore</span></span>
<span>(</span><span>batch_vectorizer, name: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>This score implements cluster semantic radius, described in paper
'Проверка гипотезы условной независимости
для оценивания качества тематической кластеризации' by Rogozina A.
At the core this score helps to discover topics uniformity.
The lower this score - better</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt>name:</dt>
<dt>Name of the score</dt>
<dt><strong><code>batch_vectorizer</code></strong></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SemanticRadiusScore(BaseScore):
    &#34;&#34;&#34;
    This score implements cluster semantic radius, described in paper
    &#39;Проверка гипотезы условной независимости 
    для оценивания качества тематической кластеризации&#39; by Rogozina A.
    At the core this score helps to discover topics uniformity.
    The lower this score - better
    &#34;&#34;&#34;  # noqa: W291
    def __init__(self, batch_vectorizer, name: str = None):
        &#34;&#34;&#34;

        Parameters
        ----------
        name:
            Name of the score
        batch_vectorizer

        &#34;&#34;&#34;
        super().__init__(name=name)

        self.batch_vectorizer = batch_vectorizer

    def __repr__(self):
        return f&#39;{self.__class__.__name__}(batch_vectorizer={self.batch_vectorizer!r})&#39;

    def update(self, score):
        known_errors = (ValueError, TypeError)
        try:
            score = np.array(score, float)
        except known_errors:
            raise ValueError(f&#39;Score call should return list of float but not {score}&#39;)
        self.value.append(score)

    def call(self, model, max_sampled_document_len=None, sample_step=5, sample_size=3, alpha=0.1):
        &#34;&#34;&#34;

        Parameters
        ----------
        model : TopicModel
        max_sampled_document_len : int
            Maximum length of pseudo-document for quantile regression
            (Default value = None)
        sample_step : int
            Grain for quantile regression
            (Default value = 5)
        sample_size : int
            Size of every sample for quantile regression  
            (Default value = 3)
        alpha : float
            (1 - alpha) quantile level, must be &lt;= 1  
            (Default value = 0.1)

        &#34;&#34;&#34;  # noqa: W291
        ntdw, ntd, nwt, nt = calculate_n(model._model, self.batch_vectorizer)

        if max_sampled_document_len is None:
            max_sampled_document_len = int(np.max(ntd.values))

        regression_coeffs = radii_vs_ntd(
            max_sampled_document_len, sample_step, sample_size, nwt, nt, alpha
        )
        radii = [
            radius_for_ntd(topic_ntd, coeff)
            for topic_ntd, coeff
            in zip(ntd.values.mean(axis=1), regression_coeffs)
        ]

        return radii</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="topicnet.cooking_machine.models.base_score.BaseScore" href="base_score.html#topicnet.cooking_machine.models.base_score.BaseScore">BaseScore</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="topicnet.cooking_machine.models.semantic_radius_score.SemanticRadiusScore.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, model, max_sampled_document_len=None, sample_step=5, sample_size=3, alpha=0.1)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>TopicModel</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>max_sampled_document_len</code></strong> :&ensp;<code>int</code></dt>
<dd>Maximum length of pseudo-document for quantile regression
(Default value = None)</dd>
<dt><strong><code>sample_step</code></strong> :&ensp;<code>int</code></dt>
<dd>Grain for quantile regression
(Default value = 5)</dd>
<dt><strong><code>sample_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Size of every sample for quantile regression<br>
(Default value = 3)</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code></dt>
<dd>(1 - alpha) quantile level, must be &lt;= 1<br>
(Default value = 0.1)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, model, max_sampled_document_len=None, sample_step=5, sample_size=3, alpha=0.1):
    &#34;&#34;&#34;

    Parameters
    ----------
    model : TopicModel
    max_sampled_document_len : int
        Maximum length of pseudo-document for quantile regression
        (Default value = None)
    sample_step : int
        Grain for quantile regression
        (Default value = 5)
    sample_size : int
        Size of every sample for quantile regression  
        (Default value = 3)
    alpha : float
        (1 - alpha) quantile level, must be &lt;= 1  
        (Default value = 0.1)

    &#34;&#34;&#34;  # noqa: W291
    ntdw, ntd, nwt, nt = calculate_n(model._model, self.batch_vectorizer)

    if max_sampled_document_len is None:
        max_sampled_document_len = int(np.max(ntd.values))

    regression_coeffs = radii_vs_ntd(
        max_sampled_document_len, sample_step, sample_size, nwt, nt, alpha
    )
    radii = [
        radius_for_ntd(topic_ntd, coeff)
        for topic_ntd, coeff
        in zip(ntd.values.mean(axis=1), regression_coeffs)
    ]

    return radii</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="topicnet.cooking_machine.models.base_score.BaseScore" href="base_score.html#topicnet.cooking_machine.models.base_score.BaseScore">BaseScore</a></b></code>:
<ul class="hlist">
<li><code><a title="topicnet.cooking_machine.models.base_score.BaseScore.update" href="base_score.html#topicnet.cooking_machine.models.base_score.BaseScore.update">update</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="topicnet.cooking_machine.models" href="index.html">topicnet.cooking_machine.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="topicnet.cooking_machine.models.semantic_radius_score.calculate_n" href="#topicnet.cooking_machine.models.semantic_radius_score.calculate_n">calculate_n</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.semantic_radius_score.cressie_reed_sampled" href="#topicnet.cooking_machine.models.semantic_radius_score.cressie_reed_sampled">cressie_reed_sampled</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.semantic_radius_score.radii_for_ntd" href="#topicnet.cooking_machine.models.semantic_radius_score.radii_for_ntd">radii_for_ntd</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.semantic_radius_score.radii_vs_ntd" href="#topicnet.cooking_machine.models.semantic_radius_score.radii_vs_ntd">radii_vs_ntd</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.semantic_radius_score.radius_for_ntd" href="#topicnet.cooking_machine.models.semantic_radius_score.radius_for_ntd">radius_for_ntd</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.semantic_radius_score.radius_vs_ndt" href="#topicnet.cooking_machine.models.semantic_radius_score.radius_vs_ndt">radius_vs_ndt</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.semantic_radius_score.synthetic_doc_ntdw_and_ntd" href="#topicnet.cooking_machine.models.semantic_radius_score.synthetic_doc_ntdw_and_ntd">synthetic_doc_ntdw_and_ntd</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.semantic_radius_score.third_degree" href="#topicnet.cooking_machine.models.semantic_radius_score.third_degree">third_degree</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="topicnet.cooking_machine.models.semantic_radius_score.SemanticRadiusScore" href="#topicnet.cooking_machine.models.semantic_radius_score.SemanticRadiusScore">SemanticRadiusScore</a></code></h4>
<ul class="">
<li><code><a title="topicnet.cooking_machine.models.semantic_radius_score.SemanticRadiusScore.call" href="#topicnet.cooking_machine.models.semantic_radius_score.SemanticRadiusScore.call">call</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.0</a>.</p>
</footer>
</body>
</html>
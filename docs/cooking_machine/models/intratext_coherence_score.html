<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.6.3" />
<title>topicnet.cooking_machine.models.intratext_coherence_score API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>topicnet.cooking_machine.models.intratext_coherence_score</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>Source code</summary>
<pre><code class="python">import dill
import numpy as np
import pandas as pd
import sys
import tqdm
import warnings

from collections import defaultdict
from enum import Enum, IntEnum, auto
from typing import (
    Dict,
    List,
    Optional,
    Tuple,
    Union,
)

from .base_score import BaseScore
from .base_model import BaseModel
from ..dataset import (
    Dataset,
    VW_TEXT_COL, RAW_TEXT_COL,
    DEFAULT_ARTM_MODALITY, MODALITY_START_SYMBOL
)


class TextType(Enum):
    VW_TEXT = VW_TEXT_COL
    RAW_TEXT = RAW_TEXT_COL


class ComputationMethod(IntEnum):
    &#34;&#34;&#34;
    Ways to compute intra-text coherence
    (see more about coherence below in IntratextCoherenceScore)

    Attributes
    ----------
    SEGMENT_LENGTH :
        Estimate the length of topic segments
    SEGMENT_WEIGHT :
        Estimate the weight of topic segment
        (weight - sum of specificities for the topic over words in segment)
    SUM_OVER_WINDOW :
        Sum of specificities for the topic over words in given window.
        The process is as follows:
        word of the topic is found in text, it is the center of the first window;
        next word of the topic is found (outside of the previous window), window; etc
    &#34;&#34;&#34;
    SEGMENT_LENGTH = auto()
    SEGMENT_WEIGHT = auto()
    SUM_OVER_WINDOW = auto()


class WordTopicRelatednessType(IntEnum):
    &#34;&#34;&#34;
    Word-topic relatedness estimate

    Attributes
    ----------
    PWT :
        p(w | t)
    PTW :
        p(t | w)
    &#34;&#34;&#34;
    PWT = auto()
    PTW = auto()


class SpecificityEstimationMethod(IntEnum):
    &#34;&#34;&#34;
    Way to estimate how particular word is specific for particular topic.
    Unlike probability, eg. p(w | t), specificity_estimation takes into account
    values for all topics, eg. p(w | t_1), p(w | t_2), ..., p(w | t_n):
    the higher the value p(w | t) comparing other p(w | t_i),
    the higher the specificity_estimation of word &#34;w&#34; for the topic &#34;t&#34;

    Attributes
    ----------
    NONE :
        Don&#39;t try to estimate specificity_estimation, return the probability as is
    MAXIMUM :
        From probability, corresponding to word and topic,
        extract *maximum* among probabilities for the word and other topics
    AVERAGE :
        From probability, corresponding to word and topic,
        extract *average* among probabilities for the word and other topics
    &#34;&#34;&#34;
    NONE = auto()
    MAXIMUM = auto()
    AVERAGE = auto()


class IntratextCoherenceScore(BaseScore):
    &#34;&#34;&#34;Computes intratext coherence

    For each topic of topic model its distribution throughout document collection is observed.
    Hypothetically, the better the topic, the more often it is represented by
    long segments of words highly related to the topic.
    The score tries to bring to life this idea.

    For more details one may see the article http://www.dialog-21.ru/media/4281/alekseevva.pdf
    &#34;&#34;&#34;
    def __init__(  # noqa: C901
            self,
            dataset: Union[Dataset, str],
            name: str = None,
            keep_dataset_in_memory: bool = None,
            keep_dataset: bool = True,
            documents: List[str] = None,
            documents_fraction: float = 1.0,
            text_type: TextType = TextType.VW_TEXT,
            computation_method: ComputationMethod = ComputationMethod.SEGMENT_WEIGHT,
            word_topic_relatedness: WordTopicRelatednessType = WordTopicRelatednessType.PWT,
            specificity_estimation: SpecificityEstimationMethod = SpecificityEstimationMethod.NONE,
            max_num_out_of_topic_words: int = 10,
            window: int = 20,
            start_fit_iteration: int = 0,
            fit_iteration_step: int = 1,
            seed: int = 11221963,
            verbose: bool = False,
    ):
        &#34;&#34;&#34;
        Parameters
        ----------
        name:
            Name of the score
        dataset : Dataset
            Dataset with document collection, or path to dataset
            (any model passed to `call()` is supposed to be trained on it)
        keep_dataset_in_memory
            Whether to keep `dataset` in memory or not
            (parameter `_small_data` of the `dataset` object).
            If `dataset` is given as object of type `Dataset` (and not as `str` path to dataset),
            the parameter will be set equal to `dataset._small_data`.
            Otherwise, the default value is `True` and `dataset._small_data` will be overwritten.
        keep_dataset
            Whether to keep `dataset` constantly as inner part of the score,
            or recreate it for each `call()` invocation and then dispose
        documents : list of str
            Which documents from the dataset are to be used for computing coherence
        documents_fraction
            The fraction of all the documents in the Dataset to be used for coherence computation
            if `documents` parameter is not specified
        text_type : TextType
            What text to use when computing coherence: raw text or VW text
            Preferable to use VW (as it is usually preprocessed, stop-words removed etc.),
            and with words in *natural order*.
            Score needs &#34;real&#34; text to compute coherence
        computation_method : ComputationMethod
            The way to compute intra-text coherence
        word_topic_relatedness : WordTopicRelatednessType
            How to estimate word relevance to topic: using p(w | t) or p(t | w)
        specificity_estimation : SpecificityEstimationMethod
            How to estimate specificity of word to topic
        max_num_out_of_topic_words : int
            In case computation_method = ComputationMethod.SEGMENT_LENGTH or
            ComputationMethod.SEGMENT_WEIGHT:
            Maximum number of words not of the topic which can be encountered without stopping
            the process of adding words to the current segment
        window : int
            In case computation_method = ComputationMethod.SUM_OVER_WINDOW:
            Window width. So the window will be the words with positions
            in [current position - window / 2, current position + window / 2)
        start_fit_iteration
            Indicates how many calls are skipped before the actual score is calculated.
            Replaces not calculated values with placeholders
            (for consistency of score values with number of model fit iterations).
        fit_iteration_step
            Number of iterations between `score.call()` invocations which actually update the score
        seed
            Random seed used for documents subsampling if `documents` parameter is not specified
        Notes
        -----
        Parameters `start_fit_iteration` and `fit_iteration_step` are introduced
        to reduce the time needed for one model training.
        If one is interested only in the last score value
        at the end of the training process (and not in the dependence of score on iteration),
        one should adjust `start_fit_iteration` and `fit_iteration_step` correspondingly.
        For example:

        &gt;&gt;&gt; # dataset = Dataset(...)
        &gt;&gt;&gt; # topic_model = TopicModel(...)
        &gt;&gt;&gt; num_iterations = 100
        &gt;&gt;&gt; topic_model.custom_scores[&#39;intratext_coherence&#39;] = IntratextCoherenceScore(
        &gt;&gt;&gt;     dataset,
        &gt;&gt;&gt;     start_fit_iteration=num_iterations - 1  # last iteration: starting from zero
        &gt;&gt;&gt; )
        &gt;&gt;&gt; topic_model._fit(dataset.get_batch_vectorizer(), num_iterations=num_iterations)
        &#34;&#34;&#34;
        # TODO: word_topic_relatedness seems to be connected with TopTokensViewer stuff
        super().__init__(name=name)

        self._keep_dataset = keep_dataset

        if isinstance(dataset, str):
            if keep_dataset_in_memory is None:
                keep_dataset_in_memory = True

            dataset = Dataset(data_path=dataset, keep_in_memory=keep_dataset_in_memory)

        self._keep_dataset_in_memory = dataset._small_data

        if not isinstance(dataset, Dataset):
            raise TypeError(
                f&#39;Got &#34;{type(dataset)}&#34; as \&#34;dataset\&#34;. Expect it to derive from &#34;Dataset&#34;&#39;)

        if not isinstance(text_type, TextType):
            raise TypeError(
                f&#39;Wrong &#34;text_type&#34;: \&#34;{text_type}\&#34;. &#39;
                f&#39;Expect to be \&#34;{TextType}\&#34;&#39;)

        if not isinstance(computation_method, ComputationMethod):
            raise TypeError(
                f&#39;Wrong &#34;computation_method&#34;: \&#34;{computation_method}\&#34;. &#39;
                f&#39;Expect to be \&#34;{ComputationMethod}\&#34;&#39;)

        if not isinstance(word_topic_relatedness, WordTopicRelatednessType):
            raise TypeError(
                f&#39;Wrong &#34;word_topic_relatedness&#34;: \&#34;{word_topic_relatedness}\&#34;. &#39;
                f&#39;Expect to be \&#34;{WordTopicRelatednessType}\&#34;&#39;)

        if not isinstance(specificity_estimation, SpecificityEstimationMethod):
            raise TypeError(
                f&#39;Wrong &#34;specificity_estimation&#34;: \&#34;{specificity_estimation}\&#34;. &#39;
                f&#39;Expect to be \&#34;{SpecificityEstimationMethod}\&#34;&#39;)

        if not isinstance(max_num_out_of_topic_words, int):
            raise TypeError(
                f&#39;Wrong &#34;max_num_out_of_topic_words&#34;: \&#34;{max_num_out_of_topic_words}\&#34;. &#39;
                f&#39;Expect to be \&#34;int\&#34;&#39;)

        if not isinstance(window, int):
            raise TypeError(f&#39;Wrong &#34;window&#34;: \&#34;{window}\&#34;. Expect to be \&#34;int\&#34;&#39;)

        if window &lt; 0 or (window == 0 and computation_method == ComputationMethod.SUM_OVER_WINDOW):
            raise ValueError(
                f&#39;Wrong value for &#34;window&#34;: \&#34;{window}\&#34;. &#39;
                f&#39;Expect to be non-negative. And greater than zero in case &#39;
                f&#39;computation_method == ComputationMethod.SUM_OVER_WINDOW&#39;)

        if not isinstance(start_fit_iteration, int):
            raise TypeError(
                f&#39;Wrong &#34;start_fit_iteration&#34;: \&#34;{start_fit_iteration}\&#34;.&#39;
                f&#39; Expect to be \&#34;int\&#34;&#39;
            )

        if not isinstance(fit_iteration_step, int):
            raise TypeError(
                f&#39;Wrong &#34;fit_iteration_step&#34;: \&#34;{start_fit_iteration}\&#34;.&#39;
                f&#39; Expect to be \&#34;int\&#34;&#39;
            )
        if fit_iteration_step &lt;= 0:
            raise ValueError(
                f&#39;Wrong &#34;fit_iteration_step&#34;: \&#34;{fit_iteration_step}\&#34;.&#39;
                f&#39; Expect to be &gt; 0&#39;
            )

        if documents_fraction &lt;= 0:
            raise ValueError(
                f&#39;Wrong &#34;documents_fraction&#34;: \&#34;{documents_fraction}\&#34;.&#39;
                f&#39; Expect to be in (0, 1]&#39;
            )
        if documents_fraction &gt; 1.0:
            warnings.warn(
                f&#39;Parameter documents_fraction={documents_fraction} can\&#39;t be bigger than 1.0&#39;
                f&#39; Setting it equal to 1.0&#39;
            )

            documents_fraction = 1.0

        self._dataset = dataset
        self._dataset_file_path = dataset._data_path
        self._dataset_internals_folder_path = dataset._internals_folder_path

        self._text_type = text_type
        self._computation_method = computation_method
        self._word_topic_relatedness = word_topic_relatedness
        self._specificity_estimation_method = specificity_estimation
        self._max_num_out_of_topic_words = max_num_out_of_topic_words
        self._window = window

        self._verbose = verbose

        self._current_iteration = 0
        self._start_fit_iteration = start_fit_iteration
        self._fit_iteration_step = fit_iteration_step

        if documents is not None:
            self._documents = documents
        else:
            all_documents = list(self._dataset.get_dataset().index)
            documents_fraction = min(documents_fraction, 1.0)
            num_documents_to_choose = int(
                np.ceil(len(all_documents) * documents_fraction)
            )
            custom_random = np.random.RandomState(seed)

            self._documents = list(
                custom_random.choice(
                    all_documents,
                    size=num_documents_to_choose,
                    replace=False
                )
            )

    def __repr__(self):
        return (f&#39;{self.__class__.__name__}(&#39;
                f&#39;text_type={self._text_type!r}&#39;
                f&#39;computation_method={self._computation_method!r}&#39;
                f&#39;word_topic_relatedness={self._word_topic_relatedness!r}&#39;
                f&#39;specificity_estimation_method={self._specificity_estimation_method!r}&#39;
                f&#39;max_num_out_of_topic_words={self._max_num_out_of_topic_words!r}&#39;
                f&#39;window={self._window!r}&#39;
                f&#39;)&#39;)

    @property
    def dataset(self) -&gt; Dataset:
        return self._dataset

    @dataset.setter
    def dataset(self, new_dataset: Dataset) -&gt; None:
        self._dataset = new_dataset
        self._dataset_file_path = new_dataset._data_path
        self._dataset_internals_folder_path = new_dataset._internals_folder_path
        self._keep_dataset_in_memory = new_dataset._small_data

    def save(self, path: str) -&gt; None:
        dataset = self._dataset
        self._dataset = None

        with open(path, &#39;wb&#39;) as f:
            dill.dump(self, f)

        self._dataset = dataset

    @classmethod
    def load(cls, path: str):
        &#34;&#34;&#34;

        Parameters
        ----------
        path

        Returns
        -------
        IntratextCoherenceScore

        &#34;&#34;&#34;
        score: IntratextCoherenceScore

        with open(path, &#39;rb&#39;) as f:
            score = dill.load(f)

        if not score._keep_dataset:
            score._dataset = None
        else:
            score._dataset = Dataset(
                score._dataset_file_path,
                internals_folder_path=score._dataset_internals_folder_path,
                keep_in_memory=score._keep_dataset_in_memory,
            )

        return score

    def call(self, model: BaseModel) -&gt; float:
        if (self._current_iteration - self._start_fit_iteration) % self._fit_iteration_step != 0:
            self._current_iteration += 1

            return float(&#39;nan&#39;)

        try:
            if self._dataset is None:
                self._dataset = Dataset(
                    self._dataset_file_path,
                    internals_folder_path=self._dataset_internals_folder_path,
                    keep_in_memory=self._keep_dataset_in_memory,
                )

            topic_coherences = self.compute(model, None)

            coherence_values = list(
                v if v is not None else 0.0  # TODO: state the behavior clearer somehow
                for v in topic_coherences.values()
            )

            self._current_iteration += 1

            return float(np.median(coherence_values))  # TODO: or mean?

        finally:
            if not self._keep_dataset:
                self._dataset = None

    def compute(
            self,
            model: BaseModel,
            topics: List[str] = None,
            documents: List[str] = None
    ) -&gt; Dict[str, Optional[float]]:

        if not isinstance(model, BaseModel):
            raise TypeError(
                f&#39;Got &#34;{type(model)}&#34; as &#34;model&#34;. &#39;
                f&#39;Expect it to derive from &#34;BaseModel&#34;&#39;)

        if topics is None:
            topics = IntratextCoherenceScore._get_topics(model)

        if documents is None:
            documents = list(self._documents)

        if not isinstance(topics, list):
            raise TypeError(
                f&#39;Got &#34;{type(topics)}&#34; as &#34;topics&#34;. Expect list of topic names&#39;)

        if not isinstance(documents, list):
            raise TypeError(
                f&#39;Got &#34;{type(documents)}&#34; as &#34;documents&#34;. Expect list of document ids&#39;)

        word_topic_relatednesses = self._get_word_topic_relatednesses(model)

        topic_document_coherences = np.zeros((len(topics), len(documents)))
        document_indices_with_topic_coherence = defaultdict(list)

        if not self._verbose:
            document_enumeration = enumerate(documents)
        else:
            document_enumeration = tqdm.tqdm(
                enumerate(documents), total=len(documents), file=sys.stdout
            )

        for document_index, document in document_enumeration:
            for topic_index, topic in enumerate(topics):
                # TODO: read document text only once for all topics
                topic_coherence = self._compute_coherence(
                    topic, document, word_topic_relatednesses)

                if topic_coherence is not None:
                    topic_document_coherences[topic_index, document_index] = topic_coherence
                    document_indices_with_topic_coherence[topic].append(document_index)

        topic_coherences = [
            topic_document_coherences[topic_index, document_indices_with_topic_coherence[topic]]
            if len(document_indices_with_topic_coherence) &gt; 0 else list()
            for topic_index, topic in enumerate(topics)
        ]

        return dict(zip(
            topics,
            [float(np.mean(coherence_values))
             if len(coherence_values) &gt; 0 else None
             for coherence_values in topic_coherences]
        ))

    @staticmethod
    def _get_topics(model):
        return list(model.get_phi().columns)

    def _get_word_topic_relatednesses(self, model) -&gt; pd.DataFrame:
        phi = model.get_phi()

        word_topic_probs = self._get_word_topic_probs(phi)

        if self._specificity_estimation_method == SpecificityEstimationMethod.NONE:
            pass

        elif self._specificity_estimation_method == SpecificityEstimationMethod.AVERAGE:
            word_topic_probs[:] = (
                word_topic_probs.values -
                    np.sum(word_topic_probs.values, axis=1, keepdims=True) /  # noqa E131
                        max(word_topic_probs.shape[1], 1)  # noqa E131
            )

        elif self._specificity_estimation_method == SpecificityEstimationMethod.MAXIMUM:
            new_columns = []

            for t in word_topic_probs.columns:
                new_column = (
                    word_topic_probs[t].values -
                    np.max(
                        word_topic_probs[word_topic_probs.columns.difference([t])].values, axis=1)
                )
                new_columns.append(list(new_column))

            word_topic_probs[:] = np.array(new_columns).T

        return word_topic_probs

    def _get_word_topic_probs(self, phi: pd.DataFrame) -&gt; pd.DataFrame:
        if self._word_topic_relatedness == WordTopicRelatednessType.PWT:
            return phi

        elif self._word_topic_relatedness == WordTopicRelatednessType.PTW:
            # Treat all topics as equally probable
            eps = np.finfo(float).tiny

            pwt = phi
            pwt_values = pwt.values

            return pd.DataFrame(
                index=pwt.index,
                columns=pwt.columns,
                data=pwt_values / (pwt_values.sum(axis=1).reshape(-1, 1) + eps)
            )

        assert False

    def _compute_coherence(self, topic, document, word_topic_relatednesses):
        assert isinstance(self._computation_method, ComputationMethod)

        words = self._get_words(document)

        if self._computation_method == ComputationMethod.SUM_OVER_WINDOW:
            average_sum_over_window = self._sum_relatednesses_over_window(
                topic, words, word_topic_relatednesses
            )

            return average_sum_over_window

        topic_segment_length, topic_segment_weight = self._compute_segment_characteristics(
            topic, words, word_topic_relatednesses
        )

        if self._computation_method == ComputationMethod.SEGMENT_LENGTH:
            return topic_segment_length

        elif self._computation_method == ComputationMethod.SEGMENT_WEIGHT:
            return topic_segment_weight

    def _get_words(self, document):
        def get_biggest_modality_or_default():
            modalities = list(self._dataset.get_possible_modalities())

            if len(modalities) == 0:
                return DEFAULT_ARTM_MODALITY

            modalities_vocabulary_sizes = list(map(
                lambda m: self._dataset.get_dataset().loc[m].shape[0],
                modalities
            ))

            return modalities[np.argmax(modalities_vocabulary_sizes)]

        if self._text_type == TextType.RAW_TEXT:
            text = self._dataset.get_source_document(document).values[0, 0]  # TODO: this way?
            modality = get_biggest_modality_or_default()

            return list(map(lambda w: (modality, w), text.split()))

        if self._text_type == TextType.VW_TEXT:
            text = self._dataset.get_vw_document(document).values[0, 0]  # TODO: this way?

            words = []
            modality = None

            # TODO: there was similar bunch of code somewhere...
            for word in text.split()[1:]:  # skip document id
                if word.startswith(MODALITY_START_SYMBOL):
                    modality = word[1:]

                    continue

                word = word.split(&#39;:&#39;)[0]

                if modality is not None:
                    word = (modality, word)  # phi multiIndex
                else:
                    word = (DEFAULT_ARTM_MODALITY, word)

                words.append(word)

            return words

        assert False

    def _compute_segment_characteristics(
            self, topic, words, word_topic_relatednesses: pd.DataFrame
    ) -&gt; Tuple[float, float]:

        topic_segment_lengths = []
        topic_segment_weights = []

        topic_index = word_topic_relatednesses.columns.get_loc(topic)
        word_topic_indices = np.argmax(word_topic_relatednesses.values, axis=1)

        def get_word_topic_index(word):
            if word not in word_topic_relatednesses.index:
                return -1
            else:
                return word_topic_indices[
                    word_topic_relatednesses.index.get_loc(word)
                ]

        index = 0

        while index &lt; len(words):
            original_index = index

            if get_word_topic_index(words[index]) != topic_index:
                index += 1

                continue

            segment_length = 1
            segment_weight = IntratextCoherenceScore._get_relatedness(
                words[index], topic, word_topic_relatednesses
            )

            num_out_of_topic_words = 0

            index += 1

            while index &lt; len(words) and num_out_of_topic_words &lt; self._max_num_out_of_topic_words:
                if get_word_topic_index(words[index]) != topic_index:
                    num_out_of_topic_words += 1
                else:
                    segment_length += 1
                    segment_weight += IntratextCoherenceScore._get_relatedness(
                        words[index], topic, word_topic_relatednesses
                    )

                    num_out_of_topic_words = 0

                index += 1

            topic_segment_lengths.append(segment_length)
            topic_segment_weights.append(segment_weight)

            assert index &gt; original_index

        if len(topic_segment_lengths) == 0:
            return None, None
        else:
            return np.mean(topic_segment_lengths), np.mean(topic_segment_weights)

    def _sum_relatednesses_over_window(
            self, topic, words, word_topic_relatednesses) -&gt; float:

        topic_index = word_topic_relatednesses.columns.get_loc(topic)
        word_topic_indices = np.argmax(word_topic_relatednesses.values, axis=1)

        def get_word_topic_index(word):
            if word not in word_topic_relatednesses.index:
                return -1
            else:
                return word_topic_indices[
                    word_topic_relatednesses.index.get_loc(word)
                ]

        def find_next_topic_word(starting_index: int) -&gt; int:
            index = starting_index

            while index &lt; len(words) and\
                    get_word_topic_index(words[index]) != topic_index:
                index += 1

            if index == len(words):
                return -1  # failed to find next topic word

            return index

        word_index = find_next_topic_word(0)

        if word_index == -1:
            return None

        sums = list()

        while word_index &lt; len(words) and word_index != -1:
            original_word_index = word_index

            window_lower_bound = word_index - int(np.floor(self._window // 2))
            window_upper_bound = word_index + int(np.ceil(self._window // 2))

            sum_in_window = np.sum(
                [
                    IntratextCoherenceScore._get_relatedness(
                        w, topic, word_topic_relatednesses
                    )
                    for w in words[window_lower_bound:window_upper_bound]
                ]
            )

            sums.append(sum_in_window)

            word_index = find_next_topic_word(window_upper_bound)

            assert word_index &gt; original_word_index or word_index == -1

        return np.mean(sums)

    @staticmethod
    def _get_relatedness(
            word, topic, word_topic_relatednesses: pd.DataFrame) -&gt; float:

        if word in word_topic_relatednesses.index:
            return word_topic_relatednesses.loc[word, topic]

        # TODO: throw Warning or log somewhere?
        return np.mean(word_topic_relatednesses.values)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="topicnet.cooking_machine.models.intratext_coherence_score.ComputationMethod"><code class="flex name class">
<span>class <span class="ident">ComputationMethod</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Ways to compute intra-text coherence
(see more about coherence below in IntratextCoherenceScore)</p>
<h2 id="attributes">Attributes</h2>
<p>SEGMENT_LENGTH :
Estimate the length of topic segments
SEGMENT_WEIGHT :
Estimate the weight of topic segment
(weight - sum of specificities for the topic over words in segment)
SUM_OVER_WINDOW :
Sum of specificities for the topic over words in given window.
The process is as follows:
word of the topic is found in text, it is the center of the first window;
next word of the topic is found (outside of the previous window), window; etc</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class ComputationMethod(IntEnum):
    &#34;&#34;&#34;
    Ways to compute intra-text coherence
    (see more about coherence below in IntratextCoherenceScore)

    Attributes
    ----------
    SEGMENT_LENGTH :
        Estimate the length of topic segments
    SEGMENT_WEIGHT :
        Estimate the weight of topic segment
        (weight - sum of specificities for the topic over words in segment)
    SUM_OVER_WINDOW :
        Sum of specificities for the topic over words in given window.
        The process is as follows:
        word of the topic is found in text, it is the center of the first window;
        next word of the topic is found (outside of the previous window), window; etc
    &#34;&#34;&#34;
    SEGMENT_LENGTH = auto()
    SEGMENT_WEIGHT = auto()
    SUM_OVER_WINDOW = auto()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.IntEnum</li>
<li>builtins.int</li>
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="topicnet.cooking_machine.models.intratext_coherence_score.ComputationMethod.SEGMENT_LENGTH"><code class="name">var <span class="ident">SEGMENT_LENGTH</span></code></dt>
<dd>
<section class="desc"></section>
</dd>
<dt id="topicnet.cooking_machine.models.intratext_coherence_score.ComputationMethod.SEGMENT_WEIGHT"><code class="name">var <span class="ident">SEGMENT_WEIGHT</span></code></dt>
<dd>
<section class="desc"></section>
</dd>
<dt id="topicnet.cooking_machine.models.intratext_coherence_score.ComputationMethod.SUM_OVER_WINDOW"><code class="name">var <span class="ident">SUM_OVER_WINDOW</span></code></dt>
<dd>
<section class="desc"></section>
</dd>
</dl>
</dd>
<dt id="topicnet.cooking_machine.models.intratext_coherence_score.IntratextCoherenceScore"><code class="flex name class">
<span>class <span class="ident">IntratextCoherenceScore</span></span>
<span>(</span><span>dataset, name=None, keep_dataset_in_memory=None, keep_dataset=True, documents=None, documents_fraction=1.0, text_type=<TextType.VW_TEXT: 'vw_text'>, computation_method=<ComputationMethod.SEGMENT_WEIGHT: 2>, word_topic_relatedness=<WordTopicRelatednessType.PWT: 1>, specificity_estimation=<SpecificityEstimationMethod.NONE: 1>, max_num_out_of_topic_words=10, window=20, start_fit_iteration=0, fit_iteration_step=1, seed=11221963, verbose=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Computes intratext coherence</p>
<p>For each topic of topic model its distribution throughout document collection is observed.
Hypothetically, the better the topic, the more often it is represented by
long segments of words highly related to the topic.
The score tries to bring to life this idea.</p>
<p>For more details one may see the article <a href="http://www.dialog-21.ru/media/4281/alekseevva.pdf">http://www.dialog-21.ru/media/4281/alekseevva.pdf</a></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt>name:</dt>
<dt>Name of the score</dt>
<dt><strong><code>dataset</code></strong> :&ensp;<code>Dataset</code></dt>
<dd>Dataset with document collection, or path to dataset
(any model passed to <code>call()</code> is supposed to be trained on it)</dd>
<dt><strong><code>keep_dataset_in_memory</code></strong></dt>
<dd>Whether to keep <code>dataset</code> in memory or not
(parameter <code>_small_data</code> of the <code>dataset</code> object).
If <code>dataset</code> is given as object of type <code>Dataset</code> (and not as <code>str</code> path to dataset),
the parameter will be set equal to <code>dataset._small_data</code>.
Otherwise, the default value is <code>True</code> and <code>dataset._small_data</code> will be overwritten.</dd>
<dt><strong><code>keep_dataset</code></strong></dt>
<dd>Whether to keep <code>dataset</code> constantly as inner part of the score,
or recreate it for each <code>call()</code> invocation and then dispose</dd>
<dt><strong><code>documents</code></strong> :&ensp;<code>list</code> of <code>str</code></dt>
<dd>Which documents from the dataset are to be used for computing coherence</dd>
<dt><strong><code>documents_fraction</code></strong></dt>
<dd>The fraction of all the documents in the Dataset to be used for coherence computation
if <code>documents</code> parameter is not specified</dd>
<dt><strong><code>text_type</code></strong> :&ensp;<a title="topicnet.cooking_machine.models.intratext_coherence_score.TextType" href="#topicnet.cooking_machine.models.intratext_coherence_score.TextType"><code>TextType</code></a></dt>
<dd>What text to use when computing coherence: raw text or VW text
Preferable to use VW (as it is usually preprocessed, stop-words removed etc.),
and with words in <em>natural order</em>.
Score needs "real" text to compute coherence</dd>
<dt><strong><code>computation_method</code></strong> :&ensp;<a title="topicnet.cooking_machine.models.intratext_coherence_score.ComputationMethod" href="#topicnet.cooking_machine.models.intratext_coherence_score.ComputationMethod"><code>ComputationMethod</code></a></dt>
<dd>The way to compute intra-text coherence</dd>
<dt><strong><code>word_topic_relatedness</code></strong> :&ensp;<a title="topicnet.cooking_machine.models.intratext_coherence_score.WordTopicRelatednessType" href="#topicnet.cooking_machine.models.intratext_coherence_score.WordTopicRelatednessType"><code>WordTopicRelatednessType</code></a></dt>
<dd>How to estimate word relevance to topic: using p(w | t) or p(t | w)</dd>
<dt><strong><code>specificity_estimation</code></strong> :&ensp;<a title="topicnet.cooking_machine.models.intratext_coherence_score.SpecificityEstimationMethod" href="#topicnet.cooking_machine.models.intratext_coherence_score.SpecificityEstimationMethod"><code>SpecificityEstimationMethod</code></a></dt>
<dd>How to estimate specificity of word to topic</dd>
<dt><strong><code>max_num_out_of_topic_words</code></strong> :&ensp;<code>int</code></dt>
<dd>In case computation_method = ComputationMethod.SEGMENT_LENGTH or
ComputationMethod.SEGMENT_WEIGHT:
Maximum number of words not of the topic which can be encountered without stopping
the process of adding words to the current segment</dd>
<dt><strong><code>window</code></strong> :&ensp;<code>int</code></dt>
<dd>In case computation_method = ComputationMethod.SUM_OVER_WINDOW:
Window width. So the window will be the words with positions
in [current position - window / 2, current position + window / 2)</dd>
<dt><strong><code>start_fit_iteration</code></strong></dt>
<dd>Indicates how many calls are skipped before the actual score is calculated.
Replaces not calculated values with placeholders
(for consistency of score values with number of model fit iterations).</dd>
<dt><strong><code>fit_iteration_step</code></strong></dt>
<dd>Number of iterations between <code>score.call()</code> invocations which actually update the score</dd>
<dt><strong><code>seed</code></strong></dt>
<dd>Random seed used for documents subsampling if <code>documents</code> parameter is not specified</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>Parameters <code>start_fit_iteration</code> and <code>fit_iteration_step</code> are introduced
to reduce the time needed for one model training.
If one is interested only in the last score value
at the end of the training process (and not in the dependence of score on iteration),
one should adjust <code>start_fit_iteration</code> and <code>fit_iteration_step</code> correspondingly.
For example:</p>
<pre><code>&gt;&gt;&gt; # dataset = Dataset(...)
&gt;&gt;&gt; # topic_model = TopicModel(...)
&gt;&gt;&gt; num_iterations = 100
&gt;&gt;&gt; topic_model.custom_scores['intratext_coherence'] = IntratextCoherenceScore(
&gt;&gt;&gt;     dataset,
&gt;&gt;&gt;     start_fit_iteration=num_iterations - 1  # last iteration: starting from zero
&gt;&gt;&gt; )
&gt;&gt;&gt; topic_model._fit(dataset.get_batch_vectorizer(), num_iterations=num_iterations)
</code></pre></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class IntratextCoherenceScore(BaseScore):
    &#34;&#34;&#34;Computes intratext coherence

    For each topic of topic model its distribution throughout document collection is observed.
    Hypothetically, the better the topic, the more often it is represented by
    long segments of words highly related to the topic.
    The score tries to bring to life this idea.

    For more details one may see the article http://www.dialog-21.ru/media/4281/alekseevva.pdf
    &#34;&#34;&#34;
    def __init__(  # noqa: C901
            self,
            dataset: Union[Dataset, str],
            name: str = None,
            keep_dataset_in_memory: bool = None,
            keep_dataset: bool = True,
            documents: List[str] = None,
            documents_fraction: float = 1.0,
            text_type: TextType = TextType.VW_TEXT,
            computation_method: ComputationMethod = ComputationMethod.SEGMENT_WEIGHT,
            word_topic_relatedness: WordTopicRelatednessType = WordTopicRelatednessType.PWT,
            specificity_estimation: SpecificityEstimationMethod = SpecificityEstimationMethod.NONE,
            max_num_out_of_topic_words: int = 10,
            window: int = 20,
            start_fit_iteration: int = 0,
            fit_iteration_step: int = 1,
            seed: int = 11221963,
            verbose: bool = False,
    ):
        &#34;&#34;&#34;
        Parameters
        ----------
        name:
            Name of the score
        dataset : Dataset
            Dataset with document collection, or path to dataset
            (any model passed to `call()` is supposed to be trained on it)
        keep_dataset_in_memory
            Whether to keep `dataset` in memory or not
            (parameter `_small_data` of the `dataset` object).
            If `dataset` is given as object of type `Dataset` (and not as `str` path to dataset),
            the parameter will be set equal to `dataset._small_data`.
            Otherwise, the default value is `True` and `dataset._small_data` will be overwritten.
        keep_dataset
            Whether to keep `dataset` constantly as inner part of the score,
            or recreate it for each `call()` invocation and then dispose
        documents : list of str
            Which documents from the dataset are to be used for computing coherence
        documents_fraction
            The fraction of all the documents in the Dataset to be used for coherence computation
            if `documents` parameter is not specified
        text_type : TextType
            What text to use when computing coherence: raw text or VW text
            Preferable to use VW (as it is usually preprocessed, stop-words removed etc.),
            and with words in *natural order*.
            Score needs &#34;real&#34; text to compute coherence
        computation_method : ComputationMethod
            The way to compute intra-text coherence
        word_topic_relatedness : WordTopicRelatednessType
            How to estimate word relevance to topic: using p(w | t) or p(t | w)
        specificity_estimation : SpecificityEstimationMethod
            How to estimate specificity of word to topic
        max_num_out_of_topic_words : int
            In case computation_method = ComputationMethod.SEGMENT_LENGTH or
            ComputationMethod.SEGMENT_WEIGHT:
            Maximum number of words not of the topic which can be encountered without stopping
            the process of adding words to the current segment
        window : int
            In case computation_method = ComputationMethod.SUM_OVER_WINDOW:
            Window width. So the window will be the words with positions
            in [current position - window / 2, current position + window / 2)
        start_fit_iteration
            Indicates how many calls are skipped before the actual score is calculated.
            Replaces not calculated values with placeholders
            (for consistency of score values with number of model fit iterations).
        fit_iteration_step
            Number of iterations between `score.call()` invocations which actually update the score
        seed
            Random seed used for documents subsampling if `documents` parameter is not specified
        Notes
        -----
        Parameters `start_fit_iteration` and `fit_iteration_step` are introduced
        to reduce the time needed for one model training.
        If one is interested only in the last score value
        at the end of the training process (and not in the dependence of score on iteration),
        one should adjust `start_fit_iteration` and `fit_iteration_step` correspondingly.
        For example:

        &gt;&gt;&gt; # dataset = Dataset(...)
        &gt;&gt;&gt; # topic_model = TopicModel(...)
        &gt;&gt;&gt; num_iterations = 100
        &gt;&gt;&gt; topic_model.custom_scores[&#39;intratext_coherence&#39;] = IntratextCoherenceScore(
        &gt;&gt;&gt;     dataset,
        &gt;&gt;&gt;     start_fit_iteration=num_iterations - 1  # last iteration: starting from zero
        &gt;&gt;&gt; )
        &gt;&gt;&gt; topic_model._fit(dataset.get_batch_vectorizer(), num_iterations=num_iterations)
        &#34;&#34;&#34;
        # TODO: word_topic_relatedness seems to be connected with TopTokensViewer stuff
        super().__init__(name=name)

        self._keep_dataset = keep_dataset

        if isinstance(dataset, str):
            if keep_dataset_in_memory is None:
                keep_dataset_in_memory = True

            dataset = Dataset(data_path=dataset, keep_in_memory=keep_dataset_in_memory)

        self._keep_dataset_in_memory = dataset._small_data

        if not isinstance(dataset, Dataset):
            raise TypeError(
                f&#39;Got &#34;{type(dataset)}&#34; as \&#34;dataset\&#34;. Expect it to derive from &#34;Dataset&#34;&#39;)

        if not isinstance(text_type, TextType):
            raise TypeError(
                f&#39;Wrong &#34;text_type&#34;: \&#34;{text_type}\&#34;. &#39;
                f&#39;Expect to be \&#34;{TextType}\&#34;&#39;)

        if not isinstance(computation_method, ComputationMethod):
            raise TypeError(
                f&#39;Wrong &#34;computation_method&#34;: \&#34;{computation_method}\&#34;. &#39;
                f&#39;Expect to be \&#34;{ComputationMethod}\&#34;&#39;)

        if not isinstance(word_topic_relatedness, WordTopicRelatednessType):
            raise TypeError(
                f&#39;Wrong &#34;word_topic_relatedness&#34;: \&#34;{word_topic_relatedness}\&#34;. &#39;
                f&#39;Expect to be \&#34;{WordTopicRelatednessType}\&#34;&#39;)

        if not isinstance(specificity_estimation, SpecificityEstimationMethod):
            raise TypeError(
                f&#39;Wrong &#34;specificity_estimation&#34;: \&#34;{specificity_estimation}\&#34;. &#39;
                f&#39;Expect to be \&#34;{SpecificityEstimationMethod}\&#34;&#39;)

        if not isinstance(max_num_out_of_topic_words, int):
            raise TypeError(
                f&#39;Wrong &#34;max_num_out_of_topic_words&#34;: \&#34;{max_num_out_of_topic_words}\&#34;. &#39;
                f&#39;Expect to be \&#34;int\&#34;&#39;)

        if not isinstance(window, int):
            raise TypeError(f&#39;Wrong &#34;window&#34;: \&#34;{window}\&#34;. Expect to be \&#34;int\&#34;&#39;)

        if window &lt; 0 or (window == 0 and computation_method == ComputationMethod.SUM_OVER_WINDOW):
            raise ValueError(
                f&#39;Wrong value for &#34;window&#34;: \&#34;{window}\&#34;. &#39;
                f&#39;Expect to be non-negative. And greater than zero in case &#39;
                f&#39;computation_method == ComputationMethod.SUM_OVER_WINDOW&#39;)

        if not isinstance(start_fit_iteration, int):
            raise TypeError(
                f&#39;Wrong &#34;start_fit_iteration&#34;: \&#34;{start_fit_iteration}\&#34;.&#39;
                f&#39; Expect to be \&#34;int\&#34;&#39;
            )

        if not isinstance(fit_iteration_step, int):
            raise TypeError(
                f&#39;Wrong &#34;fit_iteration_step&#34;: \&#34;{start_fit_iteration}\&#34;.&#39;
                f&#39; Expect to be \&#34;int\&#34;&#39;
            )
        if fit_iteration_step &lt;= 0:
            raise ValueError(
                f&#39;Wrong &#34;fit_iteration_step&#34;: \&#34;{fit_iteration_step}\&#34;.&#39;
                f&#39; Expect to be &gt; 0&#39;
            )

        if documents_fraction &lt;= 0:
            raise ValueError(
                f&#39;Wrong &#34;documents_fraction&#34;: \&#34;{documents_fraction}\&#34;.&#39;
                f&#39; Expect to be in (0, 1]&#39;
            )
        if documents_fraction &gt; 1.0:
            warnings.warn(
                f&#39;Parameter documents_fraction={documents_fraction} can\&#39;t be bigger than 1.0&#39;
                f&#39; Setting it equal to 1.0&#39;
            )

            documents_fraction = 1.0

        self._dataset = dataset
        self._dataset_file_path = dataset._data_path
        self._dataset_internals_folder_path = dataset._internals_folder_path

        self._text_type = text_type
        self._computation_method = computation_method
        self._word_topic_relatedness = word_topic_relatedness
        self._specificity_estimation_method = specificity_estimation
        self._max_num_out_of_topic_words = max_num_out_of_topic_words
        self._window = window

        self._verbose = verbose

        self._current_iteration = 0
        self._start_fit_iteration = start_fit_iteration
        self._fit_iteration_step = fit_iteration_step

        if documents is not None:
            self._documents = documents
        else:
            all_documents = list(self._dataset.get_dataset().index)
            documents_fraction = min(documents_fraction, 1.0)
            num_documents_to_choose = int(
                np.ceil(len(all_documents) * documents_fraction)
            )
            custom_random = np.random.RandomState(seed)

            self._documents = list(
                custom_random.choice(
                    all_documents,
                    size=num_documents_to_choose,
                    replace=False
                )
            )

    def __repr__(self):
        return (f&#39;{self.__class__.__name__}(&#39;
                f&#39;text_type={self._text_type!r}&#39;
                f&#39;computation_method={self._computation_method!r}&#39;
                f&#39;word_topic_relatedness={self._word_topic_relatedness!r}&#39;
                f&#39;specificity_estimation_method={self._specificity_estimation_method!r}&#39;
                f&#39;max_num_out_of_topic_words={self._max_num_out_of_topic_words!r}&#39;
                f&#39;window={self._window!r}&#39;
                f&#39;)&#39;)

    @property
    def dataset(self) -&gt; Dataset:
        return self._dataset

    @dataset.setter
    def dataset(self, new_dataset: Dataset) -&gt; None:
        self._dataset = new_dataset
        self._dataset_file_path = new_dataset._data_path
        self._dataset_internals_folder_path = new_dataset._internals_folder_path
        self._keep_dataset_in_memory = new_dataset._small_data

    def save(self, path: str) -&gt; None:
        dataset = self._dataset
        self._dataset = None

        with open(path, &#39;wb&#39;) as f:
            dill.dump(self, f)

        self._dataset = dataset

    @classmethod
    def load(cls, path: str):
        &#34;&#34;&#34;

        Parameters
        ----------
        path

        Returns
        -------
        IntratextCoherenceScore

        &#34;&#34;&#34;
        score: IntratextCoherenceScore

        with open(path, &#39;rb&#39;) as f:
            score = dill.load(f)

        if not score._keep_dataset:
            score._dataset = None
        else:
            score._dataset = Dataset(
                score._dataset_file_path,
                internals_folder_path=score._dataset_internals_folder_path,
                keep_in_memory=score._keep_dataset_in_memory,
            )

        return score

    def call(self, model: BaseModel) -&gt; float:
        if (self._current_iteration - self._start_fit_iteration) % self._fit_iteration_step != 0:
            self._current_iteration += 1

            return float(&#39;nan&#39;)

        try:
            if self._dataset is None:
                self._dataset = Dataset(
                    self._dataset_file_path,
                    internals_folder_path=self._dataset_internals_folder_path,
                    keep_in_memory=self._keep_dataset_in_memory,
                )

            topic_coherences = self.compute(model, None)

            coherence_values = list(
                v if v is not None else 0.0  # TODO: state the behavior clearer somehow
                for v in topic_coherences.values()
            )

            self._current_iteration += 1

            return float(np.median(coherence_values))  # TODO: or mean?

        finally:
            if not self._keep_dataset:
                self._dataset = None

    def compute(
            self,
            model: BaseModel,
            topics: List[str] = None,
            documents: List[str] = None
    ) -&gt; Dict[str, Optional[float]]:

        if not isinstance(model, BaseModel):
            raise TypeError(
                f&#39;Got &#34;{type(model)}&#34; as &#34;model&#34;. &#39;
                f&#39;Expect it to derive from &#34;BaseModel&#34;&#39;)

        if topics is None:
            topics = IntratextCoherenceScore._get_topics(model)

        if documents is None:
            documents = list(self._documents)

        if not isinstance(topics, list):
            raise TypeError(
                f&#39;Got &#34;{type(topics)}&#34; as &#34;topics&#34;. Expect list of topic names&#39;)

        if not isinstance(documents, list):
            raise TypeError(
                f&#39;Got &#34;{type(documents)}&#34; as &#34;documents&#34;. Expect list of document ids&#39;)

        word_topic_relatednesses = self._get_word_topic_relatednesses(model)

        topic_document_coherences = np.zeros((len(topics), len(documents)))
        document_indices_with_topic_coherence = defaultdict(list)

        if not self._verbose:
            document_enumeration = enumerate(documents)
        else:
            document_enumeration = tqdm.tqdm(
                enumerate(documents), total=len(documents), file=sys.stdout
            )

        for document_index, document in document_enumeration:
            for topic_index, topic in enumerate(topics):
                # TODO: read document text only once for all topics
                topic_coherence = self._compute_coherence(
                    topic, document, word_topic_relatednesses)

                if topic_coherence is not None:
                    topic_document_coherences[topic_index, document_index] = topic_coherence
                    document_indices_with_topic_coherence[topic].append(document_index)

        topic_coherences = [
            topic_document_coherences[topic_index, document_indices_with_topic_coherence[topic]]
            if len(document_indices_with_topic_coherence) &gt; 0 else list()
            for topic_index, topic in enumerate(topics)
        ]

        return dict(zip(
            topics,
            [float(np.mean(coherence_values))
             if len(coherence_values) &gt; 0 else None
             for coherence_values in topic_coherences]
        ))

    @staticmethod
    def _get_topics(model):
        return list(model.get_phi().columns)

    def _get_word_topic_relatednesses(self, model) -&gt; pd.DataFrame:
        phi = model.get_phi()

        word_topic_probs = self._get_word_topic_probs(phi)

        if self._specificity_estimation_method == SpecificityEstimationMethod.NONE:
            pass

        elif self._specificity_estimation_method == SpecificityEstimationMethod.AVERAGE:
            word_topic_probs[:] = (
                word_topic_probs.values -
                    np.sum(word_topic_probs.values, axis=1, keepdims=True) /  # noqa E131
                        max(word_topic_probs.shape[1], 1)  # noqa E131
            )

        elif self._specificity_estimation_method == SpecificityEstimationMethod.MAXIMUM:
            new_columns = []

            for t in word_topic_probs.columns:
                new_column = (
                    word_topic_probs[t].values -
                    np.max(
                        word_topic_probs[word_topic_probs.columns.difference([t])].values, axis=1)
                )
                new_columns.append(list(new_column))

            word_topic_probs[:] = np.array(new_columns).T

        return word_topic_probs

    def _get_word_topic_probs(self, phi: pd.DataFrame) -&gt; pd.DataFrame:
        if self._word_topic_relatedness == WordTopicRelatednessType.PWT:
            return phi

        elif self._word_topic_relatedness == WordTopicRelatednessType.PTW:
            # Treat all topics as equally probable
            eps = np.finfo(float).tiny

            pwt = phi
            pwt_values = pwt.values

            return pd.DataFrame(
                index=pwt.index,
                columns=pwt.columns,
                data=pwt_values / (pwt_values.sum(axis=1).reshape(-1, 1) + eps)
            )

        assert False

    def _compute_coherence(self, topic, document, word_topic_relatednesses):
        assert isinstance(self._computation_method, ComputationMethod)

        words = self._get_words(document)

        if self._computation_method == ComputationMethod.SUM_OVER_WINDOW:
            average_sum_over_window = self._sum_relatednesses_over_window(
                topic, words, word_topic_relatednesses
            )

            return average_sum_over_window

        topic_segment_length, topic_segment_weight = self._compute_segment_characteristics(
            topic, words, word_topic_relatednesses
        )

        if self._computation_method == ComputationMethod.SEGMENT_LENGTH:
            return topic_segment_length

        elif self._computation_method == ComputationMethod.SEGMENT_WEIGHT:
            return topic_segment_weight

    def _get_words(self, document):
        def get_biggest_modality_or_default():
            modalities = list(self._dataset.get_possible_modalities())

            if len(modalities) == 0:
                return DEFAULT_ARTM_MODALITY

            modalities_vocabulary_sizes = list(map(
                lambda m: self._dataset.get_dataset().loc[m].shape[0],
                modalities
            ))

            return modalities[np.argmax(modalities_vocabulary_sizes)]

        if self._text_type == TextType.RAW_TEXT:
            text = self._dataset.get_source_document(document).values[0, 0]  # TODO: this way?
            modality = get_biggest_modality_or_default()

            return list(map(lambda w: (modality, w), text.split()))

        if self._text_type == TextType.VW_TEXT:
            text = self._dataset.get_vw_document(document).values[0, 0]  # TODO: this way?

            words = []
            modality = None

            # TODO: there was similar bunch of code somewhere...
            for word in text.split()[1:]:  # skip document id
                if word.startswith(MODALITY_START_SYMBOL):
                    modality = word[1:]

                    continue

                word = word.split(&#39;:&#39;)[0]

                if modality is not None:
                    word = (modality, word)  # phi multiIndex
                else:
                    word = (DEFAULT_ARTM_MODALITY, word)

                words.append(word)

            return words

        assert False

    def _compute_segment_characteristics(
            self, topic, words, word_topic_relatednesses: pd.DataFrame
    ) -&gt; Tuple[float, float]:

        topic_segment_lengths = []
        topic_segment_weights = []

        topic_index = word_topic_relatednesses.columns.get_loc(topic)
        word_topic_indices = np.argmax(word_topic_relatednesses.values, axis=1)

        def get_word_topic_index(word):
            if word not in word_topic_relatednesses.index:
                return -1
            else:
                return word_topic_indices[
                    word_topic_relatednesses.index.get_loc(word)
                ]

        index = 0

        while index &lt; len(words):
            original_index = index

            if get_word_topic_index(words[index]) != topic_index:
                index += 1

                continue

            segment_length = 1
            segment_weight = IntratextCoherenceScore._get_relatedness(
                words[index], topic, word_topic_relatednesses
            )

            num_out_of_topic_words = 0

            index += 1

            while index &lt; len(words) and num_out_of_topic_words &lt; self._max_num_out_of_topic_words:
                if get_word_topic_index(words[index]) != topic_index:
                    num_out_of_topic_words += 1
                else:
                    segment_length += 1
                    segment_weight += IntratextCoherenceScore._get_relatedness(
                        words[index], topic, word_topic_relatednesses
                    )

                    num_out_of_topic_words = 0

                index += 1

            topic_segment_lengths.append(segment_length)
            topic_segment_weights.append(segment_weight)

            assert index &gt; original_index

        if len(topic_segment_lengths) == 0:
            return None, None
        else:
            return np.mean(topic_segment_lengths), np.mean(topic_segment_weights)

    def _sum_relatednesses_over_window(
            self, topic, words, word_topic_relatednesses) -&gt; float:

        topic_index = word_topic_relatednesses.columns.get_loc(topic)
        word_topic_indices = np.argmax(word_topic_relatednesses.values, axis=1)

        def get_word_topic_index(word):
            if word not in word_topic_relatednesses.index:
                return -1
            else:
                return word_topic_indices[
                    word_topic_relatednesses.index.get_loc(word)
                ]

        def find_next_topic_word(starting_index: int) -&gt; int:
            index = starting_index

            while index &lt; len(words) and\
                    get_word_topic_index(words[index]) != topic_index:
                index += 1

            if index == len(words):
                return -1  # failed to find next topic word

            return index

        word_index = find_next_topic_word(0)

        if word_index == -1:
            return None

        sums = list()

        while word_index &lt; len(words) and word_index != -1:
            original_word_index = word_index

            window_lower_bound = word_index - int(np.floor(self._window // 2))
            window_upper_bound = word_index + int(np.ceil(self._window // 2))

            sum_in_window = np.sum(
                [
                    IntratextCoherenceScore._get_relatedness(
                        w, topic, word_topic_relatednesses
                    )
                    for w in words[window_lower_bound:window_upper_bound]
                ]
            )

            sums.append(sum_in_window)

            word_index = find_next_topic_word(window_upper_bound)

            assert word_index &gt; original_word_index or word_index == -1

        return np.mean(sums)

    @staticmethod
    def _get_relatedness(
            word, topic, word_topic_relatednesses: pd.DataFrame) -&gt; float:

        if word in word_topic_relatednesses.index:
            return word_topic_relatednesses.loc[word, topic]

        # TODO: throw Warning or log somewhere?
        return np.mean(word_topic_relatednesses.values)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="topicnet.cooking_machine.models.base_score.BaseScore" href="base_score.html#topicnet.cooking_machine.models.base_score.BaseScore">BaseScore</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="topicnet.cooking_machine.models.intratext_coherence_score.IntratextCoherenceScore.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>path)</span>
</code></dt>
<dd>
<section class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>path</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><a title="topicnet.cooking_machine.models.intratext_coherence_score.IntratextCoherenceScore" href="#topicnet.cooking_machine.models.intratext_coherence_score.IntratextCoherenceScore"><code>IntratextCoherenceScore</code></a></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">@classmethod
def load(cls, path: str):
    &#34;&#34;&#34;

    Parameters
    ----------
    path

    Returns
    -------
    IntratextCoherenceScore

    &#34;&#34;&#34;
    score: IntratextCoherenceScore

    with open(path, &#39;rb&#39;) as f:
        score = dill.load(f)

    if not score._keep_dataset:
        score._dataset = None
    else:
        score._dataset = Dataset(
            score._dataset_file_path,
            internals_folder_path=score._dataset_internals_folder_path,
            keep_in_memory=score._keep_dataset_in_memory,
        )

    return score</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="topicnet.cooking_machine.models.intratext_coherence_score.IntratextCoherenceScore.dataset"><code class="name">var <span class="ident">dataset</span></code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">@property
def dataset(self) -&gt; Dataset:
    return self._dataset</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="topicnet.cooking_machine.models.intratext_coherence_score.IntratextCoherenceScore.compute"><code class="name flex">
<span>def <span class="ident">compute</span></span>(<span>self, model, topics=None, documents=None)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def compute(
        self,
        model: BaseModel,
        topics: List[str] = None,
        documents: List[str] = None
) -&gt; Dict[str, Optional[float]]:

    if not isinstance(model, BaseModel):
        raise TypeError(
            f&#39;Got &#34;{type(model)}&#34; as &#34;model&#34;. &#39;
            f&#39;Expect it to derive from &#34;BaseModel&#34;&#39;)

    if topics is None:
        topics = IntratextCoherenceScore._get_topics(model)

    if documents is None:
        documents = list(self._documents)

    if not isinstance(topics, list):
        raise TypeError(
            f&#39;Got &#34;{type(topics)}&#34; as &#34;topics&#34;. Expect list of topic names&#39;)

    if not isinstance(documents, list):
        raise TypeError(
            f&#39;Got &#34;{type(documents)}&#34; as &#34;documents&#34;. Expect list of document ids&#39;)

    word_topic_relatednesses = self._get_word_topic_relatednesses(model)

    topic_document_coherences = np.zeros((len(topics), len(documents)))
    document_indices_with_topic_coherence = defaultdict(list)

    if not self._verbose:
        document_enumeration = enumerate(documents)
    else:
        document_enumeration = tqdm.tqdm(
            enumerate(documents), total=len(documents), file=sys.stdout
        )

    for document_index, document in document_enumeration:
        for topic_index, topic in enumerate(topics):
            # TODO: read document text only once for all topics
            topic_coherence = self._compute_coherence(
                topic, document, word_topic_relatednesses)

            if topic_coherence is not None:
                topic_document_coherences[topic_index, document_index] = topic_coherence
                document_indices_with_topic_coherence[topic].append(document_index)

    topic_coherences = [
        topic_document_coherences[topic_index, document_indices_with_topic_coherence[topic]]
        if len(document_indices_with_topic_coherence) &gt; 0 else list()
        for topic_index, topic in enumerate(topics)
    ]

    return dict(zip(
        topics,
        [float(np.mean(coherence_values))
         if len(coherence_values) &gt; 0 else None
         for coherence_values in topic_coherences]
    ))</code></pre>
</details>
</dd>
<dt id="topicnet.cooking_machine.models.intratext_coherence_score.IntratextCoherenceScore.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, path)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def save(self, path: str) -&gt; None:
    dataset = self._dataset
    self._dataset = None

    with open(path, &#39;wb&#39;) as f:
        dill.dump(self, f)

    self._dataset = dataset</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="topicnet.cooking_machine.models.base_score.BaseScore" href="base_score.html#topicnet.cooking_machine.models.base_score.BaseScore">BaseScore</a></b></code>:
<ul class="hlist">
<li><code><a title="topicnet.cooking_machine.models.base_score.BaseScore.call" href="base_score.html#topicnet.cooking_machine.models.base_score.BaseScore.call">call</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.base_score.BaseScore.update" href="base_score.html#topicnet.cooking_machine.models.base_score.BaseScore.update">update</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="topicnet.cooking_machine.models.intratext_coherence_score.SpecificityEstimationMethod"><code class="flex name class">
<span>class <span class="ident">SpecificityEstimationMethod</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Way to estimate how particular word is specific for particular topic.
Unlike probability, eg. p(w | t), specificity_estimation takes into account
values for all topics, eg. p(w | t_1), p(w | t_2), &hellip;, p(w | t_n):
the higher the value p(w | t) comparing other p(w | t_i),
the higher the specificity_estimation of word "w" for the topic "t"</p>
<h2 id="attributes">Attributes</h2>
<p>NONE :
Don't try to estimate specificity_estimation, return the probability as is
MAXIMUM :
From probability, corresponding to word and topic,
extract <em>maximum</em> among probabilities for the word and other topics
AVERAGE :
From probability, corresponding to word and topic,
extract <em>average</em> among probabilities for the word and other topics</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class SpecificityEstimationMethod(IntEnum):
    &#34;&#34;&#34;
    Way to estimate how particular word is specific for particular topic.
    Unlike probability, eg. p(w | t), specificity_estimation takes into account
    values for all topics, eg. p(w | t_1), p(w | t_2), ..., p(w | t_n):
    the higher the value p(w | t) comparing other p(w | t_i),
    the higher the specificity_estimation of word &#34;w&#34; for the topic &#34;t&#34;

    Attributes
    ----------
    NONE :
        Don&#39;t try to estimate specificity_estimation, return the probability as is
    MAXIMUM :
        From probability, corresponding to word and topic,
        extract *maximum* among probabilities for the word and other topics
    AVERAGE :
        From probability, corresponding to word and topic,
        extract *average* among probabilities for the word and other topics
    &#34;&#34;&#34;
    NONE = auto()
    MAXIMUM = auto()
    AVERAGE = auto()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.IntEnum</li>
<li>builtins.int</li>
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="topicnet.cooking_machine.models.intratext_coherence_score.SpecificityEstimationMethod.AVERAGE"><code class="name">var <span class="ident">AVERAGE</span></code></dt>
<dd>
<section class="desc"></section>
</dd>
<dt id="topicnet.cooking_machine.models.intratext_coherence_score.SpecificityEstimationMethod.MAXIMUM"><code class="name">var <span class="ident">MAXIMUM</span></code></dt>
<dd>
<section class="desc"></section>
</dd>
<dt id="topicnet.cooking_machine.models.intratext_coherence_score.SpecificityEstimationMethod.NONE"><code class="name">var <span class="ident">NONE</span></code></dt>
<dd>
<section class="desc"></section>
</dd>
</dl>
</dd>
<dt id="topicnet.cooking_machine.models.intratext_coherence_score.TextType"><code class="flex name class">
<span>class <span class="ident">TextType</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>An enumeration.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class TextType(Enum):
    VW_TEXT = VW_TEXT_COL
    RAW_TEXT = RAW_TEXT_COL</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="topicnet.cooking_machine.models.intratext_coherence_score.TextType.RAW_TEXT"><code class="name">var <span class="ident">RAW_TEXT</span></code></dt>
<dd>
<section class="desc"></section>
</dd>
<dt id="topicnet.cooking_machine.models.intratext_coherence_score.TextType.VW_TEXT"><code class="name">var <span class="ident">VW_TEXT</span></code></dt>
<dd>
<section class="desc"></section>
</dd>
</dl>
</dd>
<dt id="topicnet.cooking_machine.models.intratext_coherence_score.WordTopicRelatednessType"><code class="flex name class">
<span>class <span class="ident">WordTopicRelatednessType</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Word-topic relatedness estimate</p>
<h2 id="attributes">Attributes</h2>
<p>PWT :
p(w | t)
PTW :
p(t | w)</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class WordTopicRelatednessType(IntEnum):
    &#34;&#34;&#34;
    Word-topic relatedness estimate

    Attributes
    ----------
    PWT :
        p(w | t)
    PTW :
        p(t | w)
    &#34;&#34;&#34;
    PWT = auto()
    PTW = auto()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.IntEnum</li>
<li>builtins.int</li>
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="topicnet.cooking_machine.models.intratext_coherence_score.WordTopicRelatednessType.PTW"><code class="name">var <span class="ident">PTW</span></code></dt>
<dd>
<section class="desc"></section>
</dd>
<dt id="topicnet.cooking_machine.models.intratext_coherence_score.WordTopicRelatednessType.PWT"><code class="name">var <span class="ident">PWT</span></code></dt>
<dd>
<section class="desc"></section>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="topicnet.cooking_machine.models" href="index.html">topicnet.cooking_machine.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="topicnet.cooking_machine.models.intratext_coherence_score.ComputationMethod" href="#topicnet.cooking_machine.models.intratext_coherence_score.ComputationMethod">ComputationMethod</a></code></h4>
<ul class="">
<li><code><a title="topicnet.cooking_machine.models.intratext_coherence_score.ComputationMethod.SEGMENT_LENGTH" href="#topicnet.cooking_machine.models.intratext_coherence_score.ComputationMethod.SEGMENT_LENGTH">SEGMENT_LENGTH</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.intratext_coherence_score.ComputationMethod.SEGMENT_WEIGHT" href="#topicnet.cooking_machine.models.intratext_coherence_score.ComputationMethod.SEGMENT_WEIGHT">SEGMENT_WEIGHT</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.intratext_coherence_score.ComputationMethod.SUM_OVER_WINDOW" href="#topicnet.cooking_machine.models.intratext_coherence_score.ComputationMethod.SUM_OVER_WINDOW">SUM_OVER_WINDOW</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="topicnet.cooking_machine.models.intratext_coherence_score.IntratextCoherenceScore" href="#topicnet.cooking_machine.models.intratext_coherence_score.IntratextCoherenceScore">IntratextCoherenceScore</a></code></h4>
<ul class="">
<li><code><a title="topicnet.cooking_machine.models.intratext_coherence_score.IntratextCoherenceScore.compute" href="#topicnet.cooking_machine.models.intratext_coherence_score.IntratextCoherenceScore.compute">compute</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.intratext_coherence_score.IntratextCoherenceScore.dataset" href="#topicnet.cooking_machine.models.intratext_coherence_score.IntratextCoherenceScore.dataset">dataset</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.intratext_coherence_score.IntratextCoherenceScore.load" href="#topicnet.cooking_machine.models.intratext_coherence_score.IntratextCoherenceScore.load">load</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.intratext_coherence_score.IntratextCoherenceScore.save" href="#topicnet.cooking_machine.models.intratext_coherence_score.IntratextCoherenceScore.save">save</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="topicnet.cooking_machine.models.intratext_coherence_score.SpecificityEstimationMethod" href="#topicnet.cooking_machine.models.intratext_coherence_score.SpecificityEstimationMethod">SpecificityEstimationMethod</a></code></h4>
<ul class="">
<li><code><a title="topicnet.cooking_machine.models.intratext_coherence_score.SpecificityEstimationMethod.AVERAGE" href="#topicnet.cooking_machine.models.intratext_coherence_score.SpecificityEstimationMethod.AVERAGE">AVERAGE</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.intratext_coherence_score.SpecificityEstimationMethod.MAXIMUM" href="#topicnet.cooking_machine.models.intratext_coherence_score.SpecificityEstimationMethod.MAXIMUM">MAXIMUM</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.intratext_coherence_score.SpecificityEstimationMethod.NONE" href="#topicnet.cooking_machine.models.intratext_coherence_score.SpecificityEstimationMethod.NONE">NONE</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="topicnet.cooking_machine.models.intratext_coherence_score.TextType" href="#topicnet.cooking_machine.models.intratext_coherence_score.TextType">TextType</a></code></h4>
<ul class="">
<li><code><a title="topicnet.cooking_machine.models.intratext_coherence_score.TextType.RAW_TEXT" href="#topicnet.cooking_machine.models.intratext_coherence_score.TextType.RAW_TEXT">RAW_TEXT</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.intratext_coherence_score.TextType.VW_TEXT" href="#topicnet.cooking_machine.models.intratext_coherence_score.TextType.VW_TEXT">VW_TEXT</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="topicnet.cooking_machine.models.intratext_coherence_score.WordTopicRelatednessType" href="#topicnet.cooking_machine.models.intratext_coherence_score.WordTopicRelatednessType">WordTopicRelatednessType</a></code></h4>
<ul class="">
<li><code><a title="topicnet.cooking_machine.models.intratext_coherence_score.WordTopicRelatednessType.PTW" href="#topicnet.cooking_machine.models.intratext_coherence_score.WordTopicRelatednessType.PTW">PTW</a></code></li>
<li><code><a title="topicnet.cooking_machine.models.intratext_coherence_score.WordTopicRelatednessType.PWT" href="#topicnet.cooking_machine.models.intratext_coherence_score.WordTopicRelatednessType.PWT">PWT</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.6.3</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>
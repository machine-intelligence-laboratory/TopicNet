<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.0" />
<title>topicnet.cooking_machine.recipes.intratext_coherence_pipeline API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>topicnet.cooking_machine.recipes.intratext_coherence_pipeline</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import os
import warnings

from typing import List

from .recipe_wrapper import BaseRecipe
from .. import Dataset

ONE_CONFIG_INDENT = 4 * &#39; &#39;


class IntratextCoherenceRecipe(BaseRecipe):
    &#34;&#34;&#34;
    The recipe mainly consists of basic cube stages,
    such as Decorrelation, Sparsing and Smoothing.
    In this way it is similar to ARTM baseline recipe.
    The core difference is that models selected based on their IntratextCoherenceScore
    (which is one of the scores included in TopicNet).
    PerplexityScore is also calculated to assure that models don&#39;t have high perplexity,
    but the main criteria is IntratextCoherenceScore.

    For more details about IntratextCoherence
    one may see the paper http://www.dialog-21.ru/media/4281/alekseevva.pdf

    &#34;&#34;&#34;
    def __init__(self):
        recipe_template_path = os.path.join(
            os.path.dirname(os.path.abspath(__file__)),
            &#39;intratext_coherence_maximization.yml&#39;
        )
        recipe_template = open(recipe_template_path, &#39;r&#39;).read()

        super().__init__(recipe_template=recipe_template)

    def format_recipe(
            self,
            dataset_path: str,
            num_specific_topics: int,
            main_modality: str = None,
            dictionary_filter_parameters: dict = None,
            num_background_topics: int = 1,
            modalities: List[str] = None,
            keep_dataset_in_memory: bool = True,
            keep_dataset: bool = False,
            documents_fraction: float = 0.5,
            one_stage_num_iter: int = 20,
            verbose: bool = True) -&gt; str:
        &#34;&#34;&#34;

        Parameters
        ----------
        dataset_path
            Path to the dataset .csv file
        num_specific_topics
            Number of specific topics in models to be trained
        main_modality
            Main modality in the dataset
            (usually it is plain text, and not, for example, @author or @title)
            If not specified, it will be the first modality in `modalities`
        num_background_topics
            Number of background topics in models
        modalities
            What modalities to use from those that are in the dataset.
            If not specified, all dataset&#39;s modalities will be used.
            If specified, should be non empty
        keep_dataset_in_memory
            Whether or not to keep dataset in memory when running experiment.
            True is faster, so, if dataset is not very huge, it is better to use True
        keep_dataset
            If True, the dataset will be loaded in memory only when computing coherence.
            So, memory will be free of the dataset during model training.
            This may help if the dataset is fairly big,
            but `keep_dataset_in_memory=True` still works without crash.
        documents_fraction
            Determines the number of documents that will be used for computing coherence.
            Better keep this one less than 1.0.
            For example, suppose we want to use not all dataset,
            but just a fragment of 25,000 words.
            Then we can do like so

            &gt;&gt;&gt; document_lengths = dataset._data[&#39;vw_text&#39;].apply(lambda text: len(text.split()))
            &gt;&gt;&gt; median_document_length = np.median(document_lengths)
            &gt;&gt;&gt; num_documents = dataset._data.shape[0]
            &gt;&gt;&gt; dataset_fragment_length = 25000
            &gt;&gt;&gt; num_documents_for_computing = dataset_fragment_length / median_document_length
            &gt;&gt;&gt; documents_fraction = num_documents_for_computing / num_documents

        one_stage_num_iter
            There will be five stages, each with nearly 5-values-grid search.
            One such search lasts `one_stage_num_iter` iterations
            with coherence computation in the end.
            So, there is going to be `one_stage_num_iter` * 5 * 5 training iterations (not slow)
            and 5 * 5 coherence computations (here may be slow if `documents_fraction` is high)
        verbose
            Whether to show experiment progress or not

        &#34;&#34;&#34;
        all_modalities = list(Dataset(dataset_path).get_possible_modalities())

        if len(all_modalities) == 0:
            warnings.warn(f&#39;No modalities in the dataset &#34;{dataset_path}&#34;!&#39;)

        if modalities is None:
            modalities = all_modalities
        if any([m not in all_modalities for m in modalities]):
            warnings.warn(f&#39;Not all `modalities` are found in the dataset &#34;{dataset_path}&#34;!&#39;)

        if main_modality is None:
            main_modality = modalities[0]

            warnings.warn(
                f&#39;Main modality not specified!&#39;
                f&#39; So modality &#34;{main_modality}&#34; will be used as the main one&#39;
            )

        specific_topics = [
            f&#39;topic_{i}&#39; for i in range(num_specific_topics)
        ]
        background_topics = [
            f&#39;bcg_topic_{i}&#39;
            for i in range(num_specific_topics, num_specific_topics + num_background_topics)
        ]

        if dictionary_filter_parameters is None:
            dictionary_filter_parameters = dict()

        dictionary_filter_parameters_as_yml = self._format_dictionary_filter_parameters(
            dictionary_filter_parameters,
            indent=2 * ONE_CONFIG_INDENT,
        )

        self._recipe = self.recipe_template.format(
            modality_names=modalities,
            main_modality=main_modality,
            dataset_path=dataset_path,
            dictionary_filter_parameters=dictionary_filter_parameters_as_yml,
            keep_dataset_in_memory=keep_dataset_in_memory,
            keep_dataset=keep_dataset,
            documents_fraction=documents_fraction,
            specific_topics=specific_topics,
            background_topics=background_topics,
            one_stage_num_iter=one_stage_num_iter,
            verbose=verbose,
        )

        return self._recipe</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="topicnet.cooking_machine.recipes.intratext_coherence_pipeline.IntratextCoherenceRecipe"><code class="flex name class">
<span>class <span class="ident">IntratextCoherenceRecipe</span></span>
</code></dt>
<dd>
<div class="desc"><p>The recipe mainly consists of basic cube stages,
such as Decorrelation, Sparsing and Smoothing.
In this way it is similar to ARTM baseline recipe.
The core difference is that models selected based on their IntratextCoherenceScore
(which is one of the scores included in TopicNet).
PerplexityScore is also calculated to assure that models don't have high perplexity,
but the main criteria is IntratextCoherenceScore.</p>
<p>For more details about IntratextCoherence
one may see the paper <a href="http://www.dialog-21.ru/media/4281/alekseevva.pdf">http://www.dialog-21.ru/media/4281/alekseevva.pdf</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class IntratextCoherenceRecipe(BaseRecipe):
    &#34;&#34;&#34;
    The recipe mainly consists of basic cube stages,
    such as Decorrelation, Sparsing and Smoothing.
    In this way it is similar to ARTM baseline recipe.
    The core difference is that models selected based on their IntratextCoherenceScore
    (which is one of the scores included in TopicNet).
    PerplexityScore is also calculated to assure that models don&#39;t have high perplexity,
    but the main criteria is IntratextCoherenceScore.

    For more details about IntratextCoherence
    one may see the paper http://www.dialog-21.ru/media/4281/alekseevva.pdf

    &#34;&#34;&#34;
    def __init__(self):
        recipe_template_path = os.path.join(
            os.path.dirname(os.path.abspath(__file__)),
            &#39;intratext_coherence_maximization.yml&#39;
        )
        recipe_template = open(recipe_template_path, &#39;r&#39;).read()

        super().__init__(recipe_template=recipe_template)

    def format_recipe(
            self,
            dataset_path: str,
            num_specific_topics: int,
            main_modality: str = None,
            dictionary_filter_parameters: dict = None,
            num_background_topics: int = 1,
            modalities: List[str] = None,
            keep_dataset_in_memory: bool = True,
            keep_dataset: bool = False,
            documents_fraction: float = 0.5,
            one_stage_num_iter: int = 20,
            verbose: bool = True) -&gt; str:
        &#34;&#34;&#34;

        Parameters
        ----------
        dataset_path
            Path to the dataset .csv file
        num_specific_topics
            Number of specific topics in models to be trained
        main_modality
            Main modality in the dataset
            (usually it is plain text, and not, for example, @author or @title)
            If not specified, it will be the first modality in `modalities`
        num_background_topics
            Number of background topics in models
        modalities
            What modalities to use from those that are in the dataset.
            If not specified, all dataset&#39;s modalities will be used.
            If specified, should be non empty
        keep_dataset_in_memory
            Whether or not to keep dataset in memory when running experiment.
            True is faster, so, if dataset is not very huge, it is better to use True
        keep_dataset
            If True, the dataset will be loaded in memory only when computing coherence.
            So, memory will be free of the dataset during model training.
            This may help if the dataset is fairly big,
            but `keep_dataset_in_memory=True` still works without crash.
        documents_fraction
            Determines the number of documents that will be used for computing coherence.
            Better keep this one less than 1.0.
            For example, suppose we want to use not all dataset,
            but just a fragment of 25,000 words.
            Then we can do like so

            &gt;&gt;&gt; document_lengths = dataset._data[&#39;vw_text&#39;].apply(lambda text: len(text.split()))
            &gt;&gt;&gt; median_document_length = np.median(document_lengths)
            &gt;&gt;&gt; num_documents = dataset._data.shape[0]
            &gt;&gt;&gt; dataset_fragment_length = 25000
            &gt;&gt;&gt; num_documents_for_computing = dataset_fragment_length / median_document_length
            &gt;&gt;&gt; documents_fraction = num_documents_for_computing / num_documents

        one_stage_num_iter
            There will be five stages, each with nearly 5-values-grid search.
            One such search lasts `one_stage_num_iter` iterations
            with coherence computation in the end.
            So, there is going to be `one_stage_num_iter` * 5 * 5 training iterations (not slow)
            and 5 * 5 coherence computations (here may be slow if `documents_fraction` is high)
        verbose
            Whether to show experiment progress or not

        &#34;&#34;&#34;
        all_modalities = list(Dataset(dataset_path).get_possible_modalities())

        if len(all_modalities) == 0:
            warnings.warn(f&#39;No modalities in the dataset &#34;{dataset_path}&#34;!&#39;)

        if modalities is None:
            modalities = all_modalities
        if any([m not in all_modalities for m in modalities]):
            warnings.warn(f&#39;Not all `modalities` are found in the dataset &#34;{dataset_path}&#34;!&#39;)

        if main_modality is None:
            main_modality = modalities[0]

            warnings.warn(
                f&#39;Main modality not specified!&#39;
                f&#39; So modality &#34;{main_modality}&#34; will be used as the main one&#39;
            )

        specific_topics = [
            f&#39;topic_{i}&#39; for i in range(num_specific_topics)
        ]
        background_topics = [
            f&#39;bcg_topic_{i}&#39;
            for i in range(num_specific_topics, num_specific_topics + num_background_topics)
        ]

        if dictionary_filter_parameters is None:
            dictionary_filter_parameters = dict()

        dictionary_filter_parameters_as_yml = self._format_dictionary_filter_parameters(
            dictionary_filter_parameters,
            indent=2 * ONE_CONFIG_INDENT,
        )

        self._recipe = self.recipe_template.format(
            modality_names=modalities,
            main_modality=main_modality,
            dataset_path=dataset_path,
            dictionary_filter_parameters=dictionary_filter_parameters_as_yml,
            keep_dataset_in_memory=keep_dataset_in_memory,
            keep_dataset=keep_dataset,
            documents_fraction=documents_fraction,
            specific_topics=specific_topics,
            background_topics=background_topics,
            one_stage_num_iter=one_stage_num_iter,
            verbose=verbose,
        )

        return self._recipe</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="topicnet.cooking_machine.recipes.recipe_wrapper.BaseRecipe" href="recipe_wrapper.html#topicnet.cooking_machine.recipes.recipe_wrapper.BaseRecipe">BaseRecipe</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="topicnet.cooking_machine.recipes.intratext_coherence_pipeline.IntratextCoherenceRecipe.format_recipe"><code class="name flex">
<span>def <span class="ident">format_recipe</span></span>(<span>self, dataset_path: str, num_specific_topics: int, main_modality: str = None, dictionary_filter_parameters: dict = None, num_background_topics: int = 1, modalities: List[str] = None, keep_dataset_in_memory: bool = True, keep_dataset: bool = False, documents_fraction: float = 0.5, one_stage_num_iter: int = 20, verbose: bool = True) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset_path</code></strong></dt>
<dd>Path to the dataset .csv file</dd>
<dt><strong><code>num_specific_topics</code></strong></dt>
<dd>Number of specific topics in models to be trained</dd>
<dt><strong><code>main_modality</code></strong></dt>
<dd>Main modality in the dataset
(usually it is plain text, and not, for example, @author or @title)
If not specified, it will be the first modality in <code>modalities</code></dd>
<dt><strong><code>num_background_topics</code></strong></dt>
<dd>Number of background topics in models</dd>
<dt><strong><code>modalities</code></strong></dt>
<dd>What modalities to use from those that are in the dataset.
If not specified, all dataset's modalities will be used.
If specified, should be non empty</dd>
<dt><strong><code>keep_dataset_in_memory</code></strong></dt>
<dd>Whether or not to keep dataset in memory when running experiment.
True is faster, so, if dataset is not very huge, it is better to use True</dd>
<dt><strong><code>keep_dataset</code></strong></dt>
<dd>If True, the dataset will be loaded in memory only when computing coherence.
So, memory will be free of the dataset during model training.
This may help if the dataset is fairly big,
but <code>keep_dataset_in_memory=True</code> still works without crash.</dd>
<dt><strong><code>documents_fraction</code></strong></dt>
<dd>
<p>Determines the number of documents that will be used for computing coherence.
Better keep this one less than 1.0.
For example, suppose we want to use not all dataset,
but just a fragment of 25,000 words.
Then we can do like so</p>
<blockquote>
<blockquote>
<blockquote>
<p>document_lengths = dataset._data['vw_text'].apply(lambda text: len(text.split()))
median_document_length = np.median(document_lengths)
num_documents = dataset._data.shape[0]
dataset_fragment_length = 25000
num_documents_for_computing = dataset_fragment_length / median_document_length
documents_fraction = num_documents_for_computing / num_documents</p>
</blockquote>
</blockquote>
</blockquote>
</dd>
<dt><strong><code>one_stage_num_iter</code></strong></dt>
<dd>There will be five stages, each with nearly 5-values-grid search.
One such search lasts <code>one_stage_num_iter</code> iterations
with coherence computation in the end.
So, there is going to be <code>one_stage_num_iter</code> * 5 * 5 training iterations (not slow)
and 5 * 5 coherence computations (here may be slow if <code>documents_fraction</code> is high)</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>Whether to show experiment progress or not</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def format_recipe(
        self,
        dataset_path: str,
        num_specific_topics: int,
        main_modality: str = None,
        dictionary_filter_parameters: dict = None,
        num_background_topics: int = 1,
        modalities: List[str] = None,
        keep_dataset_in_memory: bool = True,
        keep_dataset: bool = False,
        documents_fraction: float = 0.5,
        one_stage_num_iter: int = 20,
        verbose: bool = True) -&gt; str:
    &#34;&#34;&#34;

    Parameters
    ----------
    dataset_path
        Path to the dataset .csv file
    num_specific_topics
        Number of specific topics in models to be trained
    main_modality
        Main modality in the dataset
        (usually it is plain text, and not, for example, @author or @title)
        If not specified, it will be the first modality in `modalities`
    num_background_topics
        Number of background topics in models
    modalities
        What modalities to use from those that are in the dataset.
        If not specified, all dataset&#39;s modalities will be used.
        If specified, should be non empty
    keep_dataset_in_memory
        Whether or not to keep dataset in memory when running experiment.
        True is faster, so, if dataset is not very huge, it is better to use True
    keep_dataset
        If True, the dataset will be loaded in memory only when computing coherence.
        So, memory will be free of the dataset during model training.
        This may help if the dataset is fairly big,
        but `keep_dataset_in_memory=True` still works without crash.
    documents_fraction
        Determines the number of documents that will be used for computing coherence.
        Better keep this one less than 1.0.
        For example, suppose we want to use not all dataset,
        but just a fragment of 25,000 words.
        Then we can do like so

        &gt;&gt;&gt; document_lengths = dataset._data[&#39;vw_text&#39;].apply(lambda text: len(text.split()))
        &gt;&gt;&gt; median_document_length = np.median(document_lengths)
        &gt;&gt;&gt; num_documents = dataset._data.shape[0]
        &gt;&gt;&gt; dataset_fragment_length = 25000
        &gt;&gt;&gt; num_documents_for_computing = dataset_fragment_length / median_document_length
        &gt;&gt;&gt; documents_fraction = num_documents_for_computing / num_documents

    one_stage_num_iter
        There will be five stages, each with nearly 5-values-grid search.
        One such search lasts `one_stage_num_iter` iterations
        with coherence computation in the end.
        So, there is going to be `one_stage_num_iter` * 5 * 5 training iterations (not slow)
        and 5 * 5 coherence computations (here may be slow if `documents_fraction` is high)
    verbose
        Whether to show experiment progress or not

    &#34;&#34;&#34;
    all_modalities = list(Dataset(dataset_path).get_possible_modalities())

    if len(all_modalities) == 0:
        warnings.warn(f&#39;No modalities in the dataset &#34;{dataset_path}&#34;!&#39;)

    if modalities is None:
        modalities = all_modalities
    if any([m not in all_modalities for m in modalities]):
        warnings.warn(f&#39;Not all `modalities` are found in the dataset &#34;{dataset_path}&#34;!&#39;)

    if main_modality is None:
        main_modality = modalities[0]

        warnings.warn(
            f&#39;Main modality not specified!&#39;
            f&#39; So modality &#34;{main_modality}&#34; will be used as the main one&#39;
        )

    specific_topics = [
        f&#39;topic_{i}&#39; for i in range(num_specific_topics)
    ]
    background_topics = [
        f&#39;bcg_topic_{i}&#39;
        for i in range(num_specific_topics, num_specific_topics + num_background_topics)
    ]

    if dictionary_filter_parameters is None:
        dictionary_filter_parameters = dict()

    dictionary_filter_parameters_as_yml = self._format_dictionary_filter_parameters(
        dictionary_filter_parameters,
        indent=2 * ONE_CONFIG_INDENT,
    )

    self._recipe = self.recipe_template.format(
        modality_names=modalities,
        main_modality=main_modality,
        dataset_path=dataset_path,
        dictionary_filter_parameters=dictionary_filter_parameters_as_yml,
        keep_dataset_in_memory=keep_dataset_in_memory,
        keep_dataset=keep_dataset,
        documents_fraction=documents_fraction,
        specific_topics=specific_topics,
        background_topics=background_topics,
        one_stage_num_iter=one_stage_num_iter,
        verbose=verbose,
    )

    return self._recipe</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="topicnet.cooking_machine.recipes.recipe_wrapper.BaseRecipe" href="recipe_wrapper.html#topicnet.cooking_machine.recipes.recipe_wrapper.BaseRecipe">BaseRecipe</a></b></code>:
<ul class="hlist">
<li><code><a title="topicnet.cooking_machine.recipes.recipe_wrapper.BaseRecipe.build_experiment_environment" href="recipe_wrapper.html#topicnet.cooking_machine.recipes.recipe_wrapper.BaseRecipe.build_experiment_environment">build_experiment_environment</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="topicnet.cooking_machine.recipes" href="index.html">topicnet.cooking_machine.recipes</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="topicnet.cooking_machine.recipes.intratext_coherence_pipeline.IntratextCoherenceRecipe" href="#topicnet.cooking_machine.recipes.intratext_coherence_pipeline.IntratextCoherenceRecipe">IntratextCoherenceRecipe</a></code></h4>
<ul class="">
<li><code><a title="topicnet.cooking_machine.recipes.intratext_coherence_pipeline.IntratextCoherenceRecipe.format_recipe" href="#topicnet.cooking_machine.recipes.intratext_coherence_pipeline.IntratextCoherenceRecipe.format_recipe">format_recipe</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.0</a>.</p>
</footer>
</body>
</html>